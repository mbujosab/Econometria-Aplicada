{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8239a120-9317-40bb-8fc6-a0e5663bf27d",
   "metadata": {},
   "source": [
    "Lección 8. Predicción de modelos ARIMA\n",
    "======================================\n",
    "\n",
    "**Author:** Marcos Bujosa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ad303-532b-443d-a5a8-32f0158d812b",
   "metadata": {},
   "source": [
    "<div class=\"abstract\" id=\"org31bd4da\">\n",
    "<p>\n",
    "Esta lección se centra en la predicción de modelos de series temporales univariantes.  \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2df55-c624-47d5-a850-615be6e147e3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Carga de algunos módulos de python y creación de directorios auxiliares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426bef2c-d634-4122-b9bd-8bc3f8cefbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:38:37.248377Z",
     "iopub.status.busy": "2025-10-20T14:38:37.248181Z",
     "iopub.status.idle": "2025-10-20T14:38:39.631387Z",
     "shell.execute_reply": "2025-10-20T14:38:39.630616Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as mpl\n",
    "# definimos parámetros para mejorar los gráficos\n",
    "mpl.rc('text', usetex=False)\n",
    "import matplotlib.pyplot as plt   # data visualization\n",
    "import dataframe_image as dfi   # export tables as .png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122e1a0-fa41-42fd-8b65-85a5722f6199",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Directorio auxiliar para albergar las figuras de la lección:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe275b-f569-44c7-b4cc-b0db122e90a7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004a809c-419a-4f90-87bc-27391643b767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:38:39.633885Z",
     "iopub.status.busy": "2025-10-20T14:38:39.633629Z",
     "iopub.status.idle": "2025-10-20T14:38:39.639804Z",
     "shell.execute_reply": "2025-10-20T14:38:39.639269Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\n",
    "import os\n",
    "os.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe6621-e385-4917-8956-657ef085434d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072285e9-1f55-4f87-9816-c09b08cfdd40",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Cargamos las funciones auxiliares (véase la carpeta `src/`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0efa4ab-7ce4-4f99-965d-98f2f58a049a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:38:39.641713Z",
     "iopub.status.busy": "2025-10-20T14:38:39.641514Z",
     "iopub.status.idle": "2025-10-20T14:38:40.202038Z",
     "shell.execute_reply": "2025-10-20T14:38:40.201338Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "%run -i ./src/analisis_armas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc402546-ae6a-4afa-a724-e2f089114924",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5066378-6334-497b-a42c-97e765133deb",
   "metadata": {},
   "source": [
    "Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n",
    "\n",
    "-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n",
    "-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n",
    "    \n",
    "    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n",
    "-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n",
    "\n",
    "**Los modelos lineales facilitan el cálculo de estos tres aspectos**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7b243-f33f-4e62-8b28-e5d5f588d71a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicciones de error cuadrático medio mínimo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae9fbd-45d9-4042-8cf8-6818529b32c2",
   "metadata": {},
   "source": [
    "Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n",
    "\n",
    "Nos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c14977-e0a8-4224-b208-0783b4a80838",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n",
    "$$\n",
    "MSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187b871-81ce-49c8-afb2-b932fbe96e92",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del error cuadrático $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64273cae-4d6e-4ad7-9d06-eec5ec1b8276",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n",
    "$$\n",
    "E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea24f5c-daba-4394-84bc-db154130a42e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e58f6-dac1-4512-aadd-055a842a45c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n",
    "\n",
    "Como solución alternativa podemos limitarnos a buscar predictores entre:\n",
    "\n",
    "-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n",
    "-   para seleccionar aquella con menor error cuadrático medio (MSEL).\n",
    "\n",
    "Al adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n",
    "\n",
    "**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b421ee2-3125-4028-bf79-f7bfc917d99d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicciones lineales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b82c6-91af-442d-8a70-a68d03e05ef2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Breve recordatorio sobre las proyecciones ortogonales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3051436-7ee7-44a4-8a32-f28fa3a6fc43",
   "metadata": {},
   "source": [
    "Solo recordaremos que como\n",
    "\n",
    "<div class=\"org-center\">\n",
    "<p>\n",
    "<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n",
    "-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n",
    "\n",
    "La proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n",
    "\n",
    "En el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841b8ea-97e0-44a3-8c99-e8b8920bc3d3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a1c91-c14e-4340-be00-b0233b777f68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Las predicciones lineales son proyecciones ortogonales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61539b4a-9420-46a9-8c4a-a184cd8a5ad3",
   "metadata": {},
   "source": [
    "La forma general de un predictor *lineal* es $\\mathcal{h}$\n",
    "$$\n",
    "g_t(\\ell)\n",
    "\\;=\\; \n",
    "{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n",
    "\\;=\\; \n",
    "\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n",
    "\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n",
    "$$\n",
    "donde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dda8d-e31d-4363-b396-734fe608fb20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n",
    "$$\n",
    "E\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51276859-8885-4303-a0e7-669330548b36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe6cba-1e93-434e-a34d-f96b8fe55126",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n",
    "\n",
    "Es habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n",
    "\n",
    "Cuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664fe1b-b6d3-4346-8a64-92846cdbdf2b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n",
    "\n",
    "Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n",
    "$$\n",
    "MSE \\leq MSEL.\n",
    "$$\n",
    "La igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n",
    "\n",
    "Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f18e19-7cce-4ed7-8427-6536af9676a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0432cf-5bf6-457a-bfff-b70ad6ae3535",
   "metadata": {},
   "source": [
    "Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n",
    "\n",
    "Para $\\ell > 0$,\n",
    "\n",
    "-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n",
    "\n",
    "-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n",
    "    \n",
    "    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e61ce-3383-4dcb-9bf2-ddece4c566f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n",
    "\n",
    "-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n",
    "\n",
    "Además para cualquier índice $j$: \n",
    "\n",
    "-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n",
    "\n",
    "Tras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd564c-43c6-4c5d-b757-92a75a5805d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cálculo de predicciones de procesos ARIMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dbaa9-5357-4c85-917a-1c940cf06faa",
   "metadata": {},
   "source": [
    "Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n",
    "$$\n",
    "\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n",
    "= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n",
    "\\quad t\\in\\mathbb{N}.\n",
    "$$\n",
    "donde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n",
    "\n",
    "Denotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n",
    "$$\n",
    "\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n",
    "$$ \n",
    "Y denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n",
    "$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\n",
    "Entonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n",
    "$$\n",
    "\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edadad8-0a82-4e1d-bce4-b90b82d8941a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Debido a la simple estructura de los modelos ARIMA, tenemos que\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n",
    "\\label{eqn:ARMA-en-t}\n",
    "\\end{equation}\n",
    "\n",
    "De \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n",
    "\\label{eqn:ARMA-en-t+l}\n",
    "\\end{equation}\n",
    "\n",
    "Dividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = \n",
    "c + \n",
    "\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n",
    "\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \n",
    "U_{t+\\ell} + \n",
    "\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n",
    "\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n",
    "\\label{eqn:ARMA-en-t+l-bis}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c0c65-28d2-42c4-860e-90cc6e4cf267",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = \n",
    "\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n",
    "\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n",
    "\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n",
    "\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n",
    "\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n",
    "\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n",
    "\\;\\text{ con } \\ell\\geq1.\n",
    "\\label{eqn:ARMA-en-t+l-bis-notas}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd061cc-7e12-4a2d-8f52-f5b43b6e9d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n",
    "\\label{eqn:predicion-recursiva-ARMA}\n",
    "\\end{equation}\n",
    "\n",
    "donde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n",
    "\n",
    "A partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187992c-3100-4863-a467-aa4277f6d9b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a583ccc-db2e-4fc9-bdfd-b0e7b896e9b3",
   "metadata": {},
   "source": [
    "#### ARMA(1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adb05d-cbe1-45f0-82d3-0078fa588f5d",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n",
    "$$\n",
    "la ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t. \\\\\\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n",
    "  \\end{cases}\n",
    "  \\label{eqn:predicion-recursiva-ARMA11}\n",
    "\\end{equation}\n",
    "\n",
    "donde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314f0f4-7136-4860-8537-a4f80aee8e69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n",
    "-   **Para $\\ell=2$:** \\begin{align*}\n",
    "    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n",
    "    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n",
    "    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n",
    "    \\end{align*}\n",
    "-   **Para $\\ell=3$:** \\begin{align*}\n",
    "    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n",
    "    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n",
    "    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n",
    "    \\end{align*}\n",
    "-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n",
    "    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n",
    "\n",
    "Como $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n",
    "$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf55fe7-0412-4805-9d8b-31c7835e2fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### AR(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c565745-6cba-490f-be1f-578e8bf1dc3b",
   "metadata": {},
   "source": [
    "Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionAR1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec51440-f6a3-4cf2-86a8-7b7b72faef1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MA(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0ca52-199e-4cbe-b4d3-1e514e0c570e",
   "metadata": {},
   "source": [
    "De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n",
    "$$\\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n",
    "  \\end{cases}$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionMA1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b4d21-4f66-4967-9276-b8bf1409dc40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1223a-2a00-4196-a01b-2abad770896d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paseo aleatorio sin deriva\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece9bb4-be12-4f92-992c-efc61c6366f1",
   "metadata": {},
   "source": [
    "Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionRW.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6259a-98bb-404e-a85b-813466e03c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paseo aleatorio con deriva\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4127814-6719-4603-8e97-a8378548e3d1",
   "metadata": {},
   "source": [
    "Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionRWCD.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63458e-333d-48f2-a7da-198b2b9d0bd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### IMA(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc28ac6-83c4-43f1-97fb-e71d9d5b8a28",
   "metadata": {},
   "source": [
    "Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n",
    "$$\\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n",
    "  \\end{cases}$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionIMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411e01a-23b8-4a03-a49b-63a4070ed901",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### IMA(1) estacional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d62c0-114e-409d-a1fa-879ec6c23230",
   "metadata": {},
   "source": [
    "Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n",
    "$$\\text{predicción de cada mes }\n",
    "  \\begin{cases}\n",
    "    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\theta_1 U_{t+j-12-1}. \\\\\n",
    "    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n",
    "  \\end{cases}$$\n",
    "Desde el segundo año, cada predicción es como la del mismo mes del año anterior.\n",
    "\n",
    "![img](./img/figurasGretl/prediccionSIMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf65d74-ed08-4218-b550-36275c1a9abc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Varianza de las predicciones lineales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cb2ad-9ab2-44af-955e-45bfcb4b79aa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Consideremos primero modelos lineales, causales y estacionarios*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319844b-00fc-4be3-a9dd-b41ff336e725",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n",
    "$$\n",
    "Y_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n",
    "$$\n",
    "La predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n",
    "\\label{eqn:predicion-MA-infinito}\n",
    "\\end{equation}\n",
    "\n",
    "Restando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n",
    "$$\n",
    "e_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n",
    "$$\n",
    "cuya esperanza es cero. \n",
    "\n",
    "Asumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n",
    "\n",
    "$$\n",
    "E\\Big[e_t(\\ell)^2\\Big]=\n",
    "Var(e_t(\\ell))=\n",
    "%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n",
    "\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n",
    "$$\n",
    "donde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474a46e-2990-4f20-a211-dfc6a45d34ea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4aafaf-bb80-4eb0-977d-3b6d40269f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n",
    "\n",
    "-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n",
    "\n",
    "-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n",
    "\n",
    "Por ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n",
    "$$\n",
    "\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n",
    "$$\n",
    "Si $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n",
    "\n",
    "-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo que crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5053325-244a-448c-b695-18b9e9afe8a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intervalos de confianza para las previsiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123daf7-e61e-46b9-ae79-70b32eca36d0",
   "metadata": {},
   "source": [
    "Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n",
    "\n",
    "Como $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n",
    "$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \n",
    "tendremos que \n",
    "$$\n",
    "Y_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n",
    "$$\n",
    "donde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n",
    "\n",
    "En modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2671a311-3575-44d2-aa92-76a44cdc0c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaptación de las predicciones a las nuevas observaciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff43e5-4ae9-452e-9760-5fe7e54bf85c",
   "metadata": {},
   "source": [
    "Imagine que tenemos las predicciones para los periodos, desde $t+1$ hasta $t+j$ con el conjunto de información $\\mathcal{H}_{Y_t}$. Queremos adaptar las previsiones al conjunto $\\mathcal{H}_{Y_{t+1}}$.\n",
    "\n",
    "Según \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+k}$ con información hasta $t$ es:\n",
    "$$\n",
    "\\widehat{Y_{t+k|t}} = \\psi_k U_t + \\psi_{k+1} U_{t-1} + \\ldots.\n",
    "$$\n",
    "Al comparar $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n",
    "\n",
    "La nueva predicción para $Y_{t+k}$, proyectando ahora sobre $\\mathcal{H}_{Y_{t+1}}$ (es decir, incorporando $Y_{t+1}$ como nueva información), será:\n",
    "$$\n",
    "\\widehat{Y_{t+k-1|t+1}} = \\psi_{k-1} U_t + \\psi_k U_{t-1} + \\ldots\n",
    "$$\n",
    "Restando las dos previsiones, tenemos que:\n",
    "$$\n",
    "\\widehat{Y_{t+k-1|t+1}} - \\widehat{Y_{t+k|t}} = \\psi_{k-1} U_{t+1}.\n",
    "$$\n",
    "Es decir, cuando observamos $Y_{t+1}$ y calculamos el error de previsión cometido $U_{t+1}$, podremos adaptar todas las predicciones mediante:\n",
    "$$\n",
    "\\widehat{Y_{t+k-1|t+1}} = \\widehat{Y_{t+k|t}} + \\psi_{k-1} U_{t+1}\n",
    "%, \\tag{8.23}\n",
    "$$\n",
    "que indica que las predicciones se adaptan añadiendo a las predicciones anteriores una fracción del último error de predicción obtenido.  \n",
    "Si no hubo error de previsión un periodo hacia delante $(U_{t+1} = 0)$, las predicciones realizadas a un horizonte mayor no serán modificadas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0efae-4963-4a10-acc8-bc73084f3442",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos con logaritmos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235be1e-4fc4-4d0d-90ce-32f61a976a10",
   "metadata": {},
   "source": [
    "Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\n",
    "$$\n",
    "Y_{t+\\ell} = \\ln(X_{t+\\ell})\n",
    "$$\n",
    "entonces la previsión puntual  de la serie original es \n",
    "$$\n",
    "\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n",
    "$$\n",
    "Y el Intervalo de confianza\n",
    "$$\n",
    "I_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n",
    "$$\n",
    "donde \n",
    "\n",
    "\\begin{align*}\n",
    "a = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\n",
    "b = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "que es no simétrico alrededor de la previsión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea4ad4-607c-4e2a-9e08-382f6c9d2e6a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Nota sobre subespacio sobre el que proyectamos al prever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450c0e9-2c83-4440-8a00-2204f9788fea",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n",
    "$$\n",
    "Y_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n",
    "$$\n",
    "donde:\n",
    "\n",
    "-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n",
    "-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n",
    "-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n",
    "    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n",
    "    $$\n",
    "      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n",
    "      $$\n",
    "\n",
    "Bajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n",
    "$$\n",
    "\\mathcal{H}_{Y_t} \n",
    "\\;=\\;\n",
    "\\mathcal{H}_{U_t} \n",
    "$$\n",
    "donde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \n",
    "La \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n",
    "\n",
    "**Demostración:**\n",
    "\n",
    "1.  **Primera inclusión** $\\subseteq$:\n",
    "    \n",
    "    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n",
    "    $$\n",
    "       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n",
    "       $$\n",
    "    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n",
    "    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n",
    "    $$\n",
    "       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n",
    "       \\subseteq\n",
    "       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n",
    "       $$\n",
    "\n",
    "2.  **Segunda inclusión** $\\supseteq$:\n",
    "    \n",
    "    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n",
    "    $$\n",
    "       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n",
    "       $$\n",
    "    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n",
    "    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n",
    "    $$\n",
    "       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n",
    "       \\supseteq\n",
    "       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n",
    "       $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
