{"cells":[{"cell_type":"markdown","id":"44239a50-1bfb-4359-b94a-18ebdbdeba91","metadata":{},"source":"Lección 8. Predicción de modelos ARIMA\n======================================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"c8c2c490-7219-4e85-9bf0-85ffbfe1f8d9","metadata":{},"source":["<div class=\"abstract\" id=\"org83217a1\">\n<p>\nEsta lección se centra en la predicción de modelos de series temporales univariantes.  \n</p>\n\n</div>\n\n-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n\n"]},{"cell_type":"markdown","id":"672176f2-9e0b-4a85-a894-e1335060963a","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python y creación de directorios auxiliares\n\n"]},{"cell_type":"code","execution_count":1,"id":"4870fe4d-542f-40e5-9c9c-b7ebc89f6435","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=False)\nimport matplotlib.pyplot as plt   # data visualization\nimport dataframe_image as dfi   # export tables as .png"]},{"cell_type":"markdown","id":"a0489fe4-f51b-4719-88c1-6ad5bad2b1f5","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Directorio auxiliar para albergar las figuras de la lección:\n\n"]},{"cell_type":"markdown","id":"0f4cf92a-a24f-495a-8420-dee74cd24fb1","metadata":{"slideshow":{"slide_type":"skip"}},"source":["para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n\n"]},{"cell_type":"code","execution_count":1,"id":"c7e0e4af-f321-462f-a043-4aabc5f24289","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\nimport os\nos.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"]},{"cell_type":"markdown","id":"b16b24f3-ddae-4346-9b9e-1a04d238e02e","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n\n"]},{"cell_type":"markdown","id":"22f9aa6b-4e9d-4032-9127-80dac79593ad","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Cargamos las funciones auxiliares (véase la carpeta `src/`)\n\n"]},{"cell_type":"code","execution_count":1,"id":"e1a35109-beff-4eca-9cb9-f793b0bcb0fe","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n%run -i ./src/analisis_armas.py"]},{"cell_type":"markdown","id":"5b890006-e7a0-4dad-82ef-21a587a18729","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Introducción\n\n"]},{"cell_type":"markdown","id":"b6fb1303-9776-4602-884f-6e0f7d12eda7","metadata":{},"source":["Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n\n-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n    \n    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n\n**Los modelos lineales facilitan solventar estas tres necesidades**.\n\n"]},{"cell_type":"markdown","id":"09c97373-7c5b-4036-8dfd-413fa1ec79d7","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Predicciones de error cuadrático medio mínimo\n\n"]},{"cell_type":"markdown","id":"b94a968f-4d73-4fd8-9a2e-e3a83b01c794","metadata":{},"source":["Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n\nNos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n\n"]},{"cell_type":"markdown","id":"f39ce2d0-b89a-4ff2-ae2f-2cf942d7da50","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n$$\nMSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n$$\n\n"]},{"cell_type":"markdown","id":"920eff96-71e6-4761-b1c1-b50b5813a83f","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del cuadrado del error de predicción $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n\n"]},{"cell_type":"markdown","id":"2a9bf0a0-6971-4d8a-ad87-4af74180d178","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n$$\nE\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n$$\n\n"]},{"cell_type":"markdown","id":"227fd0fe-1fb4-4b66-84b0-6e074660f09e","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n\n"]},{"cell_type":"markdown","id":"f3e53319-75dd-4b5e-b0a3-43f87b5792d2","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n\nComo solución alternativa podemos limitarnos a buscar predictores entre:\n\n-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n-   y seleccionar aquella con menor error cuadrático medio (MSEL).\n\nAl adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n\n**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n\n"]},{"cell_type":"markdown","id":"856a7d80-6034-4063-8c87-260cdf05018a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Predicciones lineales\n\n"]},{"cell_type":"markdown","id":"44f31ea5-984f-4161-a0a6-df9e68a32e06","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Breve recordatorio sobre las proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"46374585-7295-4dd7-815c-7c928e802bfa","metadata":{},"source":["Solo recordaremos que como\n\n<div class=\"org-center\">\n<p>\n<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n</p>\n</div>\n\n-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n\nLa proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n\nEn el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n\n"]},{"cell_type":"markdown","id":"00a48a23-5a0c-46c7-8d71-367b806346d5","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n\n"]},{"cell_type":"markdown","id":"1601f405-3f97-4729-a8f5-932639558328","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Las predicciones lineales son proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"217f0d78-53ef-4497-be6a-cfa12e9e72a6","metadata":{},"source":["La forma general de un predictor *lineal* es \n$$g_t(\\ell)\n\\;=\\; \n{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n\\;=\\; \n\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n$$\ndonde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"df4ca27c-e8cf-4699-b713-a0c2613e70b5","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n$$\nE\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n$$\n\n"]},{"cell_type":"markdown","id":"57d9415a-19d8-4687-be16-5c059da394f4","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n\n"]},{"cell_type":"markdown","id":"06935bb8-d15d-41e4-a83f-983500c33237","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n\nEs habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n\nCuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n\n"]},{"cell_type":"markdown","id":"b17e8de3-8757-417b-8f3d-6180ec9383e6","metadata":{"slideshow":{"slide_type":"skip"}},"source":["(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n\n"]},{"cell_type":"markdown","id":"7e3fda0e-7cad-49fc-a6e3-a49f6da38d46","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n$$\nMSE \\leq MSEL.\n$$\nLa igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n\n"]},{"cell_type":"markdown","id":"6c16408d-2494-413f-9cd1-0b631e2de5b8","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n\n"]},{"cell_type":"markdown","id":"bc4624ee-de6f-47cc-b4bb-999a82e9eb54","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n\n"]},{"cell_type":"markdown","id":"12bec032-cbb0-4584-99bf-867a7ad9be4e","metadata":{},"source":["Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n\nPara $\\ell > 0$,\n\n-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n\n-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n    \n    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n\n"]},{"cell_type":"markdown","id":"ea03fca4-d4b1-4fc1-9f81-f72def8bad78","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n\n-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n\nAdemás para cualquier índice $j$: \n\n-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n\nTras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n\n"]},{"cell_type":"markdown","id":"d78506f6-5053-4e4f-a760-a44415a15dfb","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Cálculo de predicciones de procesos ARIMA\n\n"]},{"cell_type":"markdown","id":"bcd1b546-2201-4c19-a1b6-9fa6d6073f11","metadata":{},"source":["Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n$$\n\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n\\quad t\\in\\mathbb{N}.\n$$\ndonde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n\nDenotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n$$\n\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n$$ \nY denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\nEntonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n$$\n\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n$$\n\n"]},{"cell_type":"markdown","id":"77016617-3afe-407c-992e-f20a7048e96f","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Debido a la simple estructura de los modelos ARIMA, tenemos que\n\n\\begin{equation}\nY_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n\\label{eqn:ARMA-en-t}\n\\end{equation}\n\nDe \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n\n\\begin{equation}\nY_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l}\n\\end{equation}\n\nDividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n\n\\begin{equation}\nY_{t+\\ell} = \nc + \n\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \nU_{t+\\ell} + \n\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l-bis}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"8315aa7e-e8a7-497b-a378-77832039287d","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n\n\\begin{equation}\nY_{t+\\ell} = \n\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n\\;\\text{ con } \\ell\\geq1.\n\\label{eqn:ARMA-en-t+l-bis-notas}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"059bdce0-c53e-426d-9900-de88709f32fc","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n\\label{eqn:predicion-recursiva-ARMA}\n\\end{equation}\n\ndonde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n\nA partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n\n"]},{"cell_type":"markdown","id":"1b271c47-a06b-4648-9732-1aa5c76f3c91","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Ejemplos\n\n"]},{"cell_type":"markdown","id":"c60d2944-0dbb-4c0f-8a81-8c8ae6a57482","metadata":{},"source":["#### ARMA(1,1)\n\n"]},{"cell_type":"markdown","id":"b51b77eb-0b25-4ef8-8c2e-e1bbc2972fc5","metadata":{},"source":["$$\nY_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n$$\nla ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n\n\\begin{equation}\n  \\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t \\qquad \\text{cuando } \\ell=1 \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}\n  \\label{eqn:predicion-recursiva-ARMA11}\n\\end{equation}\n\ndonde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n\n"]},{"cell_type":"markdown","id":"dd7d9067-027f-4c46-ae10-58a22d33b589","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n-   **Para $\\ell=2$:** \\begin{align*}\n    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n    \\end{align*}\n-   **Para $\\ell=3$:** \\begin{align*}\n    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n    \\end{align*}\n-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n\nComo $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n\n"]},{"cell_type":"markdown","id":"aae6a312-a829-4dbd-9a7d-08bcbd214fb9","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### AR(1)\n\n"]},{"cell_type":"markdown","id":"0c83ddee-38f2-498f-91c4-53dbb53547f9","metadata":{},"source":["Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n$$\n\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionAR1.png)\n\n"]},{"cell_type":"markdown","id":"7fc3184b-9318-44d4-a4d5-1e92c9f0ea39","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### MA(1)\n\n"]},{"cell_type":"markdown","id":"312b3f06-6d1c-4dca-b2bc-2857efec015a","metadata":{},"source":["De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionMA1.png)\n\n"]},{"cell_type":"markdown","id":"af9c0a69-dd59-453d-b3aa-c142ebb0be1c","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n\n"]},{"cell_type":"markdown","id":"8c9ef2a4-79b2-458f-98ad-ab1223f9c5dd","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio sin deriva\n\n"]},{"cell_type":"markdown","id":"b0fd6606-6c5a-4550-8d2b-6def58049bb5","metadata":{},"source":["Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRW.png)\n\n"]},{"cell_type":"markdown","id":"96d650fd-ec6f-4f68-a7f8-bf7d9d135600","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio con deriva\n\n"]},{"cell_type":"markdown","id":"3f277626-7473-4180-b2ac-0142562bd0ca","metadata":{},"source":["Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRWCD.png)\n\n"]},{"cell_type":"markdown","id":"5d940deb-b463-4a52-894c-03c9fa4dc33b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1)\n\n"]},{"cell_type":"markdown","id":"5541776a-2ec1-4876-8b54-b3194cf44b0d","metadata":{},"source":["Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionIMA.png)\n\n"]},{"cell_type":"markdown","id":"7ece6d9d-c12d-45a1-b772-56cf27d1cbbf","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1) estacional\n\n"]},{"cell_type":"markdown","id":"e3bbf737-ae9a-490b-91e4-59096c1d5d33","metadata":{},"source":["Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n$$\\text{predicción de cada mes }\n  \\begin{cases}\n    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\Theta_1 U_{t+j-12-1}. \\\\\n    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n  \\end{cases}$$\nDesde el segundo año, cada predicción es como la del mismo mes del año anterior.\n\n![img](./img/figurasGretl/prediccionSIMA.png)\n\n"]},{"cell_type":"markdown","id":"01e628cf-67ab-452d-bcb5-7622dd575efe","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Varianza de las predicciones lineales\n\n"]},{"cell_type":"markdown","id":"2c5b346f-baed-42ce-b045-ef04be9e5384","metadata":{},"source":["*Consideremos primero modelos lineales, causales y estacionarios*.\n\n"]},{"cell_type":"markdown","id":"78545671-9c08-40c9-ba02-1cf6120e6f9b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n$$\nY_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n$$\nLa predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n\\label{eqn:predicion-MA-infinito}\n\\end{equation}\n\nRestando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n$$\ne_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n$$\ncuya esperanza es cero. \n\nAsumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n\n$$\nE\\Big[e_t(\\ell)^2\\Big]=\nVar(e_t(\\ell))=\n%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n$$\ndonde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"a0b4d37a-a7c7-4be6-867f-0238bc27629c","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n\n-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n\n-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n\n-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n\nPor ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n$$\n\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n$$\nSi $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n\n-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n\n"]},{"cell_type":"markdown","id":"5d996b10-ee2e-43f6-a3b5-fe306936b041","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Intervalos de confianza para las previsiones\n\n"]},{"cell_type":"markdown","id":"d6561e50-1a40-46f1-b7b5-f66c053e1ec4","metadata":{},"source":["Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n\nComo $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \ntendremos que \n$$\nY_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n$$\ndonde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n\nEn modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n\n"]},{"cell_type":"markdown","id":"22ba8903-d5e2-463d-a02d-fc34df19b93b","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Adaptación de las predicciones a las nuevas observaciones\n\n"]},{"cell_type":"markdown","id":"33d1476a-9ad7-4ea2-9ae5-68059aa8106a","metadata":{},"source":["Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\nSegún \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n$$\n\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n$$\nComparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n\nLa nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\psi_{\\ell-1} U_{t+1} + \\psi_\\ell U_{t}  + \\psi_{\\ell+1} U_{t-1} + \\ldots\n$$\nRestando las dos previsiones, tenemos que:\n$$\n\\widehat{Y_{t+\\ell|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n$$\nAl observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n%, \\tag{8.23}\n$$\n(las predicciones se revisan sumando un múltiplo del último error de predicción).\n\nSi no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n\n"]},{"cell_type":"markdown","id":"c2463e1d-32b7-4e20-9f97-a1eb53170f93","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Medidas del error de previsión\n\n"]},{"cell_type":"markdown","id":"562b00f6-bef1-4e6a-b9d8-bbc555a095b4","metadata":{},"source":["-   **Error Medio (ME):** $= \\frac{1}{T} \\sum_{t=1}^T e_t$\n    \n    Promedio de los errores de previsión.\n\n-   **Raíz del Error Cuadrático Medio (RMSE):** $= \\sqrt{\\frac{1}{T} \\sum_{t=1}^T e_t^2}$\n    \n    Es la norma euclídea de vector de errores.\n\n-   **Error Absoluto Medio (MAE):** $= \\frac{1}{T} \\sum_{t=1}^T |e_t|$\n    \n    Promedio de los errores de previsión en términos absolutos\n\n-   **Error Porcentual Medio (MPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\frac{e_t}{y_t}$\n    \n    Promedio de los errores de previsión porcentuales\n\n-   **Error Porcentual Absoluto Medio (MAPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\left|\\frac{e_t}{y_t}\\right|$\n    \n    Promedio del valor absoluto de los errores porcentuales\n\nGretl muestra otros estadísticos adicionales (véase el manual).\n\n"]},{"cell_type":"markdown","id":"8c904088-70c6-4272-8658-2998c452365b","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Modelos con logaritmos\n\n"]},{"cell_type":"markdown","id":"358ddd69-a730-40e8-812c-242630815656","metadata":{},"source":["Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\nSi\n$$\nY_{t+\\ell} = \\ln(X_{t+\\ell})\n$$\nentonces la previsión puntual  de la serie original es \n$$\n\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n$$\nY el Intervalo de confianza\n$$\nI_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n$$\ndonde \n\n\\begin{align*}\na = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\nb = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n\\end{align*}\n\nque es no simétrico alrededor de la previsión.\n\n"]},{"cell_type":"markdown","id":"6c72deb3-dff6-47e0-b42d-32795cf00d9c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["## Nota sobre subespacio sobre el que proyectamos al prever\n\n"]},{"cell_type":"markdown","id":"6cc8a144-3c45-4568-9bcb-f4d8ed1f2b22","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n$$\nY_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n$$\ndonde:\n\n-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n    $$\n      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n      $$\n\nBajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n$$\n\\mathcal{H}_{Y_t} \n\\;=\\;\n\\mathcal{H}_{U_t} \n$$\ndonde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \nLa \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n\n**Demostración:**\n\n1.  **Primera inclusión** $\\subseteq$:\n    \n    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n    $$\n       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n       $$\n    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n    $$\n       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\subseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n2.  **Segunda inclusión** $\\supseteq$:\n    \n    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n    $$\n       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n       $$\n    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n    $$\n       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\supseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}