{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6482aa-61eb-422f-859c-ee8b0cfde375",
   "metadata": {},
   "source": [
    "Lección 8. Predicción de modelos ARIMA\n",
    "======================================\n",
    "\n",
    "**Author:** Marcos Bujosa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9438ac-4871-4b7f-8da0-ec9074c50954",
   "metadata": {},
   "source": [
    "<div class=\"abstract\" id=\"org5cb210d\">\n",
    "<p>\n",
    "Esta lección se centra en la predicción de modelos de series temporales univariantes.  \n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df318101-b2df-4a17-8fca-0b5b1e94687b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Carga de algunos módulos de python y creación de directorios auxiliares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c2cd20-45b5-477f-93f3-c3fa60a6d933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:34:21.872506Z",
     "iopub.status.busy": "2025-10-21T14:34:21.872210Z",
     "iopub.status.idle": "2025-10-21T14:34:24.335581Z",
     "shell.execute_reply": "2025-10-21T14:34:24.334904Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as mpl\n",
    "# definimos parámetros para mejorar los gráficos\n",
    "mpl.rc('text', usetex=False)\n",
    "import matplotlib.pyplot as plt   # data visualization\n",
    "import dataframe_image as dfi   # export tables as .png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97dd75b-b4db-4d04-b797-8430d5623665",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Directorio auxiliar para albergar las figuras de la lección:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c5864-9659-4678-9b21-0f1708c92f1d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e803719c-8716-4eeb-bb23-c50923d536f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:34:24.338171Z",
     "iopub.status.busy": "2025-10-21T14:34:24.337907Z",
     "iopub.status.idle": "2025-10-21T14:34:24.344102Z",
     "shell.execute_reply": "2025-10-21T14:34:24.343393Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\n",
    "import os\n",
    "os.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f855c-a2f2-436a-a3a6-d7f99ddd2951",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bcc1e-29d4-4ce7-9404-ce9b82e745a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Cargamos las funciones auxiliares (véase la carpeta `src/`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ba75b6-86ea-486b-87c0-2b96268d3ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:34:24.346114Z",
     "iopub.status.busy": "2025-10-21T14:34:24.345928Z",
     "iopub.status.idle": "2025-10-21T14:34:24.914549Z",
     "shell.execute_reply": "2025-10-21T14:34:24.913816Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "%run -i ./src/analisis_armas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f7d04-3849-40fe-a648-e88a4936e1a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55029ac8-20d8-4b56-87d4-446c107bd196",
   "metadata": {},
   "source": [
    "Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n",
    "\n",
    "-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n",
    "-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n",
    "    \n",
    "    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n",
    "-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n",
    "\n",
    "**Los modelos lineales facilitan el cálculo de estos tres aspectos**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8847e-0c5e-4e66-8f71-893493c6f28c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicciones de error cuadrático medio mínimo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84498b80-61b1-48fa-9aa2-f64207ccc260",
   "metadata": {},
   "source": [
    "Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n",
    "\n",
    "Nos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91510bb9-2d68-4a0a-bfac-febffac6a56f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n",
    "$$\n",
    "MSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875e86b-480c-4e81-bc7d-4e4af83a32f4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del error cuadrático $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec8412-f40c-472c-8abb-591f69753884",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n",
    "$$\n",
    "E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097db462-f325-4cae-83a2-14f206dad233",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b48ef-f577-41cd-a2ca-26d440c203fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n",
    "\n",
    "Como solución alternativa podemos limitarnos a buscar predictores entre:\n",
    "\n",
    "-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n",
    "-   para seleccionar aquella con menor error cuadrático medio (MSEL).\n",
    "\n",
    "Al adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n",
    "\n",
    "**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15754c6-1633-4908-8f39-2eeebad3b7bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicciones lineales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204988c5-22fc-46d6-964d-8853ef7043cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Breve recordatorio sobre las proyecciones ortogonales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5019e5d-a988-4d5f-a77d-4d0225294aa3",
   "metadata": {},
   "source": [
    "Solo recordaremos que como\n",
    "\n",
    "<div class=\"org-center\">\n",
    "<p>\n",
    "<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n",
    "-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n",
    "\n",
    "La proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n",
    "\n",
    "En el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f6be7-a84d-4945-a403-4d690f61109b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074bcb4-9203-4e20-b170-e8866d3f78c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Las predicciones lineales son proyecciones ortogonales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c3a87-2901-45e4-b5b5-eb9ee9a48aed",
   "metadata": {},
   "source": [
    "La forma general de un predictor *lineal* es $\\mathcal{h}$\n",
    "$$\n",
    "g_t(\\ell)\n",
    "\\;=\\; \n",
    "{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n",
    "\\;=\\; \n",
    "\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n",
    "\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n",
    "$$\n",
    "donde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef425e2b-c826-472f-947c-3bf83ef6625f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n",
    "$$\n",
    "E\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e426a-8f68-4da9-897e-97b07b5b461b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262e618-3172-4a1a-8f3c-c37d8610cf2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n",
    "\n",
    "Es habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n",
    "\n",
    "Cuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1e52e-6b37-47dd-a836-135a005f28b6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n",
    "\n",
    "Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n",
    "$$\n",
    "MSE \\leq MSEL.\n",
    "$$\n",
    "La igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n",
    "\n",
    "Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe10d7-767b-460b-a4a7-4497885bfaee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b09439-c760-4966-84be-b85fa4d1b955",
   "metadata": {},
   "source": [
    "Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n",
    "\n",
    "Para $\\ell > 0$,\n",
    "\n",
    "-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n",
    "\n",
    "-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n",
    "    \n",
    "    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3905540-5a19-4b76-a877-cdc240d17429",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n",
    "\n",
    "-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n",
    "\n",
    "Además para cualquier índice $j$: \n",
    "\n",
    "-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n",
    "\n",
    "Tras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867297f-d591-498a-b21f-d53783e9c754",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cálculo de predicciones de procesos ARIMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fecce-ed0c-4041-8d5e-407dfdaff003",
   "metadata": {},
   "source": [
    "Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n",
    "$$\n",
    "\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n",
    "= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n",
    "\\quad t\\in\\mathbb{N}.\n",
    "$$\n",
    "donde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n",
    "\n",
    "Denotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n",
    "$$\n",
    "\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n",
    "$$ \n",
    "Y denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n",
    "$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\n",
    "Entonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n",
    "$$\n",
    "\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623d529-8b3b-4c4d-8952-116961355c37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Debido a la simple estructura de los modelos ARIMA, tenemos que\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n",
    "\\label{eqn:ARMA-en-t}\n",
    "\\end{equation}\n",
    "\n",
    "De \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n",
    "\\label{eqn:ARMA-en-t+l}\n",
    "\\end{equation}\n",
    "\n",
    "Dividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = \n",
    "c + \n",
    "\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n",
    "\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \n",
    "U_{t+\\ell} + \n",
    "\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n",
    "\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n",
    "\\label{eqn:ARMA-en-t+l-bis}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1da65-9703-4e28-bdff-be6871aba9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n",
    "\n",
    "\\begin{equation}\n",
    "Y_{t+\\ell} = \n",
    "\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n",
    "\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n",
    "\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n",
    "\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n",
    "\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n",
    "\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n",
    "\\;\\text{ con } \\ell\\geq1.\n",
    "\\label{eqn:ARMA-en-t+l-bis-notas}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed03cae-0bb3-4c7c-af25-b4acf449f4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n",
    "\\label{eqn:predicion-recursiva-ARMA}\n",
    "\\end{equation}\n",
    "\n",
    "donde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n",
    "\n",
    "A partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04de2b6-bfe1-49ff-8f27-1f6a6ca39228",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9614c-47cb-4f4d-a8bd-96c4e7934ef1",
   "metadata": {},
   "source": [
    "#### ARMA(1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bce9a-458d-43bd-b64a-e6dcaec8fa55",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n",
    "$$\n",
    "la ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t. \\\\\\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n",
    "  \\end{cases}\n",
    "  \\label{eqn:predicion-recursiva-ARMA11}\n",
    "\\end{equation}\n",
    "\n",
    "donde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3a168-072c-423f-a8af-00c3aa4d1cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n",
    "-   **Para $\\ell=2$:** \\begin{align*}\n",
    "    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n",
    "    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n",
    "    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n",
    "    \\end{align*}\n",
    "-   **Para $\\ell=3$:** \\begin{align*}\n",
    "    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n",
    "    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n",
    "    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n",
    "    \\end{align*}\n",
    "-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n",
    "    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n",
    "\n",
    "Como $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n",
    "$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad0d2f-eb38-4f95-90bf-6d4b0673499c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### AR(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099478e1-7bdd-4f59-b0b5-41d1bbbfb96d",
   "metadata": {},
   "source": [
    "Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionAR1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31d323-fdd1-4a33-bc1c-649d418c089f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MA(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2812eda-4380-4430-aa46-2eb1ff73b7b8",
   "metadata": {},
   "source": [
    "De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n",
    "$$\\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n",
    "  \\end{cases}$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionMA1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84a2ed-fd97-46ae-a044-46dcf1e90819",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013247f1-86bc-43ce-9495-4d20ee7efadb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paseo aleatorio sin deriva\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f3c6c-acd7-4d4c-a1fe-f3b88a8d4c50",
   "metadata": {},
   "source": [
    "Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionRW.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4031492-c8b6-4090-b5c9-170c1161b4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paseo aleatorio con deriva\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87e573-45e0-4c7d-b0f8-0ce6cd3ab50f",
   "metadata": {},
   "source": [
    "Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n",
    "$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionRWCD.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57bd0d-9ffe-48cf-8432-da327ca578c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### IMA(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e2dfe-35b9-4f55-9c1f-777449c78156",
   "metadata": {},
   "source": [
    "Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n",
    "$$\\begin{cases}\n",
    "    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n",
    "    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n",
    "  \\end{cases}$$\n",
    "\n",
    "![img](./img/figurasGretl/prediccionIMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a849dfb-4664-4224-af7d-05eac235278b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### IMA(1) estacional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced98a1-c431-4b1b-b955-d718d01e9f43",
   "metadata": {},
   "source": [
    "Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n",
    "$$\\text{predicción de cada mes }\n",
    "  \\begin{cases}\n",
    "    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\theta_1 U_{t+j-12-1}. \\\\\n",
    "    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n",
    "  \\end{cases}$$\n",
    "Desde el segundo año, cada predicción es como la del mismo mes del año anterior.\n",
    "\n",
    "![img](./img/figurasGretl/prediccionSIMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0450b-7ad2-45b3-a594-b35395b3c673",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Varianza de las predicciones lineales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32554d65-c09c-42e7-97f1-50be6a5c445b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Consideremos primero modelos lineales, causales y estacionarios*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738342d4-ee7c-4f03-8945-506ee9a0fec2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n",
    "$$\n",
    "Y_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n",
    "$$\n",
    "La predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n",
    "\\label{eqn:predicion-MA-infinito}\n",
    "\\end{equation}\n",
    "\n",
    "Restando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n",
    "$$\n",
    "e_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n",
    "$$\n",
    "cuya esperanza es cero. \n",
    "\n",
    "Asumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n",
    "\n",
    "$$\n",
    "E\\Big[e_t(\\ell)^2\\Big]=\n",
    "Var(e_t(\\ell))=\n",
    "%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n",
    "\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n",
    "$$\n",
    "donde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b191dee-1ca9-440c-94f4-081c05916618",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475aed52-a482-4ead-a8a0-5e96d6bd059b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n",
    "\n",
    "-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n",
    "\n",
    "-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n",
    "\n",
    "Por ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n",
    "$$\n",
    "\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n",
    "$$\n",
    "Si $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n",
    "\n",
    "-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8e0f1-db16-454d-a762-eeaf29a39a7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intervalos de confianza para las previsiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1103e-5b37-4f85-bd3b-1dd3da14c69f",
   "metadata": {},
   "source": [
    "Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n",
    "\n",
    "Como $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n",
    "$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \n",
    "tendremos que \n",
    "$$\n",
    "Y_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n",
    "$$\n",
    "donde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n",
    "\n",
    "En modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26feb5-0926-4567-a569-a5f6c1e300bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaptación de las predicciones a las nuevas observaciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c407c-8973-4ac2-a512-ae4c0b1b95fa",
   "metadata": {},
   "source": [
    "Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\n",
    "Según \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n",
    "$$\n",
    "Comparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n",
    "\n",
    "La nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell-1|t+1}} = \\psi_{\\ell-1} U_t + \\psi_\\ell U_{t-1} + \\ldots\n",
    "$$\n",
    "Restando las dos previsiones, tenemos que:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell-1|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n",
    "$$\n",
    "Al observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n",
    "$$\n",
    "\\widehat{Y_{t+\\ell-1|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n",
    "%, \\tag{8.23}\n",
    "$$\n",
    "(las predicciones se revisan sumando un múltiplo del último error de predicción).\n",
    "\n",
    "Si no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47ce22-d15b-428d-830c-a06bd0201daa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos con logaritmos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f35c8-5e8d-40ea-8fa5-d8b8b85d084c",
   "metadata": {},
   "source": [
    "Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\n",
    "Si\n",
    "$$\n",
    "Y_{t+\\ell} = \\ln(X_{t+\\ell})\n",
    "$$\n",
    "entonces la previsión puntual  de la serie original es \n",
    "$$\n",
    "\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n",
    "$$\n",
    "Y el Intervalo de confianza\n",
    "$$\n",
    "I_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n",
    "$$\n",
    "donde \n",
    "\n",
    "\\begin{align*}\n",
    "a = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\n",
    "b = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "que es no simétrico alrededor de la previsión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5459c2-50a6-4da6-b5b7-82dfa2cd2ca3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Nota sobre subespacio sobre el que proyectamos al prever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef8118-2051-47e9-938a-849a141de703",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n",
    "$$\n",
    "Y_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n",
    "$$\n",
    "donde:\n",
    "\n",
    "-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n",
    "-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n",
    "-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n",
    "    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n",
    "    $$\n",
    "      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n",
    "      $$\n",
    "\n",
    "Bajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n",
    "$$\n",
    "\\mathcal{H}_{Y_t} \n",
    "\\;=\\;\n",
    "\\mathcal{H}_{U_t} \n",
    "$$\n",
    "donde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \n",
    "La \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n",
    "\n",
    "**Demostración:**\n",
    "\n",
    "1.  **Primera inclusión** $\\subseteq$:\n",
    "    \n",
    "    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n",
    "    $$\n",
    "       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n",
    "       $$\n",
    "    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n",
    "    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n",
    "    $$\n",
    "       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n",
    "       \\subseteq\n",
    "       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n",
    "       $$\n",
    "\n",
    "2.  **Segunda inclusión** $\\supseteq$:\n",
    "    \n",
    "    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n",
    "    $$\n",
    "       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n",
    "       $$\n",
    "    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n",
    "    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n",
    "    $$\n",
    "       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n",
    "       \\supseteq\n",
    "       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n",
    "       $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
