{"cells":[{"cell_type":"markdown","id":"2a6264c7-5416-4538-afe9-548f1c0b5c7e","metadata":{},"source":"Lección 8. Predicción de modelos ARIMA\n======================================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"3ca265ec-f9ac-4d05-a99a-166602912038","metadata":{},"source":["<div class=\"abstract\" id=\"orgdc966c9\">\n<p>\nEsta lección se centra en la predicción de modelos de series temporales univariantes.  \n</p>\n\n</div>\n\n-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n\n"]},{"cell_type":"markdown","id":"6d2f966d-3f6b-4ff5-a3af-79d43466b136","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python y creación de directorios auxiliares\n\n"]},{"cell_type":"code","execution_count":1,"id":"f76cf07e-0482-4225-adf9-2362380b1b8b","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=False)\nimport matplotlib.pyplot as plt   # data visualization\nimport dataframe_image as dfi   # export tables as .png"]},{"cell_type":"markdown","id":"e1e681cc-b3e7-4fa2-abb5-5434c13696c6","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Directorio auxiliar para albergar las figuras de la lección:\n\n"]},{"cell_type":"markdown","id":"14fe89af-26bf-4a36-bf2d-1009eb0babff","metadata":{"slideshow":{"slide_type":"skip"}},"source":["para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n\n"]},{"cell_type":"code","execution_count":1,"id":"add144db-9a7b-47ba-bccd-5b2d6d310092","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\nimport os\nos.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"]},{"cell_type":"markdown","id":"3be24e0e-1d27-4469-a9bf-4273cccaeae3","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n\n"]},{"cell_type":"markdown","id":"85a7669d-0d7d-412f-a589-e11c123862c6","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Cargamos las funciones auxiliares (véase la carpeta `src/`)\n\n"]},{"cell_type":"code","execution_count":1,"id":"01fd44e4-ffec-429c-879c-6771bf837ca7","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n%run -i ./src/analisis_armas.py"]},{"cell_type":"markdown","id":"c1542fdc-aa5a-49d2-8cd7-ca2dfa6d020f","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Introducción\n\n"]},{"cell_type":"markdown","id":"e30b1942-95b0-46fc-b8d9-55220aae567a","metadata":{},"source":["Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n\n-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n    \n    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n\n**Los modelos lineales facilitan solventar estas tres necesidades**.\n\n"]},{"cell_type":"markdown","id":"c4de3174-2b1e-4361-852b-3c29b8792454","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Predicciones de error cuadrático medio mínimo\n\n"]},{"cell_type":"markdown","id":"7eaecd6c-e746-4e33-a67b-e6c8b97cce97","metadata":{},"source":["Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n\nNos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n\n"]},{"cell_type":"markdown","id":"ed3cb74c-3bf3-4f4a-9125-a33c269a018b","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n$$\nMSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n$$\n\n"]},{"cell_type":"markdown","id":"a339b722-69f0-4494-9693-35d9e6ec1dbe","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del cuadrado del error de predicción $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n\n"]},{"cell_type":"markdown","id":"de4dc402-3336-4eae-8cf5-bf8e348dd49a","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n$$\nE\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n$$\n\n"]},{"cell_type":"markdown","id":"61b2a998-4a32-43ba-ae16-f3dbd9416744","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n\n"]},{"cell_type":"markdown","id":"4adb6daf-176e-4095-b497-4c1b21518ac3","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n\nComo solución alternativa podemos limitarnos a buscar predictores entre:\n\n-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n-   y seleccionar aquella con menor error cuadrático medio (MSEL).\n\nAl adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n\n**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n\n"]},{"cell_type":"markdown","id":"6ac8cd95-2d32-41aa-81c2-7fce000a3aa0","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Predicciones lineales\n\n"]},{"cell_type":"markdown","id":"69b348be-b261-4fd3-9ba1-ee53ddbb996d","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Breve recordatorio sobre las proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"a1008502-4fa6-4a7d-80ce-37e7415cf417","metadata":{},"source":["Solo recordaremos que como\n\n<div class=\"org-center\">\n<p>\n<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n</p>\n</div>\n\n-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n\nLa proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n\nEn el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n\n"]},{"cell_type":"markdown","id":"4a59088b-3445-4a76-9523-5460e31fe92f","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n\n"]},{"cell_type":"markdown","id":"aac5e793-6145-43ad-973c-cbbc11a5d952","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Las predicciones lineales son proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"6e2287fd-851f-4b19-91f3-eb1df87c9d6b","metadata":{},"source":["La forma general de un predictor *lineal* es \n$$g_t(\\ell)\n\\;=\\; \n{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n\\;=\\; \n\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n$$\ndonde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"a83b32c1-9f16-4276-93fc-d7c792296100","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n$$\nE\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n$$\n\n"]},{"cell_type":"markdown","id":"0dff5cbd-1081-4fe9-9e9d-1aa738f15f94","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n\n"]},{"cell_type":"markdown","id":"af0dd129-f7cf-41cc-990b-9dcf8c8c5147","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n\nEs habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n\nCuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n\n"]},{"cell_type":"markdown","id":"63d2428f-69e6-48f7-b9f2-923387b06336","metadata":{"slideshow":{"slide_type":"skip"}},"source":["(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n\n"]},{"cell_type":"markdown","id":"bd49b247-c573-4f21-b212-6ae66f428d08","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n$$\nMSE \\leq MSEL.\n$$\nLa igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n\n"]},{"cell_type":"markdown","id":"145d61e3-670b-4354-b031-9b724a96657f","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n\n"]},{"cell_type":"markdown","id":"6e5aec7e-780f-4cce-852d-2a057b636406","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n\n"]},{"cell_type":"markdown","id":"9df34538-a7aa-47b9-97ef-82a9292cab80","metadata":{},"source":["Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n\nPara $\\ell > 0$,\n\n-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n\n-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n    \n    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n\n"]},{"cell_type":"markdown","id":"ce9afb5e-3e59-406e-be11-ae944e0d771e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n\n-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n\nAdemás para cualquier índice $j$: \n\n-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n\nTras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n\n"]},{"cell_type":"markdown","id":"b52fc675-b522-4297-b54d-45b6b2cb9d6a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Cálculo de predicciones de procesos ARIMA\n\n"]},{"cell_type":"markdown","id":"494d974b-b539-47bf-bb5e-21f9c6516f43","metadata":{},"source":["Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n$$\n\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n\\quad t\\in\\mathbb{N}.\n$$\ndonde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n\nDenotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n$$\n\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n$$ \nY denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\nEntonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n$$\n\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n$$\n\n"]},{"cell_type":"markdown","id":"1ec611c9-08d9-4e82-883d-a8f5135f7248","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Debido a la simple estructura de los modelos ARIMA, tenemos que\n\n\\begin{equation}\nY_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n\\label{eqn:ARMA-en-t}\n\\end{equation}\n\nDe \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n\n\\begin{equation}\nY_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l}\n\\end{equation}\n\nDividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n\n\\begin{equation}\nY_{t+\\ell} = \nc + \n\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \nU_{t+\\ell} + \n\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l-bis}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"6d5ba4fd-ccb4-4fb1-9e01-3bf5f332aae5","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n\n\\begin{equation}\nY_{t+\\ell} = \n\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n\\;\\text{ con } \\ell\\geq1.\n\\label{eqn:ARMA-en-t+l-bis-notas}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"695d4c0c-1847-4077-bd98-8b3c3537f9ef","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n\\label{eqn:predicion-recursiva-ARMA}\n\\end{equation}\n\ndonde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n\nA partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n\n"]},{"cell_type":"markdown","id":"564d5406-1582-417c-8a3d-8fc8bc2282d0","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Ejemplos\n\n"]},{"cell_type":"markdown","id":"6b2167ba-2df4-4a8f-bff3-73551a7b326e","metadata":{},"source":["#### ARMA(1,1)\n\n"]},{"cell_type":"markdown","id":"e519c674-51b0-460d-ba9c-2a59eb2853c0","metadata":{},"source":["$$\nY_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n$$\nla ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n\n\\begin{equation}\n  \\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t \\qquad \\text{cuando } \\ell=1 \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}\n  \\label{eqn:predicion-recursiva-ARMA11}\n\\end{equation}\n\ndonde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n\n"]},{"cell_type":"markdown","id":"2652479d-d3a2-4c8d-acb3-fef47b006917","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n-   **Para $\\ell=2$:** \\begin{align*}\n    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n    \\end{align*}\n-   **Para $\\ell=3$:** \\begin{align*}\n    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n    \\end{align*}\n-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n\nComo $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n\n"]},{"cell_type":"markdown","id":"880d0414-de91-4f87-bdc4-8dcef4b5e87a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### AR(1)\n\n"]},{"cell_type":"markdown","id":"b8876b88-ab39-4fe6-a4b4-3cc8758a9873","metadata":{},"source":["Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n$$\n\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionAR1.png)\n\n"]},{"cell_type":"markdown","id":"b1cdb6d1-9a21-4b12-a3a7-bd60269471af","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### MA(1)\n\n"]},{"cell_type":"markdown","id":"4f624be0-685a-49a4-bf13-4d8cc7982c61","metadata":{},"source":["De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionMA1.png)\n\n"]},{"cell_type":"markdown","id":"eb36f7fa-0a23-4385-bc40-9508745e836c","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n\n"]},{"cell_type":"markdown","id":"4f9b1453-c890-49d9-8ed9-eb6c8ee9c01f","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio sin deriva\n\n"]},{"cell_type":"markdown","id":"3963602f-91c7-4cff-a9aa-0cb51e326dcd","metadata":{},"source":["Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRW.png)\n\n"]},{"cell_type":"markdown","id":"d8c5e197-e093-4f2d-8945-d82305000a49","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio con deriva\n\n"]},{"cell_type":"markdown","id":"82c31697-2f4c-4f04-96ed-676e3740c3b8","metadata":{},"source":["Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRWCD.png)\n\n"]},{"cell_type":"markdown","id":"e5a3cf77-b653-4005-8e33-80710e42c892","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1)\n\n"]},{"cell_type":"markdown","id":"0458d823-a0c1-4bd0-b2a0-1cdad3808d28","metadata":{},"source":["Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionIMA.png)\n\n"]},{"cell_type":"markdown","id":"5dd8ffe7-8c5c-4d49-8476-92dc9e6d7735","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1) estacional\n\n"]},{"cell_type":"markdown","id":"a5ae1157-8659-49fa-8cee-892bc279bbc4","metadata":{},"source":["Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n$$\\text{predicción de cada mes }\n  \\begin{cases}\n    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\Theta_1 U_{t+j-12-1}. \\\\\n    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n  \\end{cases}$$\nDesde el segundo año, cada predicción es como la del mismo mes del año anterior.\n\n![img](./img/figurasGretl/prediccionSIMA.png)\n\n"]},{"cell_type":"markdown","id":"a8582c5f-ebce-4595-88e3-995427a10ba2","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Varianza de las predicciones lineales\n\n"]},{"cell_type":"markdown","id":"e0f0851b-3233-40e9-9c69-332d31c16997","metadata":{},"source":["*Consideremos primero modelos lineales, causales y estacionarios*.\n\n"]},{"cell_type":"markdown","id":"48d79717-1724-41d7-9914-08ed6ed139cd","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n$$\nY_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n$$\nLa predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n\\label{eqn:predicion-MA-infinito}\n\\end{equation}\n\nRestando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n$$\ne_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n$$\ncuya esperanza es cero. \n\nAsumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n\n$$\nE\\Big[e_t(\\ell)^2\\Big]=\nVar(e_t(\\ell))=\n%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n$$\ndonde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"9cb42623-f66e-41ff-be8c-53b8dcb3c7b6","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n\n-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n\n-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n\n-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n\nPor ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n$$\n\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n$$\nSi $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n\n-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n\n"]},{"cell_type":"markdown","id":"0b9043b5-9e3c-4d99-bbf1-e60fba378063","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Intervalos de confianza para las previsiones\n\n"]},{"cell_type":"markdown","id":"4de2e103-1696-40b7-8927-e4b63255529c","metadata":{},"source":["Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n\nComo $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \ntendremos que \n$$\nY_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n$$\ndonde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n\nEn modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n\n"]},{"cell_type":"markdown","id":"b9f24f28-b17e-4ea5-b892-547b51d2363f","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Adaptación de las predicciones a las nuevas observaciones\n\n"]},{"cell_type":"markdown","id":"5037bef5-4863-44b6-8c4d-43a609d78f22","metadata":{},"source":["Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\nSegún \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n$$\n\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n$$\nComparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n\nLa nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\psi_{\\ell-1} U_{t+1} + \\psi_\\ell U_{t}  + \\psi_{\\ell+1} U_{t-1} + \\ldots\n$$\nRestando las dos previsiones, tenemos que:\n$$\n\\widehat{Y_{t+\\ell|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n$$\nAl observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n%, \\tag{8.23}\n$$\n(las predicciones se revisan sumando un múltiplo del último error de predicción).\n\nSi no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n\n"]},{"cell_type":"markdown","id":"fdcfb0ef-614e-44e8-82dd-1cbb7ca581a6","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Medidas del error de previsión\n\n"]},{"cell_type":"markdown","id":"05a41cf7-436a-48a4-9d15-92e9771b2114","metadata":{},"source":["-   **Error Medio (ME):** $= \\frac{1}{T} \\sum_{t=1}^T e_t$\n    \n    Promedio de los errores de previsión.\n\n-   **Raíz del Error Cuadrático Medio (RMSE):** $= \\sqrt{\\frac{1}{T} \\sum_{t=1}^T e_t^2}$\n    \n    Es la norma euclídea de vector de errores.\n\n-   **Error Absoluto Medio (MAE):** $= \\frac{1}{T} \\sum_{t=1}^T |e_t|$\n    \n    Promedio de los errores de previsión en términos absolutos\n\n-   **Error Porcentual Medio (MPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\frac{e_t}{y_t}$\n    \n    Promedio de los errores de previsión porcentuales\n\n-   **Error Porcentual Absoluto Medio (MAPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\left|\\frac{e_t}{y_t}\\right|$\n    \n    Promedio del valor absoluto de los errores porcentuales\n\nGretl muestra otros estadísticos adicionales (véase el manual).\n\n"]},{"cell_type":"markdown","id":"2c31a33d-83a4-4267-8a18-ab037d720dc8","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Modelos con logaritmos\n\n"]},{"cell_type":"markdown","id":"7c355929-dd61-45ed-b780-0fdbccd36a6e","metadata":{},"source":["Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\nSi\n$$\nY_{t+\\ell} = \\ln(X_{t+\\ell})\n$$\nentonces la previsión puntual  de la serie original es \n$$\n\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n$$\nY el Intervalo de confianza\n$$\nI_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n$$\ndonde \n\n\\begin{align*}\na = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\nb = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n\\end{align*}\n\nque es no simétrico alrededor de la previsión.\n\n"]},{"cell_type":"markdown","id":"ede8318a-8696-41f6-b3f6-362df38f97df","metadata":{"slideshow":{"slide_type":"skip"}},"source":["## Nota sobre subespacio sobre el que proyectamos al prever\n\n"]},{"cell_type":"markdown","id":"93a7e749-66a3-4e77-99c7-22134a72d703","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n$$\nY_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n$$\ndonde:\n\n-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n    $$\n      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n      $$\n\nBajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n$$\n\\mathcal{H}_{Y_t} \n\\;=\\;\n\\mathcal{H}_{U_t} \n$$\ndonde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \nLa \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n\n**Demostración:**\n\n1.  **Primera inclusión** $\\subseteq$:\n    \n    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n    $$\n       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n       $$\n    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n    $$\n       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\subseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n2.  **Segunda inclusión** $\\supseteq$:\n    \n    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n    $$\n       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n       $$\n    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n    $$\n       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\supseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}