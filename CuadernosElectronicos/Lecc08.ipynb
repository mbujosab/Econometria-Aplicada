{"cells":[{"cell_type":"markdown","id":"7fabb0ea-734c-40c7-84ad-fac5d4d01b66","metadata":{},"source":"Lección 8. Predicción de modelos ARIMA\n======================================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"9809223c-ad40-4ff4-8a5a-55f926625622","metadata":{},"source":["<div class=\"abstract\" id=\"org280e1de\">\n<p>\nEsta lección se centra en la predicción de modelos de series temporales univariantes.  \n</p>\n\n</div>\n\n-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n\n"]},{"cell_type":"markdown","id":"3c680b48-5def-4ee7-9875-de97cea30928","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python y creación de directorios auxiliares\n\n"]},{"cell_type":"code","execution_count":1,"id":"07ced5be-49a4-450b-b232-f796b925425e","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=False)\nimport matplotlib.pyplot as plt   # data visualization\nimport dataframe_image as dfi   # export tables as .png"]},{"cell_type":"markdown","id":"5fbf24a3-de67-4afa-ab83-52c1a84a42f5","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Directorio auxiliar para albergar las figuras de la lección:\n\n"]},{"cell_type":"markdown","id":"55c5b9b4-dace-49c2-b4b5-1873ca0fd1bb","metadata":{"slideshow":{"slide_type":"skip"}},"source":["para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n\n"]},{"cell_type":"code","execution_count":1,"id":"020e2502-706a-4a90-aba9-b2972040eb4b","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\nimport os\nos.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"]},{"cell_type":"markdown","id":"e4acd58e-18e8-47e5-b873-daae424a8817","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n\n"]},{"cell_type":"markdown","id":"af86cf89-583f-405d-8b6d-fe97615568ad","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Cargamos las funciones auxiliares (véase la carpeta `src/`)\n\n"]},{"cell_type":"code","execution_count":1,"id":"7dc8b084-bc23-42e4-8f61-84d7e9e4c3ab","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n%run -i ./src/analisis_armas.py"]},{"cell_type":"markdown","id":"df165dc9-c060-4dc7-92c1-387b6e6fcb2c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Introducción\n\n"]},{"cell_type":"markdown","id":"6b27a880-e485-40e6-9fc9-42064b1b346f","metadata":{},"source":["Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n\n-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n    \n    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n\n**Los modelos lineales facilitan el cálculo de estos tres aspectos**.\n\n"]},{"cell_type":"markdown","id":"5070ff88-7bd5-487b-bc9c-2783d6627e5f","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Predicciones de error cuadrático medio mínimo\n\n"]},{"cell_type":"markdown","id":"8a688cde-1a41-45b8-8ed6-1af4f2d7215d","metadata":{},"source":["Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n\nNos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n\n"]},{"cell_type":"markdown","id":"a2304483-1079-4b5c-8e5e-d82179d4980c","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n$$\nMSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n$$\n\n"]},{"cell_type":"markdown","id":"6ae38785-c791-4536-bd53-74eebaff45c0","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del error cuadrático $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n\n"]},{"cell_type":"markdown","id":"01070355-7112-4c29-89bc-f343048b9b13","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n$$\nE\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n$$\n\n"]},{"cell_type":"markdown","id":"abf34ef6-e601-4ca8-ad36-4f307cdc95dc","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n\n"]},{"cell_type":"markdown","id":"0115b7ad-0352-485b-9492-6da157a68447","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n\nComo solución alternativa podemos limitarnos a buscar predictores entre:\n\n-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n-   para seleccionar aquella con menor error cuadrático medio (MSEL).\n\nAl adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n\n**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n\n"]},{"cell_type":"markdown","id":"d69438bb-1c8d-4a6c-84c2-b7b4cd5fbd7b","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Predicciones lineales\n\n"]},{"cell_type":"markdown","id":"98ab83ac-315a-49d9-b9a9-164bdeac8f24","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Breve recordatorio sobre las proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"a46aa9bc-8a80-455a-a84d-8f3c0c935d68","metadata":{},"source":["Solo recordaremos que como\n\n<div class=\"org-center\">\n<p>\n<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n</p>\n</div>\n\n-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n\nLa proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n\nEn el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n\n"]},{"cell_type":"markdown","id":"64cc36c9-c0a2-4a16-9d04-f583dfa9fb7c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n\n"]},{"cell_type":"markdown","id":"c29bc038-50be-4064-82bb-57a7627b35c6","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Las predicciones lineales son proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"d6c9f933-db31-4f90-8cbc-ee828acd9d73","metadata":{},"source":["La forma general de un predictor *lineal* es $\\mathcal{h}$\n$$\ng_t(\\ell)\n\\;=\\; \n{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n\\;=\\; \n\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n$$\ndonde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"6b734af2-2d4f-47f3-9b01-5d2336855ddd","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n$$\nE\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n$$\n\n"]},{"cell_type":"markdown","id":"c2d9ad12-c37f-47fa-bf3c-e166b9c60984","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n\n"]},{"cell_type":"markdown","id":"8ad0cc0f-db31-4386-a33e-2bf9f7ca359d","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n\nEs habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n\nCuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n\n"]},{"cell_type":"markdown","id":"1894e248-cac1-446e-83e7-a3ce4b1959f0","metadata":{"slideshow":{"slide_type":"skip"}},"source":["(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n\nRestringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n$$\nMSE \\leq MSEL.\n$$\nLa igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n\nLos resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n\n"]},{"cell_type":"markdown","id":"e290fc25-a7bc-49dc-abcc-cd07d5bbb6f8","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n\n"]},{"cell_type":"markdown","id":"ca271b81-c454-4f5e-b4e7-31263f1fb7a6","metadata":{},"source":["Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n\nPara $\\ell > 0$,\n\n-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n\n-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n    \n    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n\n"]},{"cell_type":"markdown","id":"9e6d8182-0b79-4b1e-beb7-c1219aca9383","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n\n-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n\nAdemás para cualquier índice $j$: \n\n-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n\nTras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n\n"]},{"cell_type":"markdown","id":"3b48c43d-2395-4afc-ad8c-e4230cd82f6c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Cálculo de predicciones de procesos ARIMA\n\n"]},{"cell_type":"markdown","id":"88458bee-5ef0-462c-98d7-58e1d2b54ec5","metadata":{},"source":["Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n$$\n\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n\\quad t\\in\\mathbb{N}.\n$$\ndonde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n\nDenotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n$$\n\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n$$ \nY denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\nEntonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n$$\n\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n$$\n\n"]},{"cell_type":"markdown","id":"2ae2bd7b-e50c-49fa-8946-7f06542a30c1","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Debido a la simple estructura de los modelos ARIMA, tenemos que\n\n\\begin{equation}\nY_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n\\label{eqn:ARMA-en-t}\n\\end{equation}\n\nDe \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n\n\\begin{equation}\nY_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l}\n\\end{equation}\n\nDividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n\n\\begin{equation}\nY_{t+\\ell} = \nc + \n\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \nU_{t+\\ell} + \n\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l-bis}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"63819a41-2bbe-431c-954e-9f84c69288f1","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n\n\\begin{equation}\nY_{t+\\ell} = \n\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n\\;\\text{ con } \\ell\\geq1.\n\\label{eqn:ARMA-en-t+l-bis-notas}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"958e8160-acc6-4929-b194-c26d6a7358c5","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n\\label{eqn:predicion-recursiva-ARMA}\n\\end{equation}\n\ndonde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n\nA partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n\n"]},{"cell_type":"markdown","id":"29d0ea18-6ca3-4d91-8c5e-38221c20eaab","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Ejemplos\n\n"]},{"cell_type":"markdown","id":"cc66e105-eec7-4377-a8f0-a568b73d9e20","metadata":{},"source":["#### ARMA(1,1)\n\n"]},{"cell_type":"markdown","id":"2dba7606-2bf8-4cf5-9dcb-48c3af1c4012","metadata":{},"source":["$$\nY_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n$$\nla ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n\n\\begin{equation}\n  \\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t. \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}\n  \\label{eqn:predicion-recursiva-ARMA11}\n\\end{equation}\n\ndonde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n\n"]},{"cell_type":"markdown","id":"ddc1b6f2-6469-4981-839e-7bbc840d1bac","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n-   **Para $\\ell=2$:** \\begin{align*}\n    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n    \\end{align*}\n-   **Para $\\ell=3$:** \\begin{align*}\n    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n    \\end{align*}\n-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n\nComo $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n\n"]},{"cell_type":"markdown","id":"8af96057-ee67-4639-bbab-204131e09af0","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### AR(1)\n\n"]},{"cell_type":"markdown","id":"8bf60589-aff0-46aa-b0a8-a810f49bd24f","metadata":{},"source":["Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n$$\n\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionAR1.png)\n\n"]},{"cell_type":"markdown","id":"7cd08b52-18ec-4ba1-b5f6-7dd1f875f60d","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### MA(1)\n\n"]},{"cell_type":"markdown","id":"22fca52d-ab6d-493e-857c-d407bed05983","metadata":{},"source":["De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionMA1.png)\n\n"]},{"cell_type":"markdown","id":"d4e465b0-0a4c-46ad-b89e-fd143095ceb4","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n\n"]},{"cell_type":"markdown","id":"133b7491-9678-44f1-b163-2fcae7e2534b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio sin deriva\n\n"]},{"cell_type":"markdown","id":"d509b4ff-9c92-44d2-b36f-f107148ee187","metadata":{},"source":["Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRW.png)\n\n"]},{"cell_type":"markdown","id":"f672aecc-9ae6-42d6-9362-57660fe1e2f0","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio con deriva\n\n"]},{"cell_type":"markdown","id":"d60a60e1-ece9-4d09-b2d2-b8bf6a3c0957","metadata":{},"source":["Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRWCD.png)\n\n"]},{"cell_type":"markdown","id":"22487de6-5fe3-41de-bb90-d38f44fcc55e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1)\n\n"]},{"cell_type":"markdown","id":"a8b46993-2ad7-467e-b7bf-b9961b8bbba2","metadata":{},"source":["Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionIMA.png)\n\n"]},{"cell_type":"markdown","id":"f0cd2810-eeb1-49e4-bc51-0c8c95f72d6b","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1) estacional\n\n"]},{"cell_type":"markdown","id":"45713db7-6a51-437e-9832-c3a18e279796","metadata":{},"source":["Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n$$\\text{predicción de cada mes }\n  \\begin{cases}\n    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\theta_1 U_{t+j-12-1}. \\\\\n    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n  \\end{cases}$$\nDesde el segundo año, cada predicción es como la del mismo mes del año anterior.\n\n![img](./img/figurasGretl/prediccionSIMA.png)\n\n"]},{"cell_type":"markdown","id":"cc12e5fa-7f3f-445f-bb4e-a0037aaa91ca","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Varianza de las predicciones lineales\n\n"]},{"cell_type":"markdown","id":"73adb2c1-dcd4-4abd-9cf2-212bdcfed875","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Consideremos primero modelos lineales, causales y estacionarios*.\n\n"]},{"cell_type":"markdown","id":"ce666c06-423d-4d09-9556-ba49d2bc3be5","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n$$\nY_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n$$\nLa predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n\\label{eqn:predicion-MA-infinito}\n\\end{equation}\n\nRestando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n$$\ne_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n$$\ncuya esperanza es cero. \n\nAsumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n\n$$\nE\\Big[e_t(\\ell)^2\\Big]=\nVar(e_t(\\ell))=\n%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n$$\ndonde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"6007b66c-9be2-4b13-85ff-0dcc9e2d42b1","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n\n"]},{"cell_type":"markdown","id":"0605cb36-da9e-4e12-972f-01d976495b96","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n\n-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n\n-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n\nPor ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n$$\n\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n$$\nSi $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n\n-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n\n"]},{"cell_type":"markdown","id":"d0286be0-fbf3-4085-8959-f895b6af31d3","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Intervalos de confianza para las previsiones\n\n"]},{"cell_type":"markdown","id":"76f3c148-b65e-42eb-b980-23f2885a0bfb","metadata":{},"source":["Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n\nComo $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \ntendremos que \n$$\nY_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n$$\ndonde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n\nEn modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n\n"]},{"cell_type":"markdown","id":"309eee92-3e71-4804-9506-e575175078f9","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Adaptación de las predicciones a las nuevas observaciones\n\n"]},{"cell_type":"markdown","id":"b414fbf7-72a2-4a30-82b0-40c600ba33fc","metadata":{},"source":["Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\nSegún \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n$$\n\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n$$\nComparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n\nLa nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} = \\psi_{\\ell-1} U_t + \\psi_\\ell U_{t-1} + \\ldots\n$$\nRestando las dos previsiones, tenemos que:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n$$\nAl observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n%, \\tag{8.23}\n$$\n(las predicciones se revisan sumando un múltiplo del último error de predicción).\n\nSi no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n\n"]},{"cell_type":"markdown","id":"5b754282-6861-4bea-8636-cec5351dfeb3","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Modelos con logaritmos\n\n"]},{"cell_type":"markdown","id":"4a288176-725d-4569-9123-670f5ce1ce43","metadata":{},"source":["Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\nSi\n$$\nY_{t+\\ell} = \\ln(X_{t+\\ell})\n$$\nentonces la previsión puntual  de la serie original es \n$$\n\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n$$\nY el Intervalo de confianza\n$$\nI_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n$$\ndonde \n\n\\begin{align*}\na = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\nb = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n\\end{align*}\n\nque es no simétrico alrededor de la previsión.\n\n"]},{"cell_type":"markdown","id":"02f685ca-15e6-47b1-910d-670e92992ee5","metadata":{"slideshow":{"slide_type":"skip"}},"source":["## Nota sobre subespacio sobre el que proyectamos al prever\n\n"]},{"cell_type":"markdown","id":"15652d8a-0bea-41dd-a668-7e5820db7b37","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n$$\nY_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n$$\ndonde:\n\n-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n    $$\n      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n      $$\n\nBajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n$$\n\\mathcal{H}_{Y_t} \n\\;=\\;\n\\mathcal{H}_{U_t} \n$$\ndonde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \nLa \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n\n**Demostración:**\n\n1.  **Primera inclusión** $\\subseteq$:\n    \n    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n    $$\n       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n       $$\n    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n    $$\n       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\subseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n2.  **Segunda inclusión** $\\supseteq$:\n    \n    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n    $$\n       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n       $$\n    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n    $$\n       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\supseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}