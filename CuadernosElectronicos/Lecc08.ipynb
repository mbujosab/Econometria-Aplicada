{"cells":[{"cell_type":"markdown","id":"bf94ece8-1d4b-48a7-ba6a-4410171de8b1","metadata":{},"source":"Lección 8. Predicción de modelos ARIMA\n======================================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"3bea07f1-d9ee-419b-b160-264de227c94b","metadata":{},"source":["<div class=\"abstract\" id=\"org89e16a1\">\n<p>\nEsta lección se centra en la predicción de modelos de series temporales univariantes.  \n</p>\n\n</div>\n\n-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n\n"]},{"cell_type":"markdown","id":"0e31f76c-34c7-4326-926e-dc6781f1da3e","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python y creación de directorios auxiliares\n\n"]},{"cell_type":"code","execution_count":1,"id":"1a95f0d5-6e5a-4c44-a6ea-6f8935eee7c0","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=False)\nimport matplotlib.pyplot as plt   # data visualization\nimport dataframe_image as dfi   # export tables as .png"]},{"cell_type":"markdown","id":"3128fa7c-1861-4269-99ab-1c86290e82aa","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Directorio auxiliar para albergar las figuras de la lección:\n\n"]},{"cell_type":"markdown","id":"0100956e-c38b-4a1f-9f10-d9c289bfed38","metadata":{"slideshow":{"slide_type":"skip"}},"source":["para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n\n"]},{"cell_type":"code","execution_count":1,"id":"4add2581-5c1d-438b-9db1-cb084dba97b7","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\nimport os\nos.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"]},{"cell_type":"markdown","id":"278bacbb-f963-402b-9df9-32a44ecc2943","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n\n"]},{"cell_type":"markdown","id":"bc3651d8-6140-4059-bf42-9357e4f8771a","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Cargamos las funciones auxiliares (véase la carpeta `src/`)\n\n"]},{"cell_type":"code","execution_count":1,"id":"2ca61ae1-4b17-4891-b1a3-032e2a48be00","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n%run -i ./src/analisis_armas.py"]},{"cell_type":"markdown","id":"91c15389-f532-4c88-96a8-1df245fcaa36","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Introducción\n\n"]},{"cell_type":"markdown","id":"9f5186eb-9b92-457e-b167-ee13b42eb5d6","metadata":{},"source":["Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n\n-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n    \n    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n\n**Los modelos lineales facilitan solventar estas tres necesidades**.\n\n"]},{"cell_type":"markdown","id":"66791873-af4a-4fb7-87a3-6ba2bc5e09b0","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Predicciones de error cuadrático medio mínimo\n\n"]},{"cell_type":"markdown","id":"0485ee1f-6ba2-45be-b242-8e218d5cfeed","metadata":{},"source":["Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n\nNos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n\n"]},{"cell_type":"markdown","id":"ae1fe346-d78e-4168-a469-c4e796383b4d","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n$$\nMSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n$$\n\n"]},{"cell_type":"markdown","id":"b9996e33-211e-4fda-91a9-b7cb932c5e6d","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del cuadrado del error de predicción $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n\n"]},{"cell_type":"markdown","id":"69647f14-e4af-444e-bbeb-2b47f841ba5e","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n$$\nE\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n$$\n\n"]},{"cell_type":"markdown","id":"7c7a0f59-5228-41fc-8b65-633d945391a0","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n\n"]},{"cell_type":"markdown","id":"4aa893c8-fa99-490b-8287-dabde4860910","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n\nComo solución alternativa podemos limitarnos a buscar predictores entre:\n\n-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n-   y seleccionar aquella con menor error cuadrático medio (MSEL).\n\nAl adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n\n**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n\n"]},{"cell_type":"markdown","id":"7fc8fadf-456d-45b6-bf16-78afccf7ba69","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Predicciones lineales\n\n"]},{"cell_type":"markdown","id":"db0ab019-59d1-498f-a303-6c780cc70949","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Breve recordatorio sobre las proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"aac10ef5-167e-424f-b4f5-2e4aae8ae66c","metadata":{},"source":["Solo recordaremos que como\n\n<div class=\"org-center\">\n<p>\n<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n</p>\n</div>\n\n-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n\nLa proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n\nEn el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n\n"]},{"cell_type":"markdown","id":"78da064f-4518-4746-bb90-1c1575696eb9","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n\n"]},{"cell_type":"markdown","id":"8bf047c6-4ca1-4105-85ef-af154d35fe9f","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Las predicciones lineales son proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"c46254f1-e583-4144-a106-3b79b1f441d2","metadata":{},"source":["La forma general de un predictor *lineal* es \n$$g_t(\\ell)\n\\;=\\; \n{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n\\;=\\; \n\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n$$\ndonde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"39b18671-81cf-455a-afaa-a93e33815e38","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n$$\nE\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n$$\n\n"]},{"cell_type":"markdown","id":"89dc3956-6730-4258-a13a-f5cf389667ee","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n\n"]},{"cell_type":"markdown","id":"ecf4cb5a-ffe5-4d6b-9bdd-4c6f68ed2465","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n\nEs habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n\nCuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n\n"]},{"cell_type":"markdown","id":"f79e60ac-af4d-48aa-b912-7e3aeec5d7bd","metadata":{"slideshow":{"slide_type":"skip"}},"source":["(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n\n"]},{"cell_type":"markdown","id":"a7f90d02-6d33-40b5-935d-040cba143b6b","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n$$\nMSE \\leq MSEL.\n$$\nLa igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n\n"]},{"cell_type":"markdown","id":"8e71a4cf-b8a6-4e0d-a5f1-6bb7bab1fa8c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n\n"]},{"cell_type":"markdown","id":"3db581dc-6e06-4fa4-b8b1-2075ee2576ca","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n\n"]},{"cell_type":"markdown","id":"e4920d01-949e-42b8-8f04-fb75ca2a16f7","metadata":{},"source":["Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n\nPara $\\ell > 0$,\n\n-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n\n-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n    \n    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n\n"]},{"cell_type":"markdown","id":"be3c3520-c99e-4187-82b4-1b7afa6bfecc","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n\n-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n\nAdemás para cualquier índice $j$: \n\n-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n\nTras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n\n"]},{"cell_type":"markdown","id":"9233133e-1fe2-4bdc-847b-2cf9e4617f4a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Cálculo de predicciones de procesos ARIMA\n\n"]},{"cell_type":"markdown","id":"053892ed-16b3-4a39-8ab2-b51b40fe720f","metadata":{},"source":["Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n$$\n\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n\\quad t\\in\\mathbb{N}.\n$$\ndonde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n\nDenotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n$$\n\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n$$ \nY denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\nEntonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n$$\n\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n$$\n\n"]},{"cell_type":"markdown","id":"f132b816-b654-470d-99f9-1ef0d7abc66d","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Debido a la simple estructura de los modelos ARIMA, tenemos que\n\n\\begin{equation}\nY_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n\\label{eqn:ARMA-en-t}\n\\end{equation}\n\nDe \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n\n\\begin{equation}\nY_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l}\n\\end{equation}\n\nDividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n\n\\begin{equation}\nY_{t+\\ell} = \nc + \n\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \nU_{t+\\ell} + \n\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l-bis}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"6f620acf-f861-4cad-9a4b-d5d5a73163e3","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n\n\\begin{equation}\nY_{t+\\ell} = \n\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n\\;\\text{ con } \\ell\\geq1.\n\\label{eqn:ARMA-en-t+l-bis-notas}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"d9a02908-10c3-499e-8256-ee9e79e35870","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n\\label{eqn:predicion-recursiva-ARMA}\n\\end{equation}\n\ndonde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n\nA partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n\n"]},{"cell_type":"markdown","id":"31b924fa-adfd-4cf3-a4f0-d5fd02cb30ab","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Ejemplos\n\n"]},{"cell_type":"markdown","id":"3637d561-dfd5-41a7-82f6-51c55a2d94cf","metadata":{},"source":["#### ARMA(1,1)\n\n"]},{"cell_type":"markdown","id":"5b4b487e-8949-4bc5-838e-f13855617454","metadata":{},"source":["$$\nY_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n$$\nla ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n\n\\begin{equation}\n  \\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t \\qquad \\text{cuando } \\ell=1 \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}\n  \\label{eqn:predicion-recursiva-ARMA11}\n\\end{equation}\n\ndonde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n\n"]},{"cell_type":"markdown","id":"3fa1376f-bbe6-4b49-906e-55c123e2fa92","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n-   **Para $\\ell=2$:** \\begin{align*}\n    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n    \\end{align*}\n-   **Para $\\ell=3$:** \\begin{align*}\n    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n    \\end{align*}\n-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n\nComo $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n\n"]},{"cell_type":"markdown","id":"b8a7b315-2686-48bb-b379-b5aba8f95edc","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### AR(1)\n\n"]},{"cell_type":"markdown","id":"9041bd42-5187-4221-b75b-ca1952b6d574","metadata":{},"source":["Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n$$\n\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionAR1.png)\n\n"]},{"cell_type":"markdown","id":"0b98af1f-7a3a-4f83-8940-4208aca316dc","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### MA(1)\n\n"]},{"cell_type":"markdown","id":"d479b235-d89a-4504-b25b-2b4e2327ef80","metadata":{},"source":["De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionMA1.png)\n\n"]},{"cell_type":"markdown","id":"6e517494-6cd6-4c08-9eb0-d4d219fd27b7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n\n"]},{"cell_type":"markdown","id":"6425ddad-0883-466b-8e1a-50f25bd454a7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio sin deriva\n\n"]},{"cell_type":"markdown","id":"b04f9deb-9438-4feb-b202-f02ea4d1fb0c","metadata":{},"source":["Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRW.png)\n\n"]},{"cell_type":"markdown","id":"4770ca9b-b639-433c-9dca-24870bf955bc","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio con deriva\n\n"]},{"cell_type":"markdown","id":"49492ac8-623d-4108-afff-2d583dec7f51","metadata":{},"source":["Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRWCD.png)\n\n"]},{"cell_type":"markdown","id":"894b0746-bab1-4894-92d4-ef9dcc7fd435","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1)\n\n"]},{"cell_type":"markdown","id":"03b562c5-3977-4ee8-8883-6d086cbcf8c3","metadata":{},"source":["Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionIMA.png)\n\n"]},{"cell_type":"markdown","id":"d37141a9-8902-4778-89a9-ff8165a198e4","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1) estacional\n\n"]},{"cell_type":"markdown","id":"54d8b5e9-574a-4292-a8bb-63872cf377a2","metadata":{},"source":["Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n$$\\text{predicción de cada mes }\n  \\begin{cases}\n    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\Theta_1 U_{t+j-12-1}. \\\\\n    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n  \\end{cases}$$\nDesde el segundo año, cada predicción es como la del mismo mes del año anterior.\n\n![img](./img/figurasGretl/prediccionSIMA.png)\n\n"]},{"cell_type":"markdown","id":"27533175-5473-4477-a078-8e0580f4dd73","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Varianza de las predicciones lineales\n\n"]},{"cell_type":"markdown","id":"2ac0a4b1-4426-4b53-9e95-61746d64cfd9","metadata":{},"source":["*Consideremos primero modelos lineales, causales y estacionarios*.\n\n"]},{"cell_type":"markdown","id":"43c1fffe-92f8-4221-b819-10df31b6e5dc","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n$$\nY_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n$$\nLa predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n\\label{eqn:predicion-MA-infinito}\n\\end{equation}\n\nRestando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n$$\ne_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n$$\ncuya esperanza es cero. \n\nAsumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n\n$$\nE\\Big[e_t(\\ell)^2\\Big]=\nVar(e_t(\\ell))=\n%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n$$\ndonde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"4976a6d2-320d-4f99-a261-b53d18b91d07","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n\n-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n\n-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n\n-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n\nPor ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n$$\n\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n$$\nSi $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n\n-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n\n"]},{"cell_type":"markdown","id":"97e4fade-0764-446e-841b-2a0324c5e6b4","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Intervalos de confianza para las previsiones\n\n"]},{"cell_type":"markdown","id":"f4375516-b100-4772-9c7c-2b6280eb1daf","metadata":{},"source":["Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n\nComo $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \ntendremos que \n$$\nY_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n$$\ndonde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n\nEn modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n\n"]},{"cell_type":"markdown","id":"c22f39a0-b3e1-489a-9bf2-14504864bc6e","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Adaptación de las predicciones a las nuevas observaciones\n\n"]},{"cell_type":"markdown","id":"4043cfad-2180-4c02-a71a-94571a108730","metadata":{},"source":["Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\nSegún \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n$$\n\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n$$\nComparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n\nLa nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\psi_{\\ell-1} U_{t+1} + \\psi_\\ell U_{t}  + \\psi_{\\ell+1} U_{t-1} + \\ldots\n$$\nRestando las dos previsiones, tenemos que:\n$$\n\\widehat{Y_{t+\\ell|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n$$\nAl observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n$$\n\\widehat{Y_{t+\\ell|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n%, \\tag{8.23}\n$$\n(las predicciones se revisan sumando un múltiplo del último error de predicción).\n\nSi no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n\n"]},{"cell_type":"markdown","id":"05b7bf96-0b82-4bbd-89fb-91dc8971646a","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Medidas del error de previsión\n\n"]},{"cell_type":"markdown","id":"a9290c05-d9ba-44f4-af5a-2c1683f8053d","metadata":{},"source":["-   **Error Medio (ME):** $= \\frac{1}{T} \\sum_{t=1}^T e_t$\n    \n    Promedio de los errores de previsión.\n\n-   **Raíz del Error Cuadrático Medio (RMSE):** $= \\sqrt{\\frac{1}{T} \\sum_{t=1}^T e_t^2}$\n    \n    Es la norma euclídea de vector de errores.\n\n-   **Error Absoluto Medio (MAE):** $= \\frac{1}{T} \\sum_{t=1}^T |e_t|$\n    \n    Promedio de los errores de previsión en términos absolutos\n\n-   **Error Porcentual Medio (MPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\frac{e_t}{y_t}$\n    \n    Promedio de los errores de previsión porcentuales\n\n-   **Error Porcentual Absoluto Medio (MAPE):** $= \\frac{1}{T} \\sum_{t=1}^T 100\\, \\left|\\frac{e_t}{y_t}\\right|$\n    \n    Promedio del valor absoluto de los errores porcentuales\n\nGretl muestra otros estadísticos adicionales (véase el manual).\n\n"]},{"cell_type":"markdown","id":"aba94849-9297-4459-aac8-f816a3061312","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Modelos con logaritmos\n\n"]},{"cell_type":"markdown","id":"cf9b7b73-f03a-46d7-ab5f-94d22c1d15b6","metadata":{},"source":["Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\nSi\n$$\nY_{t+\\ell} = \\ln(X_{t+\\ell})\n$$\nentonces la previsión puntual  de la serie original es \n$$\n\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n$$\nY el Intervalo de confianza\n$$\nI_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n$$\ndonde \n\n\\begin{align*}\na = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\nb = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n\\end{align*}\n\nque es no simétrico alrededor de la previsión.\n\n"]},{"cell_type":"markdown","id":"12fb8291-1ebb-42a3-81cd-7228ea65eea0","metadata":{"slideshow":{"slide_type":"skip"}},"source":["## Nota sobre subespacio sobre el que proyectamos al prever\n\n"]},{"cell_type":"markdown","id":"922ac21c-9e5a-4370-8b4f-e23e48df24dc","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n$$\nY_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n$$\ndonde:\n\n-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n    $$\n      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n      $$\n\nBajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n$$\n\\mathcal{H}_{Y_t} \n\\;=\\;\n\\mathcal{H}_{U_t} \n$$\ndonde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \nLa \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n\n**Demostración:**\n\n1.  **Primera inclusión** $\\subseteq$:\n    \n    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n    $$\n       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n       $$\n    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n    $$\n       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\subseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n2.  **Segunda inclusión** $\\supseteq$:\n    \n    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n    $$\n       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n       $$\n    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n    $$\n       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\supseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}