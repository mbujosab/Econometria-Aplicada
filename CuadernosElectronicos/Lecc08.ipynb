{"cells":[{"cell_type":"markdown","id":"90b7865b-19dc-465e-982c-003f4d1bcfc5","metadata":{},"source":"Lección 8. Predicción de modelos ARIMA\n======================================\n\n**Author:** Marcos Bujosa\n\n"},{"cell_type":"markdown","id":"f2bad8a1-8e72-432f-8959-51eca60028d1","metadata":{},"source":["<div class=\"abstract\" id=\"orgfa3b0e5\">\n<p>\nEsta lección se centra en la predicción de modelos de series temporales univariantes.  \n</p>\n\n</div>\n\n-   ([slides](https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html)) &mdash; ([html](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html)) &mdash; ([pdf](https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf)) &mdash; ([mybinder](https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb))\n\n"]},{"cell_type":"markdown","id":"2f9a3c06-69e2-4006-b671-dc35a2df5b50","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Carga de algunos módulos de python y creación de directorios auxiliares\n\n"]},{"cell_type":"code","execution_count":1,"id":"9d476989-f4a3-4c0c-93bd-603cf36dcba1","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\n# definimos parámetros para mejorar los gráficos\nmpl.rc('text', usetex=False)\nimport matplotlib.pyplot as plt   # data visualization\nimport dataframe_image as dfi   # export tables as .png"]},{"cell_type":"markdown","id":"d46c3465-59d3-4e26-8883-55c50c0a276c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["##### Directorio auxiliar para albergar las figuras de la lección:\n\n"]},{"cell_type":"markdown","id":"ff4d5a6c-4638-44c6-bb17-d378b15ef793","metadata":{"slideshow":{"slide_type":"skip"}},"source":["para publicar la lección como pdf o página web, necesito los gráficos como ficheros `.png` alojados algún directorio específico:\n\n"]},{"cell_type":"code","execution_count":1,"id":"fc3d36fe-61c9-4751-9ad0-5f51c065aa90","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["imagenes_leccion = \"./img/lecc07\" # directorio para las imágenes de la lección\nimport os\nos.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe"]},{"cell_type":"markdown","id":"9bd2562c-93a0-455e-90f0-a90d1e80b27d","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Gráficos para las ACF, PACF y densidades espectrales teóricas\n\n"]},{"cell_type":"markdown","id":"3d7d5b61-60d3-4f81-a0b0-483f8a59e99f","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Cargamos las funciones auxiliares (véase la carpeta `src/`)\n\n"]},{"cell_type":"code","execution_count":1,"id":"24afea6f-612a-44fb-b612-9ce1a625da4a","metadata":{"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n%run -i ./src/analisis_armas.py"]},{"cell_type":"markdown","id":"146052fb-4da2-40ee-ae97-8b1478f9531e","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Introducción\n\n"]},{"cell_type":"markdown","id":"202a455a-383c-4bac-92fd-066f43e8f92e","metadata":{},"source":["Consideremos una serie temporal $\\boldsymbol{y}_1^T=(y_1,\\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : \n\n-   Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.\n-   Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.\n    \n    Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.\n-   Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información.\n\n**Los modelos lineales facilitan el cálculo de estos tres aspectos**.\n\n"]},{"cell_type":"markdown","id":"a8cfd773-eed6-4554-8cbc-169b868b7675","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Predicciones de error cuadrático medio mínimo\n\n"]},{"cell_type":"markdown","id":"aded636f-b73d-4d0f-a334-eb52fcbeb757","metadata":{},"source":["Queremos predecir $Y_{t+\\ell}$ usando una función $g$ de variables aleatorias con índice $n\\leq t$, es decir, una función del conjunto $\\boldsymbol{Y}_{|:t} = \\{Y_n \\mid n \\leq t\\}$.\n\nNos referiremos al conjunto $\\boldsymbol{Y}_{|:t}$ como *\\`\\`el pasado''* de $\\boldsymbol{Y}$.\n\n"]},{"cell_type":"markdown","id":"781febb5-fc7a-4cd1-b070-d814768ebd5a","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Para evaluar diferentes funciones de predicción, estableceremos como criterio *minimizar el error cuadrático medio* (MSE):\n$$\nMSE\\big(Y_{t+\\ell},\\,g_t(\\ell)\\big) = E\\Big(Y_{t+\\ell}-g_t(\\ell)\\Big)^2.\n$$\n\n"]},{"cell_type":"markdown","id":"f9115188-ccff-4b1a-80ae-c8134a6b316c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Preferiremos aquel método $g_t(\\ell)$ que minimiza la esperanza del error cuadrático $Y_{t+\\ell}-g_t(\\ell)$; donde $g_t(\\ell)$ es la función de predicción de ${Y}_{t+\\ell}$, $t$ es el índice de la última variable aleatoria considerada y $\\ell$ representa el horizonte de predicción.\n\n"]},{"cell_type":"markdown","id":"b10df774-d78d-421a-abb3-7de4424507d7","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["La función $g_t(\\ell)$ que minimiza dicho criterio MSE es la **esperanza condicional** de $Y_{t+\\ell}$:\n$$\nE\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big).\n$$\n\n"]},{"cell_type":"markdown","id":"e6a28617-dba2-49c9-a42b-3d0a0f465e03","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.\n\n"]},{"cell_type":"markdown","id":"28ed2845-0305-4022-ae24-e2c1486858c6","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil&hellip; o sencillamente inviable. \n\nComo solución alternativa podemos limitarnos a buscar predictores entre:\n\n-   las funciones de $\\boldsymbol{Y}_{|:t}$ (i.e., funciones \\`\\`del pasado'') que son *<u>lineales</u>*;\n-   para seleccionar aquella con menor error cuadrático medio (MSEL).\n\nAl adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \\geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.\n\n**Pregunta**: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?\n\n"]},{"cell_type":"markdown","id":"d7fcbd79-6e98-4815-9862-8141d6fb7991","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Predicciones lineales\n\n"]},{"cell_type":"markdown","id":"b3ac1831-310e-4b72-8ee3-e391c3a0c282","metadata":{"slideshow":{"slide_type":"skip"}},"source":["#### Breve recordatorio sobre las proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"33d91528-d432-4d90-a01c-9efdcaefc487","metadata":{},"source":["Solo recordaremos que como\n\n<div class=\"org-center\">\n<p>\n<u>la proyección ortogonal de $X$ sobre $\\mathcal{H}$ es el elemento de $\\mathcal{H}$ más <i>próximo</i> a $X$</u>\n</p>\n</div>\n\n-   si $X\\in\\mathcal{H}$, la proyección de $X$ sobre $\\mathcal{H}$ es el propio $X$.\n-   si $Y$ es ortogonal a todo $X\\in\\mathcal{H}$, es decir, si $Y\\perp\\mathcal{H}$, la proyección de $Y$ sobre $\\mathcal{H}$ es cero.\n\nLa proyección ortogonal de $X$ sobre el espacio euclídeo $\\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\\widehat{X}$, es perpendicular a $\\mathcal{H}$. \n\nEn el contexto de la predicción lineal, dicha diferencia $X-\\widehat{X}$ recibirá el nombre de *error de predicción*.\n\n"]},{"cell_type":"markdown","id":"d68a944c-823b-4f6e-898d-8f584b7b61ec","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Puede consultar algunos resultados sobre las proyecciones ortogonales en este [enlace](https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13).*\n\n"]},{"cell_type":"markdown","id":"cc93286a-427d-4378-923e-3349fbb25adb","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Las predicciones lineales son proyecciones ortogonales\n\n"]},{"cell_type":"markdown","id":"5253da3d-8b67-4434-9bd9-587f467b53f7","metadata":{},"source":["La forma general de un predictor *lineal* es $\\mathcal{h}$\n$$\ng_t(\\ell)\n\\;=\\; \n{b_\\ell}_0 Y_t + {b_\\ell}_1 Y_{t-1} + {b_\\ell}_2 Y_{t-2} + \\cdots\n\\;=\\; \n\\boldsymbol{b_\\ell}*(\\boldsymbol{Y}_{|:t})\n\\;=\\; \\boldsymbol{b_\\ell}(B)Y_t,\n$$\ndonde $\\boldsymbol{b_\\ell}$ es una serie formal de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"ff097366-98f1-4982-a89c-8ae2936b5477","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción $\\big(Y_{t+\\ell}-g_t(\\ell)\\big)$ es ortogonal a cada una de las variables aleatorias del pasado del proceso; es decir, cuando\n$$\nE\\Big((Y_{t+\\ell}-\\boldsymbol{b_\\ell}*\\boldsymbol{Y}_{|:t})\\cdot{Y}_{n}\\Big) = \\boldsymbol{0}, \\;\\text{para cualquier } n\\leq t.\n$$\n\n"]},{"cell_type":"markdown","id":"a6e9ca55-07b9-4831-8574-995e1e865224","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Consecuentemente, el mejor predictor lineal (que denotaremos con $\\widehat{Y_{t+\\ell|t}}$) es la proyección ortogonal de $Y_{t+\\ell}$ sobre la *clausura* del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\\ldots$.\n\n"]},{"cell_type":"markdown","id":"7b113788-9caa-4b51-a0a6-0873ec2b0cb8","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Denotando dicha clausura con $\\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\\widehat{Y_{t+\\ell|t}}$, es la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$.\n\nEs habitual referirse a los elementos de $\\mathcal{H}_{Y_t}$ como *variables observadas*; y a todo el espacio $\\mathcal{H}_{Y_t}$ como el *conjunto de información* empleado en la previsión.\n\nCuando $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\\;\\mathcal{H}_{Y_t} = \\mathcal{H}_{U_t}$\n\n"]},{"cell_type":"markdown","id":"557cfdb0-54ab-4e87-818e-4f4954276e32","metadata":{"slideshow":{"slide_type":"skip"}},"source":["(véase la sección [Nota sobre subespacio sobre el que proyectamos al prever](Lecc08.md)).<a href=\"#nil\">[nil]</a>\n\nRestringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.\n$$\nMSE \\leq MSEL.\n$$\nLa igualdad se da solo cuando la esperanza condicional es lineal en $\\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\\big(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t}\\big)$ coincide con la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).\n\nLos resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\\boldsymbol{Y}_{|1:T}=(Y_1,\\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza<a href=\"#nil\">[nil]</a>) sobre el conjunto finito de variables aleatorias del pasado $\\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.\n\n"]},{"cell_type":"markdown","id":"c16d1d16-71a6-43ab-b5dd-52a6016a52f7","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible\n\n"]},{"cell_type":"markdown","id":"58f63f32-119e-4f83-bf0e-529f0355edfe","metadata":{},"source":["Sea $\\boldsymbol{Y}=\\boldsymbol{\\varphi}*\\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.\n\nPara $\\ell > 0$,\n\n-   denominamos *<u>previsión de $Y_{t+\\ell}$ basada en la historia del proceso hasta $Y_t$</u>* a la proyección ortogonal de $Y_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t};\\;$ que denotamos con $\\widehat{Y_{t+\\ell|t}}$.\n\n-   Análogamente, la *\\`\\`previsión $\\widehat{U_{t+\\ell|t}}$ basada en el pasado de $\\boldsymbol{Y}$''* es la proyección ortogonal de $U_{t+\\ell}$ sobre $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$. \n    \n    Fíjese que $\\widehat{U_{t+\\ell|t}}$ <u>siempre es cero</u>; pues el ruido blanco es ortogonal a su pasado, $U_{t+\\ell}\\perp\\mathcal{H}_{U_t},\\;$ por ser incorrelado y con esperanza nula\n\n"]},{"cell_type":"markdown","id":"4fc381e3-eb04-4816-8ae4-12da6a049654","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sin embargo, para los índices $(t-k)$ donde $k \\geq 0$: \n\n-   la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\\mathcal{H}_{Y_t}=\\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son *variables observadas*).\n\nAdemás para cualquier índice $j$: \n\n-   Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\\widehat{c_{j|t}}=c$ dado que $1\\cdot c\\in\\mathcal{H}_{Y_t}$.\n\nTras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.\n\n"]},{"cell_type":"markdown","id":"7e14dbc9-eb93-4dfe-a97e-663663b3151e","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Cálculo de predicciones de procesos ARIMA\n\n"]},{"cell_type":"markdown","id":"df4b0e9e-8800-4d1d-9445-80b2386e8b19","metadata":{},"source":["Sea $ \\boldsymbol{Y} $ un proceso ARIMA$(p,d,q)\\times(P,D,Q)_S$ \n$$\n\\boldsymbol{\\phi}_p(\\mathsf{B})\\,\\boldsymbol{\\Phi}_P(\\mathsf{B}^S)\\,\\nabla^d\\,\\nabla_{_S}^D\\, Y_t\n= c +\\boldsymbol{\\theta}_q(\\mathsf{B})\\,\\boldsymbol{\\Theta}_q(\\mathsf{B}^S)\\, U_t; \n\\quad t\\in\\mathbb{N}.\n$$\ndonde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.\n\nDenotemos con $\\boldsymbol{\\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:\n$$\n\\boldsymbol{\\varphi}(z)=\\boldsymbol{\\phi}_p(z)*\\boldsymbol{\\Phi}_P(z^S)*\\nabla^d*\\nabla_{_S}^D.\n$$ \nY denotemos con $\\boldsymbol{\\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:\n$$\\boldsymbol{\\vartheta}(z)=\\boldsymbol{\\theta}_q(z)*\\boldsymbol{\\Theta}_q(z^S).$$\nEntonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:\n$$\n\\boldsymbol{\\varphi}(\\mathsf{B})\\, Y_t = c + \\boldsymbol{\\vartheta}(\\mathsf{B})\\, U_t.\n$$\n\n"]},{"cell_type":"markdown","id":"c8996b63-64d5-43f8-9b39-bcb5621df264","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Debido a la simple estructura de los modelos ARIMA, tenemos que\n\n\\begin{equation}\nY_{t} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t-k} + U_{t} + \\sum_{k=1}^{q} -\\vartheta_k U_{t-k}.\n\\label{eqn:ARMA-en-t}\n\\end{equation}\n\nDe \\eqref{eqn:ARMA-en-t}, para $ \\ell = 0, \\pm 1, \\pm 2, \\dots $ y $ t $ arbitrario\n\n\\begin{equation}\nY_{t+\\ell} = c + \\sum_{k=1}^{p} \\varphi_k Y_{t+\\ell-k} + U_{t+\\ell} + \\sum_{k=1}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l}\n\\end{equation}\n\nDividiendo los sumatorios de $\\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\\ell-1$, y por otro la suma del resto (de $\\ell$ en adelante); tenemos que\n\n\\begin{equation}\nY_{t+\\ell} = \nc + \n\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k} + \n\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \nU_{t+\\ell} + \n\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k} + \n\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k},\n\\label{eqn:ARMA-en-t+l-bis}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"e8c746cf-e67d-4318-820d-226e7b9863fb","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Indicar la relación de cada elemento con el subespacio $\\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:\n\n\\begin{equation}\nY_{t+\\ell} = \n\\overbrace{c\\Bigg.}^{\\in\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} \\varphi_k Y_{t+\\ell-k}}_{\\not\\in\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell} + \n\\underbrace{U_{t+\\ell}\\Bigg.}_{\\perp\\mathcal{H}_{Y_t}} + \n\\underbrace{\\sum_{k=1}^{\\ell-1} -\\vartheta_k U_{t+\\ell-k}}_{\\perp\\mathcal{H}_{Y_t} \\text{ pues } k<\\ell} + \n\\overbrace{\\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}}^{\\in\\mathcal{H}_{Y_t} \\text{ pues } k\\geq\\ell};\n\\;\\text{ con } \\ell\\geq1.\n\\label{eqn:ARMA-en-t+l-bis-notas}\n\\end{equation}\n\n"]},{"cell_type":"markdown","id":"082cd995-3036-4078-a3c8-b947a4e3eff1","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Por linealidad de la proyección ortogonal, el predictor $\\ell$-pasos adelante de $ Y_t $ es\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = c + \\sum_{k=1}^{\\ell-1} \\varphi_k \\widehat{Y_{t+\\ell-k|t}} + \\sum_{k=\\ell}^{p} \\varphi_k Y_{t+\\ell-k} + \\sum_{k=\\ell}^{q} -\\vartheta_k U_{t+\\ell-k}:\n\\label{eqn:predicion-recursiva-ARMA}\n\\end{equation}\n\ndonde lo que era ortogonal a $\\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\\mathcal{H}_{Y_t}$ no cambia.\n\nA partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.\n\n"]},{"cell_type":"markdown","id":"204c3d1b-1e05-4dcc-b9d7-244a5874a252","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Ejemplos\n\n"]},{"cell_type":"markdown","id":"6ae5147b-5b63-4882-b4c8-f90161ce4096","metadata":{},"source":["#### ARMA(1,1)\n\n"]},{"cell_type":"markdown","id":"44d57904-1989-4ffa-b3b4-54407cab110d","metadata":{},"source":["$$\nY_{t+\\ell} = c + \\phi_1 Y_{t+\\ell-1} + U_{t+\\ell} - \\theta_1 U_{t+\\ell-1},\n$$\nla ecuación \\eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\\ell$ \n\n\\begin{equation}\n  \\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + \\phi_1 Y_{t} - \\theta_1 U_t. \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} & = c + \\phi_1 \\widehat{Y_{t+\\ell-1|t}}  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}\n  \\label{eqn:predicion-recursiva-ARMA11}\n\\end{equation}\n\ndonde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:\n\n"]},{"cell_type":"markdown","id":"0fa8d450-1f77-4043-9ae8-c916023df5f1","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   **Para $\\ell=1$:** $\\quad\\widehat{Y_{t+1|t}} = c + \\phi_1 Y_{t} - \\theta_1 U_t$.\n-   **Para $\\ell=2$:** \\begin{align*}\n    Y_{t+2} = & c + \\phi_1 \\widehat{Y_{t+1|t}} \\\\\n    = & c + \\phi_1 \\big(c + \\phi_1 Y_{t} - \\theta_1 U_t\\big) \\\\\n    = & c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t} \n    \\end{align*}\n-   **Para $\\ell=3$:** \\begin{align*}\n    Y_{t+3} = & c + \\phi_1 \\widehat{Y_{t+2|t}} \\\\\n    = & c + \\phi_1 \\big(c(1+\\phi_1) +  \\phi_1^2{Y}_{t} - \\phi_1 \\theta_1 U_{t}\\big) \\\\\n    = & c(1+\\phi_1+\\phi_1^2) + \\phi_1^3{Y}_{t} - \\phi_1^2 \\theta_1 U_{t}.\n    \\end{align*}\n-   **Para $l=k$:** Repitiendo el procedimiento llegamos a que: \n    $$\\;\\widehat{Y_{t+\\ell|t}} = c(1+\\phi_1+\\cdots+\\phi_1^\\ell) + \\phi_1^\\ell{Y}_{t} - \\phi_1^{\\ell-1} \\theta_1 U_{t}.$$\n\nComo $|\\phi_1|<1$ y $|\\theta|<1$, la predicción cuando $\\ell\\to\\infty$ tiende al valor esperado del proceso\n$$\\lim\\limits_{\\ell\\to\\infty}\\widehat{Y_{t+\\ell|t}}=\\frac{c}{1-\\phi_1}=\\mu.$$\n\n"]},{"cell_type":"markdown","id":"0df9b95c-f5df-4707-a72f-949adcf571f8","metadata":{"slideshow":{"slide_type":"slide"}},"source":["#### AR(1)\n\n"]},{"cell_type":"markdown","id":"aeee6ca8-18fa-4422-993a-551450cebbf5","metadata":{},"source":["Basta sustituir $\\theta=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), \n$$\n\\widehat{Y_{t+\\ell|t}} = \\frac{c}{1-\\phi_1} + \\phi_1^\\ell{Y}_{t}, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionAR1.png)\n\n"]},{"cell_type":"markdown","id":"0ddb0d79-29bc-43bb-8f71-d82e51cc375e","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### MA(1)\n\n"]},{"cell_type":"markdown","id":"a1af95a5-63c8-4d50-ad76-45a2905a151c","metadata":{},"source":["De igual manera, sustituyendo $\\phi=0$ en \\eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = \\frac{c}{1-\\phi_1} - \\theta U_t, \\\\\\\\\n    \\widehat{Y_{t+\\ell|t}} &  = \\frac{c}{1-\\phi_1}, \\quad \\text{para } \\ell \\ge 2.\n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionMA1.png)\n\n"]},{"cell_type":"markdown","id":"0f18e308-d736-4fff-b293-4ee0bc61ea85","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["En ambos casos se observa que $ \\widehat{Y_{t+\\ell|t}} $ converge a su valor esperado $ \\mu = E(Y_{t+\\ell}) $ cuando $ \\ell \\to \\infty $. Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.\n\n"]},{"cell_type":"markdown","id":"f8d814fb-893e-474d-a3ce-d841bafb6edc","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio sin deriva\n\n"]},{"cell_type":"markdown","id":"4d7e3d1c-23af-4d86-9f99-f876a37cae54","metadata":{},"source":["Para un paseo aleatorio $\\phi=1$, $\\theta=0$ y  $\\mu=0$  en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRW.png)\n\n"]},{"cell_type":"markdown","id":"83aa16be-90f8-4014-8f73-c59dc82aa828","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### Paseo aleatorio con deriva\n\n"]},{"cell_type":"markdown","id":"6e874989-fa4a-4832-a2f3-1a959752ea0f","metadata":{},"source":["Si el paseo aleatorio tiene deriva, el modelo es  $\\phi=1$, $\\theta=0$ y  $\\mu\\ne0$ en \\eqref{eqn:predicion-recursiva-ARMA11}:\n$$\n\\widehat{Y_{t+\\ell|t}} = c\\cdot\\ell + Y_t, \\quad \\ell = 1, 2, \\dots,\n$$\n\n![img](./img/figurasGretl/prediccionRWCD.png)\n\n"]},{"cell_type":"markdown","id":"b9069d7f-452e-41d9-83c1-bd2459c70116","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1)\n\n"]},{"cell_type":"markdown","id":"a1dccc21-6af0-4de4-8574-37273e445c9a","metadata":{},"source":["Basta sustituir $\\phi_1=1$ en \\eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), \n$$\\begin{cases}\n    \\widehat{Y_{t+1|t}} & = c + Y_{t} - \\theta_1 U_t. \\\\\n    \\widehat{Y_{t+\\ell|t}} & = c\\cdot \\ell + Y_{t} - \\theta_1 U_t  \\qquad \\text{cuando } \\ell\\geq2. \n  \\end{cases}$$\n\n![img](./img/figurasGretl/prediccionIMA.png)\n\n"]},{"cell_type":"markdown","id":"fc24ebee-f50d-4c62-8600-d96d1431c586","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["#### IMA(1) estacional\n\n"]},{"cell_type":"markdown","id":"0d0ec6ce-fee7-4b5d-9bf2-963a4207a3be","metadata":{},"source":["Para $\\nabla_{12}Y_t=(1-\\Theta_1B^{12})U_t$ (con $\\mu=0$ para simplificar); donde $j=1,2,\\ldots,12$:\n$$\\text{predicción de cada mes }\n  \\begin{cases}\n    \\widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \\theta_1 U_{t+j-12-1}. \\\\\n    \\widehat{Y_{t+j+(12\\ell)|t}} & = \\widehat{Y_{t+j|t}}  \\qquad \\text{cuando } \\ell\\geq1. \n  \\end{cases}$$\nDesde el segundo año, cada predicción es como la del mismo mes del año anterior.\n\n![img](./img/figurasGretl/prediccionSIMA.png)\n\n"]},{"cell_type":"markdown","id":"fcd238f9-8655-455f-ba03-3dc446b907ce","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Varianza de las predicciones lineales\n\n"]},{"cell_type":"markdown","id":"567174d3-e6b1-4885-aab8-582f72d37ad0","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Consideremos primero modelos lineales, causales y estacionarios*.\n\n"]},{"cell_type":"markdown","id":"54a63691-547f-4010-9198-a33452f1edf8","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["Sea $Y_t = \\psi(B)U_t$ la representación MA del proceso, donde $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$. Entonces:\n$$\nY_{t+\\ell} = \\sum_{i=0}^{\\infty} \\psi_i U_{t+\\ell-i}.\n$$\nLa predicción lineal óptima será su proyección ortogonal sobre $\\mathcal{H}_{Y_t}$:\n\n\\begin{equation}\n\\widehat{Y_{t+\\ell|t}} = \\sum_{j=0}^{\\infty} \\psi_{\\ell+j} U_{t-j}.\n\\label{eqn:predicion-MA-infinito}\n\\end{equation}\n\nRestando $\\widehat{Y_{t+\\ell|t}}$ de $Y_{t+\\ell}$ obtenemos el error de predicción:\n$$\ne_t(\\ell) \\;=\\; Y_{t+\\ell} - \\widehat{Y_{t+\\ell|t}} \\;=\\; U_{t+\\ell} + \\psi_1 U_{t+\\ell-1} + \\cdots + \\psi_{\\ell-1} U_{t+1},\n$$\ncuya esperanza es cero. \n\nAsumiendo que $\\boldsymbol{Y}$ tiene distribución gaussiana, $\\widehat{Y_{t+\\ell|t}}=E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})$; y por tanto \n\n$$\nE\\Big[e_t(\\ell)^2\\Big]=\nVar(e_t(\\ell))=\n%E\\Big[\\big(Y_{t+\\ell} - E(Y_{t+\\ell}\\mid \\boldsymbol{Y}_{|:t})\\big)^2\\Big]=\n\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2)\n$$\ndonde $1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2$ es la suma del cuadrado de los $\\ell$ primeros coeficientes de la secuencia $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-1}*\\boldsymbol{\\theta}$, que es una secuencia de cuadrado sumable.\n\n"]},{"cell_type":"markdown","id":"498f1f2b-d821-43f0-adb6-07c85156f27c","metadata":{"slideshow":{"slide_type":"skip"}},"source":["*Y ahora consideremos modelos lineales, causales NO estacionarios*.\n\n"]},{"cell_type":"markdown","id":"6dca19b4-bac4-41a1-afdc-1023b803aa76","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["-   Suponga que $\\boldsymbol{Y}$ es SARIMA ($\\boldsymbol{\\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto $Y_t = \\psi(B)U_n=0$ para $t<0$).\n\n-   El cálculo sigue siendo el mismo. Aunque ahora $\\boldsymbol{\\psi}=\\boldsymbol{\\phi}^{-\\triangleright}*\\boldsymbol{\\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.\n\n-   La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como $\\psi_k \\to 0$ cuando $k$ aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.\n\nPor ejemplo, en un modelo AR(1), donde $\\psi_k = \\phi^k$, cuando $\\ell$ tiende a infinito   \n$$\n\\lim_{\\ell\\to\\infty}\\,Var(e_T(\\ell)) = \\sigma^2 (1 + \\psi_1^2 + \\psi_{2}^2 + \\cdots ) = \\sigma^2 / (1 - \\phi^2).\n$$\nSi $|\\phi|$ está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, $\\sigma^2$. Pero la incertidumbre es finita pues la varianza marginal también lo es.\n\n-   En modelos no estacionarios, como la serie $\\sum \\psi_i^2$ no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: **no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo**.\n\n"]},{"cell_type":"markdown","id":"79889c55-bd2a-4e33-94cf-95e6f1be89b2","metadata":{"slideshow":{"slide_type":"subslide"}},"source":["### Intervalos de confianza para las previsiones\n\n"]},{"cell_type":"markdown","id":"b662d1e8-c508-43f4-8456-5e73dde3ece5","metadata":{},"source":["Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. \n\nComo $Y_{t+\\ell}$ tendrá distribución normal con esperanza  $\\widehat{Y_{t+\\ell|t}}$ y varianza\n$$\\sigma^2 (1 + \\psi_1^2 + \\cdots + \\psi_{\\ell-1}^2),$$ \ntendremos que \n$$\nY_{t+\\ell} \\in \\left( \\widehat{Y_{t+\\ell|t}} \\pm z_{1-\\alpha} \\sqrt{Var(e_t(\\ell))} \\right)\n$$\ndonde $z_{1-\\alpha}$ son los valores críticos de la distribución normal.\n\nEn modelos MA($q$) la varianza deja de crecer si $\\ell>q$, pues $\\psi_{k}=0$ para todo $k>q$.\n\n"]},{"cell_type":"markdown","id":"09952d40-ee97-4441-a3c5-275505a6dca6","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Adaptación de las predicciones a las nuevas observaciones\n\n"]},{"cell_type":"markdown","id":"e9e5eb75-c7b7-4530-84cb-8f152523e1a5","metadata":{},"source":["Contamos con predicciones para $t+1$ hasta $t+j$ basadas en $\\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\\mathcal{H}_{Y_{t+1}}$.\nSegún \\eqref{eqn:predicion-MA-infinito}, la predicción de $Y_{t+\\ell}$ con información hasta $t$ es:\n$$\n\\widehat{Y_{t+\\ell|t}} = \\psi_\\ell U_t + \\psi_{\\ell+1} U_{t-1} + \\ldots.\n$$\nComparando $\\widehat{Y_{t+1|t}}$ con $Y_{t+1}$ obtenemos el error de predicción $U_{t+1} = Y_{t+1} - \\widehat{Y_{t+1|t}}$. \n\nLa nueva predicción para $Y_{t+\\ell}$, incorporando $Y_{t+1}$ como información adicional, será:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} = \\psi_{\\ell-1} U_t + \\psi_\\ell U_{t-1} + \\ldots\n$$\nRestando las dos previsiones, tenemos que:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} - \\widehat{Y_{t+\\ell|t}} = \\psi_{\\ell-1} U_{t+1}.\n$$\nAl observar $Y_{t+1}$ y obtener el error de previsión $U_{t+1}$, podremos revisar las previsiones:\n$$\n\\widehat{Y_{t+\\ell-1|t+1}} = \\widehat{Y_{t+\\ell|t}} + \\psi_{\\ell-1} U_{t+1}\n%, \\tag{8.23}\n$$\n(las predicciones se revisan sumando un múltiplo del último error de predicción).\n\nSi no hay error de previsión un periodo adelante $(U_{t+1} = 0)$, no hay revisión del resto.\n\n"]},{"cell_type":"markdown","id":"992cf324-2a8b-4d2f-969d-9fb08f06710c","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Modelos con logaritmos\n\n"]},{"cell_type":"markdown","id":"300801ce-64f6-49a4-acaf-2cf1cd43ffca","metadata":{},"source":["Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.\nSi\n$$\nY_{t+\\ell} = \\ln(X_{t+\\ell})\n$$\nentonces la previsión puntual  de la serie original es \n$$\n\\widehat{X_{t+\\ell|t}} = \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\frac{1}{2} Var\\big(e_t(\\ell)\\big) \\right)\n$$\nY el Intervalo de confianza\n$$\nI_{1-\\alpha}(X_{t+\\ell}) = (a, b)\n$$\ndonde \n\n\\begin{align*}\na = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} - \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)  \\\\\nb = & \\exp\\left( \\widehat{Y_{t+\\ell|t}} + \\big(z_{1-\\frac{\\alpha}{2}}\\big) \\sqrt{Var\\big(e_t(\\ell)\\big)} \\right)\n\\end{align*}\n\nque es no simétrico alrededor de la previsión.\n\n"]},{"cell_type":"markdown","id":"aa8c3ef4-51d3-47b9-8549-94e7ba2ba5da","metadata":{"slideshow":{"slide_type":"skip"}},"source":["## Nota sobre subespacio sobre el que proyectamos al prever\n\n"]},{"cell_type":"markdown","id":"dfb435c1-b424-47c2-beef-b9351133888b","metadata":{"slideshow":{"slide_type":"skip"}},"source":["Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por\n$$\nY_t = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k},\n$$\ndonde:\n\n-   $\\sum_{k=0}^{\\infty} \\psi_k^2 < \\infty$,\n-   $\\boldsymbol{U}$ es un proceso de ruido blanco con $E[U_t]=0$ y $E[U_t^2]=\\sigma^2 < \\infty$,\n-   el proceso es **invertible**, es decir, existe una secuencia $\\{\\pi_j\\}_{j\\ge 0}$ con\n    $\\sum_{j=0}^{\\infty} \\pi_j^2 < \\infty$ tal que\n    $$\n      U_t = \\sum_{j=0}^{\\infty} \\pi_j\\, Y_{t-j}.\n      $$\n\nBajo las hipótesis anteriores se cumple que los subespacios cerrados de $L^2$ generados por el pasado de $Y_t$ y por el pasado de las innovaciones $U_t$ son iguales:\n$$\n\\mathcal{H}_{Y_t} \n\\;=\\;\n\\mathcal{H}_{U_t} \n$$\ndonde $\\mathcal{H}_{Z_n} = \\overline{\\operatorname{sp}}\\{1,Z_n,Z_{n-1},\\ldots\\}$ denota la clausura en la norma de $L^2$ del subespacio engendrado por las variables aleatorias $\\{1,Z_n,Z_{n-1},Z_{n-2},\\ldots\\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. \nLa \\`\\`H'' de la notación $\\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de <u>Hilbert</u> y que dicho espacio está engendrado por las variables aleatorias que constituyen la <u>\\`\\`Historia''</u> del proceso desde $Z_n$ hacia atrás.\n\n**Demostración:**\n\n1.  **Primera inclusión** $\\subseteq$:\n    \n    Cada $Y_{t}$ se puede expresar como combinación lineal de las innovaciones pasadas:\n    $$\n       Y_{t} = \\sum_{k=0}^{\\infty} \\psi_k\\, U_{t-k}.\n       $$\n    Por tanto, todo $Y_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,U_t,U_{t-1}, U_{t-2}, \\dots\\}$, y al incluir la clausura en $L^2$ obtenemos\n    $$\n       \\mathcal{H}_{Y_t}  % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\subseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n2.  **Segunda inclusión** $\\supseteq$:\n    \n    Por invertibilidad, cada $U_{t}$ se puede escribir como combinación de las $Y$ pasadas:\n    $$\n       U_{t} = \\sum_{m=0}^{\\infty} \\pi_m\\, Y_{t-m}.\n       $$\n    Así, cada $U_{t}$ pertenece al subespacio cerrado generado por\n    $\\{1,Y_t,Y_{t-1},Y_{t-2},\\dots\\}$, y al incluir la clausura en $L^2$ obtenemos la inclusión opuesta.\n    $$\n       \\mathcal{H}_{Y_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{Y}_{|:(t-1)}\\}\n       \\supseteq\n       \\mathcal{H}_{U_t} % \\overline{\\operatorname{sp}}\\{\\boldsymbol{U}_{|:(t-1)}\\}\n       $$\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":5}