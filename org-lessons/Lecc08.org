#    -*- mode: org -*-

#+TITLE: Lección 8. Predicción de modelos ARIMA
#+author: Marcos Bujosa
#+LANGUAGE: es
# +OPTIONS: toc:nil

# https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/
# https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/
# https://medium.com/datainc/time-series-analysis-and-forecasting-with-arima-in-python-aa22694b3aaa

# https://www.tigerdata.com/blog/how-to-work-with-time-series-in-python
# https://pyspectrum.readthedocs.io/en/latest/ref_param.html

# https://github.com/QuantEcon/quantecon-notebooks-python/blob/master/arma.ipynb
# https://github.com/QuantEcon/lecture-python-intro
# https://quantecon.org/quantecon-py/
# https://quantecon.org/

#+include: 00preambulo_lecciones.txt
#+LaTeX_HEADER: \DeclareMathOperator*{\plim}{plim}

#+begin_src emacs-lisp :results silent :exports none
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((latex . t)))
#+end_src


#+BEGIN_SRC emacs-lisp :exports none :results silent
(use-package ox-ipynb
  :load-path (lambda () (expand-file-name "ox-ipynb" scimax-dir)))
(use-package htmlize)
#+END_SRC

#+LATEX: \maketitle

#+LATEX: \blfootnote{Licencia: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0).}


#+begin_abstract
Esta lección se centra en la predicción de modelos de series temporales univariantes.  
#+end_abstract

- ([[https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc08.slides.html][slides]]) --- ([[https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc08.html][html]]) --- ([[https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc08.pdf][pdf]]) --- ([[https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc08.ipynb][mybinder]])

***  Carga de algunos módulos de python y creación de directorios auxiliares
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: notoc
   :END:

   
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results none
# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib as mpl
# definimos parámetros para mejorar los gráficos
mpl.rc('text', usetex=False)
import matplotlib.pyplot as plt   # data visualization
import dataframe_image as dfi   # export tables as .png
#+END_SRC


**** Directorio auxiliar para albergar las figuras de la lección:
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: t 
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
para publicar la lección como pdf o página web, necesito los gráficos como ficheros ~.png~ alojados algún directorio específico:
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results silent
imagenes_leccion = "./img/lecc07" # directorio para las imágenes de la lección
import os
os.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe
#+END_SRC


*** Gráficos para las ACF, PACF y densidades espectrales teóricas
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: notoc
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Cargamos las funciones auxiliares (véase la carpeta =src/=)

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results silent
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
%run -i ./src/analisis_armas.py
#+END_SRC

# +latex: \newpage


* Introducción
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

Consideremos una serie temporal $\boldsymbol{y}_1^T=(y_1,\ldots,y_T)$. En relación con la predicción de los valores futuros de la serie, hay tres aspectos a considerar : 
- Necesitamos un método para hacer predicciones basadas en un modelo ajustado a las observaciones.
- Necesitamos evaluar la incertidumbre asociada a estas predicciones. Sin ello la utilidad de los pronósticos se ve comprometida.

  Es común expresar la incertidumbre con un intervalo de confianza para la predicción; un rango que con alta probabilidad (como 0.95 o 0.99) incluye el valor futuro pronosticado.
- Además, la previsión es un proceso dinámico: al recibir nuevos datos, es necesario actualizar los pronósticos con la nueva información. 
*Los modelos lineales facilitan solventar estas tres necesidades*.


* Predicciones de error cuadrático medio mínimo
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

Queremos predecir \(Y_{t+\ell}\) usando una función \(g\) de variables aleatorias con índice \(n\leq t\), es decir, una función del conjunto \(\boldsymbol{Y}_{|:t} = \{Y_n \mid n \leq t\}\).

Nos referiremos al conjunto $\boldsymbol{Y}_{|:t}$ como /``el pasado''/ de $\boldsymbol{Y}$. 

#+attr_ipynb: (slideshow . ((slide_type . skip)))
# Es decir, en función del ``pasado del proceso'' que llega hasta el instante correspondiente al índice $t$.

#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Para evaluar diferentes funciones de predicción, estableceremos como criterio /minimizar el error cuadrático medio/ (MSE):
\[
MSE\big(Y_{t+\ell},\,g_t(\ell)\big) = E\Big(Y_{t+\ell}-g_t(\ell)\Big)^2.
\]
#+attr_ipynb: (slideshow . ((slide_type . skip)))
Preferiremos aquel método $g_t(\ell)$ que minimiza la esperanza del cuadrado del error de predicción $Y_{t+\ell}-g_t(\ell)$; donde \(g_t(\ell)\) es la función de predicción de \({Y}_{t+\ell}\), \(t\) es el índice de la última variable aleatoria considerada y \(\ell\) representa el horizonte de predicción.


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
La función $g_t(\ell)$ que minimiza dicho criterio MSE es la *esperanza condicional* de $Y_{t+\ell}$:
\[
E\big(Y_{t+\ell}\mid \boldsymbol{Y}_{|:t}\big).
\]
#+BEGIN_EXPORT latex
\textbf{Nota}. La esperanza condicional es la proyección ortogonal sobre el espacio (de Hilbert) generado por el conjunto de todas las funciones (medibles) de $\boldsymbol{Y}_{|:t}$. Dicho conjunto es enorme y, entre otras muchas funciones, también contiene al conjunto de funciones de $\boldsymbol{Y}_{|:t}$ que son \emph{lineales}.
#+END_EXPORT

#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Las predicciones resultantes se llaman predicciones de error cuadrático medio mínimo.
#+latex: \medskip


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
El problema de la esperanza condicional radica en que, según el caso, su cálculo puede ser muy difícil\dots o sencillamente inviable. 
#+BEGIN_EXPORT latex
Dicho de otro modo, determinar específicamente cuál es la transformación de $\boldsymbol{Y}_{|:t}$ que minimiza los errores de predicción puede ser imposible en la práctica.
#+END_EXPORT

Como solución alternativa podemos limitarnos a buscar predictores entre:
- las funciones de $\boldsymbol{Y}_{|:t}$ (i.e., funciones ``del pasado'') que son /_lineales_/; 
- y seleccionar aquella con menor error cuadrático medio (MSEL).

Al adoptar esta estrategia, aceptamos cometer mayores errores de predicción, $MSEL \geq MSE$, pues al excluir funciones no lineales, podríamos estar omitiendo la que genera los errores más pequeños.

*Pregunta*: ¿Cómo tiene que ser la esperanza condicional para que $MSEL$ y $MSE$ sean iguales? ¿En qué caso ocurrirá?

** Predicciones lineales
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

*** Breve recordatorio sobre las proyecciones ortogonales
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

#+latex: \noindent
Solo recordaremos que como
#+begin_center
_la proyección ortogonal de $X$ sobre $\mathcal{H}$ es el elemento de $\mathcal{H}$ más /próximo/ a $X$_
#+end_center
- si $X\in\mathcal{H}$, la proyección de $X$ sobre $\mathcal{H}$ es el propio $X$. 
- si $Y$ es ortogonal a todo $X\in\mathcal{H}$, es decir, si $Y\perp\mathcal{H}$, la proyección de $Y$ sobre $\mathcal{H}$ es cero. 

La proyección ortogonal de $X$ sobre el espacio euclídeo $\mathcal{H}$ se caracteriza porque la diferencia entre $X$ y su proyección, $\widehat{X}$, es perpendicular a $\mathcal{H}$. 

En el contexto de la predicción lineal, dicha diferencia $X-\widehat{X}$ recibirá el nombre de /error de predicción/.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
/Puede consultar algunos resultados sobre las proyecciones ortogonales en este [[https://mbujosab.github.io/CursoDeAlgebraLineal/libro.pdf#appendix.13][enlace]]./

*** Las predicciones lineales son proyecciones ortogonales
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

La forma general de un predictor /lineal/ es 
\[g_t(\ell)
\;=\; 
{b_\ell}_0 Y_t + {b_\ell}_1 Y_{t-1} + {b_\ell}_2 Y_{t-2} + \cdots
\;=\; 
\boldsymbol{b_\ell}*(\boldsymbol{Y}_{|:t})
\;=\; \boldsymbol{b_\ell}(B)Y_t,
\]
donde $\boldsymbol{b_\ell}$ es una serie formal de cuadrado sumable. 

#+attr_ipynb: (slideshow . ((slide_type . fragment)))
El error cuadrático medio del predictor lineal (MSEL) es mínimo cuando el error de predicción \(\big(Y_{t+\ell}-g_t(\ell)\big)\) es ortogonal a cada una de las variables aleatorias del pasado del proceso;@@latex:\footnote{Cuando el proceso $\boldsymbol{Y}$ tiene media cero, esta condición es equivalente a que el error de previsión esté incorrelado con el pasado del proceso.}@@ es decir, cuando
\[
E\Big((Y_{t+\ell}-\boldsymbol{b_\ell}*\boldsymbol{Y}_{|:t})\cdot{Y}_{n}\Big) = \boldsymbol{0}, \;\text{para cualquier } n\leq t.
\]
#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Consecuentemente, el mejor predictor lineal (que denotaremos con $\widehat{Y_{t+\ell|t}}$) es la proyección ortogonal de $Y_{t+\ell}$ sobre la /clausura/ @@latex:\footnote{Sea $\overline{\operatorname{sp}}\{1,Y_t,Y_{t-1},\ldots\}$ la clausura en la norma de \(L^2\) del subespacio vectorial engendrado por la constante $1$ y el ``pasado'' de $\boldsymbol{Y}$. Es decir, completamos el conjunto $\operatorname{sp}\{1,Y_t,Y_{t-1},\ldots\}$ de las combinaciones lineales de la constante $1$ y las variables $\{1,Y_t,Y_{t-1},\ldots\}$, con los límites de todas las sucesiones de Cauchy de elementos de $\operatorname{sp}\{1,Y_t,Y_{t-1},\ldots\}$ (es decir, le añadimos la clausura). De esta manera toda sucesión de Cauchy de elementos del espacio converje en norma a algún elemento del propio espacio, pues hemos añadido los límites (así tendremos un espacio de Hilbert donde la proyección ortogonal está bien definida). Para abreviar, denotaremos  $\overline{\operatorname{sp}}\{1,Y_t,Y_{t-1},\ldots\}$ con $\mathcal{H}_{Y_t}$.}@@ del subespacio formado por las combinaciones lineales de $1$ y las variables aleatorias $Y_t,Y_{t-1},Y_{t-2},\ldots$. 


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Denotando dicha clausura con $\mathcal{H}_{Y_t}$, podemos decir que el predictor lineal que minimiza los errores de previsión, $\widehat{Y_{t+\ell|t}}$, es la proyección ortogonal de $Y_{t+\ell}$ sobre $\mathcal{H}_{Y_t}$.

Es habitual referirse a los elementos de $\mathcal{H}_{Y_t}$ como /variables observadas/;@@latex:\footnote{Este nombre viene sugerido por el contexto de la previsión de datos reales. Pero es solo una interpretación del formalismo matemático. Fíjese que los objetos con los que estamos tratando son \emph{variables aleatorias}, es decir, funciones matemáticas. Las funciones matemáticas son una costrucción intelectural por lo que, en ese sentido, hablar de \emph{variables aleatorias del pasado} es similar a hablar de \emph{los argonautas del futuro} o \emph{los unicornios del pasado}. Lo que es relevante es que dichas variables son elementos del espacio $\mathcal{H}_{Y_t}$ sobre el que estamos proyectando las variables.}@@ y a todo el espacio $\mathcal{H}_{Y_t}$ como el /conjunto de información/ empleado en la previsión.

Cuando $\boldsymbol{Y}=\boldsymbol{\varphi}*\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible, se cumple que $\;\mathcal{H}_{Y_t} = \mathcal{H}_{U_t}$ 
#+attr_ipynb: (slideshow . ((slide_type . skip)))
(véase la sección [[id:e8c024c1-d9d1-41e8-b1fa-e0deb73bbf79][Nota sobre subespacio sobre el que proyectamos al prever]]).[fn::donde $\mathcal{H}_{U_t}$ denota la clausura en la norma de \(L^2\) del subespacio generado por la constante $1$ y el pasado del proceso de ruido blanco $\boldsymbol{U}$]

#+latex:\medskip

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Restringir la búsqueda a funciones lineales implica que, si la esperanza condicional no es lineal respecto al pasado, el mejor predictor lineal será inferior a la esperanza condicional.
\[
MSE \leq MSEL.
\]
La igualdad se da solo cuando la esperanza condicional es lineal en $\boldsymbol{Y}_{|:t}$; y por tanto no ha sido excluida por la restricción. En resumen, $MSEL = MSE$ solo cuando la esperanza condicional $E\big(Y_{t+\ell}\mid \boldsymbol{Y}_{|:t}\big)$ coincide con la proyección ortogonal de $Y_{t+\ell}$ sobre $\mathcal{H}_{Y_t}$ (que es exactamente lo que ocurre cuando el proceso estocástico tiene distribución normal).

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Los resultados anteriores, aplicables a procesos estacionarios infinitos, también son válidos para procesos no estacionarios si consideramos un tramo finito del pasado de $\boldsymbol{Y}$, es decir, si nuestra predicción depende de $\boldsymbol{Y}_{|1:T}=(Y_1,\ldots,Y_T)$. En tal caso la discusión se centra en proyectar (o condicionar la esperanza[fn::entonces estaremos proyectado ortogonalmente sobre funciones medibles de un conjunto finito de variables aleatorias: $\{Y_1,\ldots,Y_T\}$.]) sobre el conjunto finito de variables aleatorias del pasado $\boldsymbol{Y}_{|1:T}$. Así, tanto la esperanza como la varianza condicional serán finitas, lo que asegura que todo esté bien definido.

*** Proyección sobre el pasado de un proceso lineal causal, estacionario e invertible
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Sea $\boldsymbol{Y}=\boldsymbol{\varphi}*\boldsymbol{U}$ es un proceso estocástico lineal causal, estacionario e invertible.

Para $\ell > 0$,

- denominamos /_previsión de $Y_{t+\ell}$ basada en la historia del proceso hasta $Y_t$_/ a la proyección ortogonal de $Y_{t+\ell}$ sobre $\mathcal{H}_{Y_t};\;$ que denotamos con $\widehat{Y_{t+\ell|t}}$.

- Análogamente, la /``previsión $\widehat{U_{t+\ell|t}}$ basada en el pasado de $\boldsymbol{Y}$''/ es la proyección ortogonal de $U_{t+\ell}$ sobre $\mathcal{H}_{Y_t}=\mathcal{H}_{U_t}$. 

  Fíjese que $\widehat{U_{t+\ell|t}}$ _siempre es cero_; pues el ruido blanco es ortogonal a su pasado, $U_{t+\ell}\perp\mathcal{H}_{U_t},\;$ por ser incorrelado y con esperanza nula@@latex:\footnote{Para una discusión sobre correlación y ortogonalidad, léase la sección 3.1.1 de los apuntes \url{https://github.com/mbujosab/Ectr/blob/master/apuntes-Ectr-print.pdf}}@@ 
# (y recuerde que $\mathcal{H}_{Y_t}=\mathcal{H}_{U_t}$).
# Es decir, la proyección es cero porque $U_{t+\ell}\perp\mathcal{H}_{U_t}\;$ 

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Sin embargo, para los índices $(t-k)$ donde $k \geq 0$: 

- la proyección de $Y_{t-k}$ y la de $U_{t-k}$ sobre $\mathcal{H}_{Y_t}$ son las propias $Y_{t-k}$ y $U_{t-k}$ respectivamente, ya que ambas variables aleatorias pertenecen al espacio $\mathcal{H}_{Y_t}=\mathcal{H}_{U_t}$ sobre el que se proyectan (i.e., son /variables observadas/).

Además para cualquier índice $j$: 
- Con las secuencias constantes ($c_t=c$ para todo $t$) tenemos que $\widehat{c_{j|t}}=c$ dado que $1\cdot c\in\mathcal{H}_{Y_t}$.

# Sin embargo, cuando $k \geq 0$, tanto la proyección de $Y_{t-k}$ como la $U_{t-k}$ de sobre $\mathcal{H}_{Y_t}$ es la propia variable $Y_{t-k}$ (ó $U_{t-k}$), pues ambas son elementos de $\mathcal{H}_{Y_t}=\mathcal{H}_{U_t}$. 

Tras este repaso de las proyecciones ortogonales, ya podemos ver cómo calcular predicciones para los modelos ARIMA causales.



* Cálculo de predicciones de procesos ARIMA
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:
 
Sea \( \boldsymbol{Y} \) un proceso ARIMA$(p,d,q)\times(P,D,Q)_S$ 
\[
\boldsymbol{\phi}_p(\mathsf{B})\,\boldsymbol{\Phi}_P(\mathsf{B}^S)\,\nabla^d\,\nabla_{_S}^D\, Y_t
= c +\boldsymbol{\theta}_q(\mathsf{B})\,\boldsymbol{\Theta}_q(\mathsf{B}^S)\, U_t; 
\quad t\in\mathbb{N}.
\]
donde $c$ puede ser distinto de cero para permitir procesos con esperanza no nula.

Denotemos con $\boldsymbol{\varphi}$ al polinomio de grado $p+P+d+D$ resultante del producto convolución de los polinomios de la parte AR:
\[
\boldsymbol{\varphi}(z)=\boldsymbol{\phi}_p(z)*\boldsymbol{\Phi}_P(z^S)*\nabla^d*\nabla_{_S}^D.
\] 
Y denotemos con $\boldsymbol{\vartheta}$ al polinomio de grado $q+Q$ resultante del producto convolución de los polinomios de la parte MA:
\[\boldsymbol{\vartheta}(z)=\boldsymbol{\theta}_q(z)*\boldsymbol{\Theta}_q(z^S).\]
Entonces podemos reescribir el modelo ARIMA de una manera mucho más compacta:
\[
\boldsymbol{\varphi}(\mathsf{B})\, Y_t = c + \boldsymbol{\vartheta}(\mathsf{B})\, U_t.
\]
#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Debido a la simple estructura de los modelos ARIMA, tenemos que
\begin{equation}
Y_{t} = c + \sum_{k=1}^{p} \varphi_k Y_{t-k} + U_{t} + \sum_{k=1}^{q} -\vartheta_k U_{t-k}.
\label{eqn:ARMA-en-t}
\end{equation}
De \eqref{eqn:ARMA-en-t}, para \( \ell = 0, \pm 1, \pm 2, \dots \) y \( t \) arbitrario
\begin{equation}
Y_{t+\ell} = c + \sum_{k=1}^{p} \varphi_k Y_{t+\ell-k} + U_{t+\ell} + \sum_{k=1}^{q} -\vartheta_k U_{t+\ell-k},
\label{eqn:ARMA-en-t+l}
\end{equation}

Dividiendo los sumatorios de $\eqref{eqn:ARMA-en-t+l}$ en dos partes: por un lado la suma desde $k=1$ hasta $\ell-1$, y por otro la suma del resto (de $\ell$ en adelante); tenemos que@@latex:\footnote{Recuerde que en la notación con sumatorios se asume que si el limite inferior del índice (el que aparece abajo del sumatorio) es mayor que el valor que aparece arriba, el sumatorio es cero: si $m>n$ entonces $\sum_{i=m}^n a_i =0$.}@@
\begin{equation}
Y_{t+\ell} = 
c + 
\sum_{k=1}^{\ell-1} \varphi_k Y_{t+\ell-k} + 
\sum_{k=\ell}^{p} \varphi_k Y_{t+\ell-k} + 
U_{t+\ell} + 
\sum_{k=1}^{\ell-1} -\vartheta_k U_{t+\ell-k} + 
\sum_{k=\ell}^{q} -\vartheta_k U_{t+\ell-k},
\label{eqn:ARMA-en-t+l-bis}
\end{equation}
#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Indicar la relación de cada elemento con el subespacio $\mathcal{H}_{Y_t}$ nos ayudará a calcular las previsiones:
\begin{equation}
Y_{t+\ell} = 
\overbrace{c\Bigg.}^{\in\mathcal{H}_{Y_t}} + 
\underbrace{\sum_{k=1}^{\ell-1} \varphi_k Y_{t+\ell-k}}_{\not\in\mathcal{H}_{Y_t} \text{ pues } k<\ell} + 
\overbrace{\sum_{k=\ell}^{p} \varphi_k Y_{t+\ell-k}}^{\in\mathcal{H}_{Y_t} \text{ pues } k\geq\ell} + 
\underbrace{U_{t+\ell}\Bigg.}_{\perp\mathcal{H}_{Y_t}} + 
\underbrace{\sum_{k=1}^{\ell-1} -\vartheta_k U_{t+\ell-k}}_{\perp\mathcal{H}_{Y_t} \text{ pues } k<\ell} + 
\overbrace{\sum_{k=\ell}^{q} -\vartheta_k U_{t+\ell-k}}^{\in\mathcal{H}_{Y_t} \text{ pues } k\geq\ell};
\;\text{ con } \ell\geq1.
\label{eqn:ARMA-en-t+l-bis-notas}
\end{equation}
#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Por linealidad de la proyección ortogonal, el predictor \(\ell\)-pasos adelante de \( Y_t \) es
\begin{equation}
\widehat{Y_{t+\ell|t}} = c + \sum_{k=1}^{\ell-1} \varphi_k \widehat{Y_{t+\ell-k|t}} + \sum_{k=\ell}^{p} \varphi_k Y_{t+\ell-k} + \sum_{k=\ell}^{q} -\vartheta_k U_{t+\ell-k}:
\label{eqn:predicion-recursiva-ARMA}
\end{equation}
donde lo que era ortogonal a $\mathcal{H}_{Y_t}$ se anula, y lo que pertenece a $\mathcal{H}_{Y_t}$ no cambia.

A partir de esta expresión podemos obtener fórmulas recursivas tanto para los predictores como para sus actualizaciones cuando se disponen de nuevas observaciones.

# *AÑADIR ECUACIÓN DE PREDICCIÓN 8.6 DEL LIBRO DE PEÑA*

** Ejemplos
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

# \overbrace{  }^{\in\mathcal{H}_{Y_t}}

# \underbrace{ }_{\perp\mathcal{H}_{Y_t}}

*** ARMA(1,1)
\[
Y_{t+\ell} = c + \phi_1 Y_{t+\ell-1} + U_{t+\ell} - \theta_1 U_{t+\ell-1},
\]
la ecuación \eqref{eqn:predicion-recursiva-ARMA} da lugar a dos casos en función de $\ell$ 
\begin{equation}
  \begin{cases}
    \widehat{Y_{t+1|t}} & = c + \phi_1 Y_{t} - \theta_1 U_t \qquad \text{cuando } \ell=1 \\\\
    \widehat{Y_{t+\ell|t}} & = c + \phi_1 \widehat{Y_{t+\ell-1|t}}  \qquad \text{cuando } \ell\geq2. 
  \end{cases}
  \label{eqn:predicion-recursiva-ARMA11}
\end{equation}
donde la segunda ecuación es recursiva (y la primera nos indica el paso inicial). Así:
#+attr_ipynb: (slideshow . ((slide_type . subslide)))
- Para $\ell=1$ :: $\quad\widehat{Y_{t+1|t}} = c + \phi_1 Y_{t} - \theta_1 U_t$.
- Para $\ell=2$ :: 
  \begin{align*}
  Y_{t+2} = & c + \phi_1 \widehat{Y_{t+1|t}} \\
  = & c + \phi_1 \big(c + \phi_1 Y_{t} - \theta_1 U_t\big) \\
  = & c(1+\phi_1) +  \phi_1^2{Y}_{t} - \phi_1 \theta_1 U_{t} 
  \end{align*}
- Para $\ell=3$ :: 
  \begin{align*}
  Y_{t+3} = & c + \phi_1 \widehat{Y_{t+2|t}} \\
  = & c + \phi_1 \big(c(1+\phi_1) +  \phi_1^2{Y}_{t} - \phi_1 \theta_1 U_{t}\big) \\
  = & c(1+\phi_1+\phi_1^2) + \phi_1^3{Y}_{t} - \phi_1^2 \theta_1 U_{t}.
  \end{align*}
- Para $l=k$ ::  Repitiendo el procedimiento llegamos a que: 
  $$\;\widehat{Y_{t+\ell|t}} = c(1+\phi_1+\cdots+\phi_1^\ell) + \phi_1^\ell{Y}_{t} - \phi_1^{\ell-1} \theta_1 U_{t}.$$
#+latex: \medskip

Como $|\phi_1|<1$ y $|\theta|<1$, la predicción cuando $\ell\to\infty$ tiende al valor esperado del proceso
$$\lim\limits_{\ell\to\infty}\widehat{Y_{t+\ell|t}}=\frac{c}{1-\phi_1}=\mu.$$
#+latex: \bigskip

*** AR(1)
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

Basta sustituir $\theta=0$ en \eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado un modelo AR(1), 
\[
\widehat{Y_{t+\ell|t}} = \frac{c}{1-\phi_1} + \phi_1^\ell{Y}_{t}, \quad \ell = 1, 2, \dots,
\]

#+attr_org: :width 600
#+attr_html: :width 400px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionAR1.png]]


*** MA(1)
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

De igual manera, sustituyendo $\phi=0$ en \eqref{eqn:predicion-recursiva-ARMA11} particularizamos para un modelo MA(1), 
$$\begin{cases}
    \widehat{Y_{t+1|t}} & = \frac{c}{1-\phi_1} - \theta U_t, \\\\
    \widehat{Y_{t+\ell|t}} &  = \frac{c}{1-\phi_1}, \quad \text{para } \ell \ge 2.
  \end{cases}$$

#+attr_org: :width 600
#+attr_html: :width 400px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionMA1.png]]


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
En ambos casos se observa que \( \widehat{Y_{t+\ell|t}} \) converge a su valor esperado \( \mu = E(Y_{t+\ell}) \) cuando \( \ell \to \infty \). Esta propiedad se verifica en todos los modelos ARMA estacionarios, causales e invertibles.


*** Paseo aleatorio sin deriva
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Para un paseo aleatorio $\phi=1$, $\theta=0$ y  $\mu=0$  en \eqref{eqn:predicion-recursiva-ARMA11}:
\[
\widehat{Y_{t+\ell|t}} = Y_t, \quad \ell = 1, 2, \dots,
\]

#+attr_org: :width 600
#+attr_html: :width 400px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionRW.png]]


*** Paseo aleatorio con deriva
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Si el paseo aleatorio tiene deriva, el modelo es  $\phi=1$, $\theta=0$ y  $\mu\ne0$ en \eqref{eqn:predicion-recursiva-ARMA11}:
\[
\widehat{Y_{t+\ell|t}} = c\cdot\ell + Y_t, \quad \ell = 1, 2, \dots,
\]

#+attr_org: :width 600
#+attr_html: :width 400px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionRWCD.png]]



*** IMA(1)
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Basta sustituir $\phi_1=1$ en \eqref{eqn:predicion-recursiva-ARMA11} para particularizar el resultado a un modelo IMA(1), 
$$\begin{cases}
    \widehat{Y_{t+1|t}} & = c + Y_{t} - \theta_1 U_t. \\
    \widehat{Y_{t+\ell|t}} & = c\cdot \ell + Y_{t} - \theta_1 U_t  \qquad \text{cuando } \ell\geq2. 
  \end{cases}$$

#+attr_org: :width 600
#+attr_html: :width 400px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionIMA.png]]


*** IMA(1) estacional
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Para $\nabla_{12}Y_t=(1-\Theta_1B^{12})U_t$ (con $\mu=0$ para simplificar); donde $j=1,2,\ldots,12$:
$$\text{predicción de cada mes }
  \begin{cases}
    \widehat{Y_{t+j|t}}          & = Y_{t+j-12} - \Theta_1 U_{t+j-12-1}. \\
    \widehat{Y_{t+j+(12\ell)|t}} & = \widehat{Y_{t+j|t}}  \qquad \text{cuando } \ell\geq1. 
  \end{cases}$$
Desde el segundo año, cada predicción es como la del mismo mes del año anterior.


#+attr_org: :width 600
#+attr_html: :width 300px
#+attr_latex: :width 225px
[[file:./img/figurasGretl/prediccionSIMA.png]]

* Varianza de las predicciones lineales
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:


# +attr_ipynb: (slideshow . ((slide_type . skip)))
/Consideremos primero modelos lineales, causales y estacionarios/.

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
#+latex: \noindent
Sea \(Y_t = \psi(B)U_t\) la representación MA del proceso, donde $\boldsymbol{\psi}=\boldsymbol{\phi}^{-1}*\boldsymbol{\theta}$. Entonces:
\[
Y_{t+\ell} = \sum_{i=0}^{\infty} \psi_i U_{t+\ell-i}.
\]
La predicción lineal óptima será su proyección ortogonal sobre $\mathcal{H}_{Y_t}$:
\begin{equation}
\widehat{Y_{t+\ell|t}} = \sum_{j=0}^{\infty} \psi_{\ell+j} U_{t-j}.
\label{eqn:predicion-MA-infinito}
\end{equation}
Restando $\widehat{Y_{t+\ell|t}}$ de $Y_{t+\ell}$ obtenemos el error de predicción:
\[
e_t(\ell) \;=\; Y_{t+\ell} - \widehat{Y_{t+\ell|t}} \;=\; U_{t+\ell} + \psi_1 U_{t+\ell-1} + \cdots + \psi_{\ell-1} U_{t+1},
\]
cuya esperanza es cero. 

Asumiendo que \(\boldsymbol{Y}\) tiene distribución gaussiana, $\widehat{Y_{t+\ell|t}}=E(Y_{t+\ell}\mid \boldsymbol{Y}_{|:t})$; y por tanto 
# Consecuentemente, 
\[
E\Big[e_t(\ell)^2\Big]=
Var(e_t(\ell))=
%E\Big[\big(Y_{t+\ell} - E(Y_{t+\ell}\mid \boldsymbol{Y}_{|:t})\big)^2\Big]=
\sigma^2 (1 + \psi_1^2 + \cdots + \psi_{\ell-1}^2)
\]
donde \(1 + \psi_1^2 + \cdots + \psi_{\ell-1}^2\) es la suma del cuadrado de los $\ell$ primeros coeficientes de la secuencia $\boldsymbol{\psi}=\boldsymbol{\phi}^{-1}*\boldsymbol{\theta}$, que es una secuencia de cuadrado sumable.

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
/Y ahora consideremos modelos lineales, causales NO estacionarios/.

- Suponga que $\boldsymbol{Y}$ es SARIMA ($\boldsymbol{\phi}$ tiene raíces en el círculo unidad) pero que simultáneamente $\boldsymbol{U}$ tiene un comienzo (que $U_t=0$ para $t<0$; por tanto \(Y_t = \psi(B)U_n=0\) para $t<0$).

- El cálculo sigue siendo el mismo. Aunque ahora $\boldsymbol{\psi}=\boldsymbol{\phi}^{-\triangleright}*\boldsymbol{\theta}$ no es de cuadrado sumable, en las sumas solo hay una cantidad finita de términos no nulos.

- La incertidumbre en las predicciones difiere entre modelos estacionarios y no estacionarios. En los estacionarios, como \(\psi_k \to 0\) cuando \(k\) aumenta, la varianza de la predicción tiene a un valor constante: la varianza marginal del proceso.
Por ejemplo, en un modelo AR(1), donde \(\psi_k = \phi^k\), cuando \(\ell\) tiende a infinito   
\[
\lim_{\ell\to\infty}\,Var(e_T(\ell)) = \sigma^2 (1 + \psi_1^2 + \psi_{2}^2 + \cdots ) = \sigma^2 / (1 - \phi^2).
\]
Si \(|\phi|\) está próximo uno, la varianza marginal será mucho más grande que la varianza condicionada, \(\sigma^2\). Pero la incertidumbre es finita pues la varianza marginal también lo es.

- En modelos no estacionarios, como la serie \(\sum \psi_i^2\) no converge, la incertidumbre en la predicción a largo plazo crece indefinidamente: *no se puede anticipar el comportamiento de un proceso no estacionario a largo plazo*.

# Sin embargo, para los modelos no estacionarios, la serie \(\sum \psi_i^2\) no converge, lo que provoca que la incertidumbre de la predicción a largo plazo aumente indefinidamente. Por lo tanto, *no es posible anticipar el comportamiento de un proceso no estacionario a largo plazo*.


** Intervalos de confianza para las previsiones
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

Si asumimos que la distribución de las innovaciones es gaussiana, podemos calcular intervalos de confianza para diferentes horizontes de predicción. 

Como \(Y_{t+\ell}\) tendrá distribución normal con esperanza  \(\widehat{Y_{t+\ell|t}}\) y varianza
\[\sigma^2 (1 + \psi_1^2 + \cdots + \psi_{\ell-1}^2),\] 
tendremos que 
\[
Y_{t+\ell} \in \left( \widehat{Y_{t+\ell|t}} \pm z_{1-\alpha} \sqrt{Var(e_t(\ell))} \right)
\]
donde \(z_{1-\alpha}\) son los valores críticos de la distribución normal.

En modelos MA($q$) la varianza deja de crecer si $\ell>q$, pues $\psi_{k}=0$ para todo $k>q$.


* Adaptación de las predicciones a las nuevas observaciones
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:


Contamos con predicciones para \(t+1\) hasta \(t+j\) basadas en $\mathcal{H}_{Y_t}$. Queremos actualizarlas con $\mathcal{H}_{Y_{t+1}}$.
Según \eqref{eqn:predicion-MA-infinito}, la predicción de \(Y_{t+\ell}\) con información hasta \(t\) es:
\[
\widehat{Y_{t+\ell|t}} = \psi_\ell U_t + \psi_{\ell+1} U_{t-1} + \ldots.
\]
Comparando $\widehat{Y_{t+1|t}}$ con \(Y_{t+1}\) obtenemos el error de predicción \(U_{t+1} = Y_{t+1} - \widehat{Y_{t+1|t}}\). 

La nueva predicción para \(Y_{t+\ell}\), incorporando $Y_{t+1}$ como información adicional, será:
\[
\widehat{Y_{t+\ell|t+1}} = \psi_{\ell-1} U_{t+1} + \psi_\ell U_{t}  + \psi_{\ell+1} U_{t-1} + \ldots
\]
Restando las dos previsiones, tenemos que:
\[
\widehat{Y_{t+\ell|t+1}} - \widehat{Y_{t+\ell|t}} = \psi_{\ell-1} U_{t+1}.
\]
Al observar \(Y_{t+1}\) y obtener el error de previsión \(U_{t+1}\), podremos revisar las previsiones:
\[
\widehat{Y_{t+\ell|t+1}} = \widehat{Y_{t+\ell|t}} + \psi_{\ell-1} U_{t+1}
%, \tag{8.23}
\]
(las predicciones se revisan sumando un múltiplo del último error de predicción).

Si no hay error de previsión un periodo adelante \((U_{t+1} = 0)\), no hay revisión del resto.

# La ecuación (8.23) tiene una interpretación interesante.  
# Las dos variables \(Y_{t+1}\) y \(Y_{t+\ell}\) tienen, dada la información hasta \(t\), una distribución conjuntamente normal con esperanzas \(\widehat{Y_{t+1|t}}\) y \(\widehat{Y_{t+\ell|t}}\), varianzas \(\sigma^2\) y \(\sigma^2(1 + \psi_1^2 + \ldots + \psi_{\ell-1}^2)\) y covarianza:
# $$\begin{aligned}
# \mathrm{cov}(Y_{t+1}, Y_{t+\ell})
# &= E\big[(Y_{t+1} - \widehat{Y_{t+1|t}})(Y_{t+\ell} - \widehat{Y_{t+\ell|t}})\big] \\
# &= E\big[U_{t+1}(U_{t+\ell} + \psi_1 U_{t+\ell-1} + \ldots)\big] = \sigma^2 \psi_{\ell-1}.
# \end{aligned}$$
# 
# La mejor predicción de \(Y_{t+\ell}\) dada \(Y_{t+1}\) y la información hasta \(t\) puede calcularse por regresión, según la expresión:
# \[
# E(Y_{t+\ell} \mid Y_{t+1}) = E(Y_{t+\ell}) + \mathrm{cov}(Y_{t+1}, Y_{t+\ell}) \mathrm{var}^{-1}(Y_{t+1}) (Y_{t+1} - E(Y_{t+1})).
# \]


* Medidas del error de previsión
  :PROPERTIES:
  :metadata: (slideshow . ((slide_type . slide)))
  :CUSTOM_ID: sec-fcast-stats
  :END:

- Error Medio (ME) :: $= \frac{1}{T} \sum_{t=1}^T e_t$

  Promedio de los errores de previsión.

- Raíz del Error Cuadrático Medio (RMSE) :: $= \sqrt{\frac{1}{T} \sum_{t=1}^T e_t^2}$

  Es la norma euclídea de vector de errores.

- Error Absoluto Medio (MAE) ::  $= \frac{1}{T} \sum_{t=1}^T |e_t|$

  Promedio de los errores de previsión en términos absolutos

- Error Porcentual Medio (MPE) :: $= \frac{1}{T} \sum_{t=1}^T 100\, \frac{e_t}{y_t}$

  Promedio de los errores de previsión porcentuales

- Error Porcentual Absoluto Medio (MAPE) :: $= \frac{1}{T} \sum_{t=1}^T 100\, \left|\frac{e_t}{y_t}\right|$

  Promedio del valor absoluto de los errores porcentuales

#+latex: \noindent
Gretl muestra otros estadísticos adicionales (véase el manual).

# Una estadística adicional relevante es el $U$ de Theil, que tiene dos variantes: $U_1$  y $U_2$. 
# La primera se define así
# \[U_1 = \left[\frac{1}{T}\sum_{t=1}^T(y_t-f_t)^2\right]^{0.5} \cdot 
#         \left[\left(\frac{1}{T}\sum_{t=1}^Ty_t^2\right)^{0.5} +
#         \left(\frac{1}{T}\sum_{t=1}^Tf_t^2\right)^{0.5}\right]^{-1}\]
# y está acotado entre 0 y 1. Un valor cercano a cero indica alta precisión en el pronóstico; $U_1$ se acerca a 1 a medida que los errores de pronóstico crecen arbitrariamente. El segundo se define como la raíz cuadrada positiva de
# \[U_2^2 = \frac{1}{T} \sum_{t=1}^{T-1} \left(\frac{f_{t+1} - y_{t+1}}{y_t}\right)^2 \cdot 
#           \left[\frac{1}{T} \sum_{t=1}^{T-1} \left(\frac{y_{t+1} - y_t}{y_t}\right)^2 \right]^{-1}\]
# $U_2$ depende de que los datos tengan un orden natural y es aplicable solo para datos de series temporales. Puede interpretarse como la relación entre el RMSE del modelo de pronóstico propuesto y el RMSE de un modelo ingenuo que simplemente predice $Y_{t+1} = Y_t= para todo $t$. 
# El modelo ingenuo produce $U_2=1$; los valores menores a 1 indican una mejora en relación con este punto de referencia y los valores mayores a 1 indican un deterioro.
# 
# Además, Theil propuso una descomposición del MSE que puede ser útil para evaluar un conjunto de pronósticos. 
# Mostró que el MSE puede descomponerse en tres componentes no negativas de la siguiente manera
# \[MSE = \left(\bar{f}-\bar{y}\right)^2 + 
#          \left(s_f - rs_y\right)^2 + 
#          \left(1-r^2\right) s_y^2\]
# donde $\bar{g}$ y $\bar{y}$ son las medias muestrales de los pronósticos y las observaciones, $s_g$ y $s_y$ son las respectivas desviaciones estándar (usando $T$ en el denominador), y $r$ es la correlación muestral entre $y$ y $g$. 
# Dividiendo toda la ecuación por la MSE obtenemos
# \[\frac{\left(\bar{f}-\bar{y}\right)^2}{\rm MSE} +
#     \frac{\left(s_f - rs_y\right)^2}{\rm MSE} + 
#     \frac{\left(1-r^2\right) s_y^2}{\rm MSE} = 1\]
# Theil llamó a los tres términos del lado izquierdo de esta ecuación la proporción de sesgo (=U^M=), la proporción de regresión (=U^R=) y la proporción de disturbio (=U^D=), respectivamente. Si =y= y =f= representan las observaciones de la variable dependiente en la muestra y los valores ajustados de una regresión lineal, entonces los dos primeros componentes, =U^M= y =U^R=, serán cero (apartando errores de redondeo), y toda la MSE será explicada por la parte no sistemática, =U^D=. Sin embargo, en el caso de la predicción fuera de la muestra (o "predicción" sobre un subconjunto de los datos utilizados en la regresión), =U^M= y =U^R= no son necesariamente cercanos a cero. =U^M= difiere de cero si y solo si la media de los pronósticos difiere de la media de las realizaciones, y =U^R= es no cero si y solo si la pendiente de una regresión simple de las realizaciones sobre los pronósticos difiere de 1.



* Modelos con logaritmos
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

Si se tomaron logaritmos para estabilizar la varianza, es necesario recuperar las previsiones de la serie original a partir de las de la serie transformada.
Si
\[
Y_{t+\ell} = \ln(X_{t+\ell})
\]
entonces la previsión puntual  de la serie original es 
\[
\widehat{X_{t+\ell|t}} = \exp\left( \widehat{Y_{t+\ell|t}} + \frac{1}{2} Var\big(e_t(\ell)\big) \right)
\]
Y el Intervalo de confianza
\[
I_{1-\alpha}(X_{t+\ell}) = (a, b)
\]
donde 
\begin{align*}
a = & \exp\left( \widehat{Y_{t+\ell|t}} - \big(z_{1-\frac{\alpha}{2}}\big) \sqrt{Var\big(e_t(\ell)\big)} \right)  \\
b = & \exp\left( \widehat{Y_{t+\ell|t}} + \big(z_{1-\frac{\alpha}{2}}\big) \sqrt{Var\big(e_t(\ell)\big)} \right)
\end{align*}
que es no simétrico alrededor de la previsión.


* Nota sobre subespacio sobre el que proyectamos al prever
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :ID:       e8c024c1-d9d1-41e8-b1fa-e0deb73bbf79
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Sea un proceso estocástico lineal causal y estacionario de segundo orden definido por
\[
Y_t = \sum_{k=0}^{\infty} \psi_k\, U_{t-k},
\]
donde:
- \(\sum_{k=0}^{\infty} \psi_k^2 < \infty\),
- \(\boldsymbol{U}\) es un proceso de ruido blanco con \(E[U_t]=0\) y \(E[U_t^2]=\sigma^2 < \infty\),
- el proceso es *invertible*, es decir, existe una secuencia \(\{\pi_j\}_{j\ge 0}\) con
  \(\sum_{j=0}^{\infty} \pi_j^2 < \infty\) tal que
  \[
  U_t = \sum_{j=0}^{\infty} \pi_j\, Y_{t-j}.
  \]

Bajo las hipótesis anteriores se cumple que los subespacios cerrados de \(L^2\) generados por el pasado de \(Y_t\) y por el pasado de las innovaciones \(U_t\) son iguales:
\[
\mathcal{H}_{Y_t} 
\;=\;
\mathcal{H}_{U_t} 
\]
donde $\mathcal{H}_{Z_n} = \overline{\operatorname{sp}}\{1,Z_n,Z_{n-1},\ldots\}$ denota la clausura en la norma de \(L^2\) del subespacio engendrado por las variables aleatorias $\{1,Z_n,Z_{n-1},Z_{n-2},\ldots\}$. He incluido la constante $1$ para poder considerar procesos con esperanza no nula. 
La ``H'' de la notación $\mathcal{H}_{Z_n}$ tiene como objetivo recordar que se trata de un espacio de _Hilbert_ y que dicho espacio está engendrado por las variables aleatorias que constituyen la _``Historia''_ del proceso desde $Z_n$ hacia atrás.


*Demostración:*

1. *Primera inclusión* \(\subseteq\):

   Cada \(Y_{t}\) se puede expresar como combinación lineal de las innovaciones pasadas:
   \[
   Y_{t} = \sum_{k=0}^{\infty} \psi_k\, U_{t-k}.
   \]
   Por tanto, todo \(Y_{t}\) pertenece al subespacio cerrado generado por
   \(\{1,U_t,U_{t-1}, U_{t-2}, \dots\}\), y al incluir la clausura en \(L^2\) obtenemos
   \[
   \mathcal{H}_{Y_t}  % \overline{\operatorname{sp}}\{\boldsymbol{Y}_{|:(t-1)}\}
   \subseteq
   \mathcal{H}_{U_t} % \overline{\operatorname{sp}}\{\boldsymbol{U}_{|:(t-1)}\}
   \]

2. *Segunda inclusión* \(\supseteq\):

   Por invertibilidad, cada \(U_{t}\) se puede escribir como combinación de las \(Y\) pasadas:
   \[
   U_{t} = \sum_{m=0}^{\infty} \pi_m\, Y_{t-m}.
   \]
   Así, cada \(U_{t}\) pertenece al subespacio cerrado generado por
   \(\{1,Y_t,Y_{t-1},Y_{t-2},\dots\}\), y al incluir la clausura en \(L^2\) obtenemos la inclusión opuesta.
   \[
   \mathcal{H}_{Y_t} % \overline{\operatorname{sp}}\{\boldsymbol{Y}_{|:(t-1)}\}
   \supseteq
   \mathcal{H}_{U_t} % \overline{\operatorname{sp}}\{\boldsymbol{U}_{|:(t-1)}\}
   \]
# Combinando ambas inclusiones, se tiene la igualdad buscada: $\;\mathcal{H}_{Y_t}=\mathcal{H}_{U_t}$.



# bibliographystyle:plainnat
# bibliography:bibliografia.bib



* COMMENT ipynb y slides                                           :noexport:

#+BEGIN_SRC emacs-lisp :results silent :eval yes
(require 'ox-ipynb)
(ox-ipynb-export-org-file-to-ipynb-file "Lecc08.org")
#+END_SRC

#+BEGIN_SRC sh :results silent :eval yes
#jupyter nbconvert --execute --inplace Lecc08.ipynb
#+END_SRC

#+BEGIN_SRC sh :results silent :eval yes
#jupyter nbconvert --config ../mycfg-GitHubPages.py --to slides --reveal-prefix "https://unpkg.com/reveal.js@5.2.1" --execute Lecc08.ipynb
#+END_SRC


* COMMENT Prueba                                                   :noexport:
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

*************** TODO COMMENT Modificar código python para simular y prever ARIMAS

# #+attr_ipynb: (slideshow . ((slide_type . skip)))
# #+BEGIN_SRC jupyter-python :results none
# from sklearn.metrics import mean_squared_error
# import warnings
# 
# from statsmodels.tsa.stattools import adfuller
# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# from statsmodels.tsa.arima.model import ARIMA
# 
# # close warnings
# warnings.filterwarnings('ignore')
# #+END_SRC
# 
# # RW
# #+BEGIN_SRC jupyter-python
# fig = plot_paseo_aleatorio_analysis(trend="t", pendiente=0.25, n=400, semilla=2025)
# fig.savefig('./img/lecc07/ACF-RWcd.png', dpi=300, bbox_inches='tight')
# 
# #+END_SRC
# 
# #+RESULTS:
# 
# 
# #+BEGIN_SRC jupyter-python
# adf_test = adfuller(data["births"])
# print('ADF Statistic: %f' % adf_test[0])
# print('p-value: %f' % adf_test[1])
# #+END_SRC



