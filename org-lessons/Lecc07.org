#    -*- mode: org -*-

#+TITLE: Lección 7. Modelos ARIMA y SARIMA. Identificación y diagnosis
#+author: Marcos Bujosa
#+LANGUAGE: es
# +OPTIONS: toc:nil

# https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/
# https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/
# https://medium.com/datainc/time-series-analysis-and-forecasting-with-arima-in-python-aa22694b3aaa

# https://www.tigerdata.com/blog/how-to-work-with-time-series-in-python
# https://pyspectrum.readthedocs.io/en/latest/ref_param.html

# https://github.com/QuantEcon/quantecon-notebooks-python/blob/master/arma.ipynb
# https://github.com/QuantEcon/lecture-python-intro
# https://quantecon.org/quantecon-py/
# https://quantecon.org/

#+include: 00preambulo_lecciones.txt
#+LaTeX_HEADER: \DeclareMathOperator*{\plim}{plim}

#+begin_src emacs-lisp :results silent :exports none
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((latex . t)))
#+end_src


#+BEGIN_SRC emacs-lisp :exports none :results silent
(use-package ox-ipynb
  :load-path (lambda () (expand-file-name "ox-ipynb" scimax-dir)))
(use-package htmlize)
#+END_SRC

#+LATEX: \maketitle

#+begin_abstract
Repasaremos los instrumentos de identificación y diagnosis del análisis univariante.
Extenderemos la notación para incorporar modelos con raíces unitarias y modelos estacionales.
Finalmente resumiremos las ideas principales del análisis univariante.
#+end_abstract

- ([[https://mbujosab.github.io/Econometria-Aplicada/Transparencias/Lecc07.slides.html][slides]]) --- ([[https://mbujosab.github.io/Econometria-Aplicada/Lecciones-html/Lecc07.html][html]]) --- ([[https://mbujosab.github.io/Econometria-Aplicada/Lecciones-pdf/Lecc07.pdf][pdf]]) --- ([[https://mybinder.org/v2/gh/mbujosab/Econometria-Aplicada/gh-pages?labpath=CuadernosElectronicos/Lecc07.ipynb][mybinder]])


# #+begin_export html
# <style>
# .reveal  td {font-size: 60%;}
# </style>
# #+end_export


***** COMMENT para Jupyter-Notebook                               :noexport:
\(
\newcommand{\lag}{\mathsf{B}}
\newcommand{\Sec}[1]{\boldsymbol{#1}}
\newcommand{\Pol}[1]{\boldsymbol{#1}}
\)


***  Carga de algunos módulos de python y creación de directorios auxiliares
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: notoc
   :END:

   
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results none
# Para trabajar con los datos y dibujarlos necesitamos cargar algunos módulos de python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib as mpl
# definimos parámetros para mejorar los gráficos
mpl.rc('text', usetex=False)
import matplotlib.pyplot as plt   # data visualization
import dataframe_image as dfi   # export tables as .png
#+END_SRC

**** Directorio auxiliar para albergar las figuras de la lección:
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: t 
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
para publicar la lección como pdf o página web, necesito los gráficos como ficheros ~.png~ alojados algún directorio específico:
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results silent
imagenes_leccion = "./img/lecc07" # directorio para las imágenes de la lección
import os
os.makedirs(imagenes_leccion, exist_ok=True) # crea el directorio si no existe
#+END_SRC


*** Gráficos para las ACF, PACF y densidades espectrales teóricas
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :UNNUMBERED: notoc
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Cargamos las funciones auxiliares (véase la carpeta =src/=)

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results silent
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
%run -i ./src/analisis_armas.py
#+END_SRC

# +latex: \newpage


* Identificación
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:


- Combinando herramientas gráficas y estadísticas, se puede inferir un modelo para los datos.
- Este proceso de especificación empírica de un modelo es conocido como "/identificación/".
#+latex: \medskip

El proceso de identificación puede estructurarse como una secuencia de
preguntas:

1) ¿Es la serie estacionaria?
2) ¿Tiene una media significativa?
3) ¿Es persistente la ACF? ¿sigue alguna pauta reconocible?
4) ¿Es persistente la PACF? ¿sigue alguna pauta reconocible?

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
- La identificación se apoya en estadísticos muestrales (media, autocorrelaciones, etc.) cuya representatividad respecto del proceso estocástico subyacente depende de la estacionariedad y la ergodicidad[fn::Véase la sección [[id:53eeba38-4fe7-432c-84f5-a5e9a0dc0808][Introducción intuitiva e informal al concepto de ergodicidad]].].
- Tras inducir la estacionariedad, especificamos un modelo tentativo decidiendo cuál de las funciones ACF o PACF es finita y cuál es persistente
#+latex: \medskip

#+ATTR_HTML: :border 2 :rules all :frame border
#+ATTR_LATEX: :environment longtable :align p{3.4cm}p{5.4cm}p{5.4cm}
|                    | *ACF finita*                                             | *ACF persistente*                |
|--------------------+----------------------------------------------------------+----------------------------------|
| *PACF finita*      | _Ruido blanco_: retardos conjuntamente NO significativos | _AR_: orden indicado por la PACF |
| *PACF persistente* | _MA_: orden indicado por la ACF                          | _ARMA_                           |

La parametrización de mayor orden en modelos ARMA con series económicas suele ser ARMA($2,1$)
#+latex: \medskip

** Instrumentos de identificación
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

#+ATTR_HTML: :border 2 :rules all :frame border
#+ATTR_LATEX: :environment longtable :align p{2.3cm}p{5.5cm}p{7.4cm}
|                                                         | Instrumento                                                                                                                         | Objetivo y observaciones                                                                                                             |
|---------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------|
| Transf. @@latex:\mbox{@@logarítmica@@latex:}@@          | Gráficos rango-media y serie temporal.                                                                                              | Conseguir independizar la variabilidad de los datos de su nivel. Las series económicas necesitan esta transformación frecuentemente. |
| $d$, orden de @@latex:\mbox{@@diferenciación@@latex:}@@ | Gráfico de la serie temporal. ACF (caída lenta y lineal). @@latex:\mbox{@@Contrastes@@latex:}@@ de raíz unitaria (DF o ADF y KPSS). | Conseguir que los datos fluctúen en torno a una media estable. En series económicas, $d$ suele ser 0, 1 ó 2.                         |
| Constante                                               | Media de la serie transformada. @@latex:\mbox{@@Desviación@@latex:}@@ típica de la media.                                           | Si la media de la serie transformada es significativa, el modelo debe incluir un término constante.                                  |
| $p$, orden AR.                                          | Si PACF cae abruptamente en el retardo $p$ y la ACF decae lentamente.                                                               | En series económicas $p$ suele ser $\leq2$.                                                                                          |
| $q$, orden MA.                                          | Si ACF cae abruptamente en el retardo $q$ y PACF  decae lentamente.                                                                 | En series económicas q suele ser $\leq1$.                                                                                            |
#+latex: \medskip


* Estimación por máxima verosimilitud
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

# +attr_ipynb: (slideshow . ((slide_type . slide)))
Los parámetros de los modelos ARMA se estiman por máxima verosimilitud exacta (resolviendo un sistema de ecuaciones no lineales de forma iterativa).

** Propiedades de los estimadores de máxima verosimilitud (MV)
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

- Invarianza :: Si $\hat{\beta}^{MV}$ es el estimador de máxima verosimilitud de $\beta$ y $g(\beta)$ es una función continua, entonces el estimador de máxima verosimilitud de $g(\beta)$ es $g(\hat{\beta}^{MV})$.
  #+attr_ipynb: (slideshow . ((slide_type . skip)))
  - Ejemplo. :: Si $\hat{\sigma}^2_{MV}$ es el estimador MV de $\sigma^2$, entonces el estimador MV de $\sigma$ es $\sqrt{\hat{\sigma}^2_{MV}}$.


- Consistencia :: A medida que el tamaño muestral $n$ crece, el estimador $\hat{\beta}^{MV}$ se aproxima en probabilidad al valor verdadero $\beta$.
  \[
  \operatorname*{plim}\limits_{n \to \infty} \hat{\beta}^{MV} = \beta
  %\mathop{\mathrm{plim}}
  \]
  #+attr_ipynb: (slideshow . ((slide_type . skip)))
   Es decir, a medida que crece la muestra, la probabilidad de estimar valores alejados del verdadero valor tiende a cero; formalmente y de manera más explícita: 
   $$\forall \epsilon > 0,\; \lim_{n\to\infty}P\Big(\lvert\hat{\beta}^{MV}​-\beta\rvert>\epsilon\Big)=0.$$

- Insesgadez asintótica ::   El estimador puede ser sesgado en muestras pequeñas, pero su sesgo desaparece conforme $n \to \infty$. Es decir, el valor esperado converge al parámetro verdadero.
  \[
  \lim_{n \to \infty} E\Big(\hat{\beta}^{MV}_n\Big) = \beta
  \]
  #+attr_ipynb: (slideshow . ((slide_type . skip)))
  - Diferencia con insesgadez :: Un estimador *insesgado* cumple $E(\hat{\beta}) = \beta$ para todo $n$, mientras que un estimador *asintóticamente insesgado* lo cumple solo en el límite.

- Eficiencia asintótica :: El estimador se vuelve más preciso a medida que crece $n$. Su varianza tiende a cero, concentrándose alrededor del valor verdadero.
  \[
  \lim_{n \to \infty} Var\Big(\hat{\beta}^{MV}_n\Big) = 0
  \]
  #+attr_ipynb: (slideshow . ((slide_type . skip)))
  - Nota :: Bajo condiciones regulares, el MLE es *asintóticamente eficiente*, es decir, alcanza la menor varianza posible entre los estimadores consistentes (cota de Cramér–Rao).

#+LATEX: \renewcommand{\arraystretch}{1.4}
#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+ATTR_LATEX: :environment longtable :align p{3.5cm}p{12cm}
| Propiedades           | Significado                                                                                                                                                             |
|-----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Invarianza            | Si $\hat{\beta}^{MV}$ es el estimador de máxima verosimilitud de $\beta$, entonces el estimador de máxima verosimilitud de $g(\beta)$ es $g\big(\hat{\beta}^{MV}\big)$. |
| Consistencia          | $\forall \epsilon > 0, \lim\limits_{n\to\infty}P\Big(\lvert\hat{\beta}^{MV}​-\beta\rvert>\epsilon\Big)=0$                                                               |
| Insesgadez asintótica | $\lim\limits_{n \to \infty} E\Big(\hat{\beta}^{MV}_n\Big) = \beta$                                                                                                      |
| Eficiencia asintótica | $\lim\limits_{n \to \infty} Var\Big(\hat{\beta}^{MV}_n\Big) = 0$                                                                                                        |
#+LATEX: \renewcommand{\arraystretch}{1}


* Diagnosis
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

** Instrumentos de diagnosis
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

#+attr_ipynb: (slideshow . ((slide_type . slide)))

#+ATTR_HTML: :border 2 :rules all :frame border
#+ATTR_LATEX: :environment longtable :align p{2.3cm}p{5.5cm}p{7.2cm}
|                                                         | Instrumento                                                | Posible diagnóstico                                                                                              |
|---------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------|
| $d$, orden de @@latex:\mbox{@@diferenciación@@latex:}@@ | /Proximidad a 1/ de alguna raíz de los polinomios AR o MA. | Conviene diferenciar si la raíz es AR; o quitar una diferencia si es MA (/salvo si hay tendencia determinista/). |
| $d$, orden de @@latex:\mbox{@@diferenciación@@latex:}@@ | Gráfico de los residuos.                                   | Si muestra rachas largas de residuos positivos o negativos, puede ser necesaria una diferencia adicional.        |
| Constante                                               | Media de los residuos.                                     | Si es significativa: añadir una  constante.                                                                      |
| Constante                                               | Constante estimada.                                        | Si NO es significativa: el modelo mejorará quitando el término constante.                                        |
| $p$ y $q$,                                              | Contrastes de significación de los parámetros estimados.   | Pueden sugerir eliminar parámetros irrelevantes.                                                                 |
| $p$ y $q$,                                              | ACF/PACF residuos. Test Q de Ljung-Box para la ACF.        | Indican posibles pautas de autocorrelación no modelizadas.                                                       |
| $p$ y $q$,                                              | Correlaciones elevadas entre los parámetros estimados.     | Puede ser síntoma de sobreparametrización.                                                                       |


# *** Sobrediferenciación e infradiferenicación
#    :PROPERTIES:
#    :metadata: (slideshow . ((slide_type . subslide)))
#    :END:
# 
# - si diferenciamos de más :: pérdida de eficiencia en la estimación. Aumento de la varianza
# - ss diferenciamos de menos :: se cometen errores de predicción crecientes con el horizonte y se subestima la varianza de la predicción. Modelo poco robusto (grandes cambios en las estimaciones ante pequeños cambios muestrales).

*** Reformulación
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:
Siempre debemos analizar las características de los residuos.
- Si los residuos presentan correlación serial debemos reformular el modelo.
  - Ejemplo :: Supongamos que hemos estimado un modelo AR(1) y el correlograma de los residuos presenta estructura MA(1): 
    debemos estimar un modelo ARMA(1,1) y analizar los nuevos residuos y la significatividad de los parámentos

*** Componentes deterministas
Un MA(1) con parámetro próximo a 1 puede indicar sobrediferenciación, pero también la presencia de una componente determinista.
 - Ejemplo :: El modelo $(1-\mathsf{B})Y_t=\beta+(1-\theta\mathsf{B}) U_t$ es equivalente al modelo $Y_t = \beta t + U_t$ cuando $\theta=1$.

   #+latex: \noindent
  (fíjese que el primer modelo no es invertible; y el segundo no es estacionario).

 

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Una vez superadas las pruebas de diagnostico, aún se puede aplicar un análisis exploratorio; consistente en añadir parámetros AR o MA para comprobar si resultan significativos y mejoran el modelo.
- no aumentar los órdenes autorregresivos y de medias móviles simultáneamente.

*** Otras herramientas estadísticas a usar, una vez comprobada la ausencia de correlación serial en los residuos
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:

# *** Estadísticos descriptivos
#    :PROPERTIES:
#    :metadata: (slideshow . ((slide_type . slide)))
#    :END:
# 
# - el nivel de la serie (media, mediana),
# - valores extremos (máximo y mínimo). A veces son outliers o errores de registro
# - dispersión de la variable (desviación típica, coeficiente de variación, rango, rango interpercentiles, rango intercuartílico)
# - otros momentos (asimetría, exceso de curtosis)

**** Término constante.
Cuando el modelo ni tiene término constante:
contrastar si la media de los residuos es significativa ( $H_0: \mu = 0$) para decidir si incorporarlo
\[
\frac{\widehat{\mu}}{\widehat{dt(\widehat{\mu})}}\underset{H_0}{\sim}t_{n-1};\qquad
\widehat{dt(\widehat{\mu})}=\frac{\widehat{\sigma}}{\sqrt{n}}.
\]

**** Verificar visualmente que la varianza de los residuos es constante.

**** Contrastar la normalidad de los residuos con el test Jarque-Bera.
[[https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test][Jarque-Bera test (Wikipedia)]]
- La transformación logarítmica :: suele inducir normalidad si no hay datos negativos.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :exports code :results raw
from scipy import stats
rng = np.random.default_rng()
x = rng.normal(0, 1, 100000)
jarque_bera_test = stats.jarque_bera(x)
jarque_bera_test
# jarque_bera_test.statistic
# jarque_bera_test.pvalue
#+END_SRC

#+RESULTS:
: SignificanceResult(statistic=0.8177207377470701, pvalue=0.6644069977431221)

# - Si hay valores atípicos :: el mejor tratamiento es /intervenirlos/.


* Raíces unitarias
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

** Notación: operadores retardo y diferencia y modelos ARIMA
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

El operador diferencia $\nabla$ se define a partir del operador retardo como $\nabla=(1 - \mathsf{B})$:
\[
 \nabla Y_t = (1 - \mathsf{B})Y_t = Y_t - Y_{t-1}.
\]
El operador diferencia estacional es ${\nabla}_{_S} = (1 - \mathsf{B}^S)$:
\[
 \nabla_{_S}Y_t = (1 - \mathsf{B}^S)Y_t = Y_t - Y_{t-S}.
\]

*** Notación: ARIMA
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

Extendemos la notación a procesos con raíces autorregresivas unitarias con ``ARIMA($p,d,q$)''; 
donde $d$ indica el número de diferencias que la serie necesita para ser $I(0)$,
\[
 \boldsymbol{\phi}_p*\nabla^d*\boldsymbol{Y} = \boldsymbol{\theta}_q* \boldsymbol{U};
\]
es decir
\[
 \boldsymbol{\phi}_p(\mathsf{B})\nabla^d Y_t = \boldsymbol{\theta}_q(\mathsf{B}) U_t; \quad t\in\mathbb{Z}.
\]

** Raíces unitarias en los polinomios AR y MA
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:
   
Cuando un polinomio tiene alguna raíz igual a uno se dice que tiene ``raíces unitarias''.

Si el polinomio AR estimado tiene alguna raíz ``próxima a uno'' es síntoma de infradiferenciación.

Si el polinomio MA estimado tiene alguna raíz ``próxima a uno'' es síntoma de sobrediferenciación.
# a) sobrediferenciación... salvo cuando...
# b) antes de diferenciar hubiera una tendencia determinista (que podemos comprobar, por ejemplo, con un test ADF).

Ejemplos:
# que ilustran los tres casos:

#+ATTR_HTML: :border 2 :rules all :frame border
#+ATTR_LATEX: :environment longtable :align p{7cm}p{7cm}
| Modelo expresado con raíces unitarias en $\boldsymbol{\phi}$ o $\boldsymbol{\theta}$ | Modelo equivalente sin raíces unitarias en $\boldsymbol{\phi}$ o $\boldsymbol{\theta}$ |
|--------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------|
| $(1-1.5\mathsf{B}+.5\mathsf{B}^2) Y_t = U_t$                                         | ${\color{blue}{(1-0.5\mathsf{B})\nabla Y_t=U_t}}$                                      |
| $(1-.5\mathsf{B}+0.7\mathsf{B}^2)\nabla^2Y_t=(1-\mathsf{B})U_t$                      | ${\color{blue}{(1-.5\mathsf{B}+0.7\mathsf{B}^2)\nabla Y_t =  U_t}}$                    |
| $\nabla Y_t = \beta+          (1-\mathsf{B}) U_t$                                    | ${\color{blue}{Y_t = \beta t + U_t}}\quad$ (¡no estacionario!)                         |

** Paseos aleatorios
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:


Un paseo aleatorio representa una variable cuyos incrementos son ruido blanco:
\[Y_t = \mu + Y_{t-1} + U_t.\]

Cuando $\mu\ne0$ se denomina /paseo aleatorio con deriva/: $\;\nabla Y_t = \mu +  U_t$.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-RWcd.png :exports code :results silent
fig = plot_paseo_aleatorio_analysis(trend="t", pendiente=0.25, n=400, semilla=2025)
fig.savefig('./img/lecc07/ACF-RWcd.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-RWcd.png]]
 
El proceso tiene mayor inercia cuanto mayor es $|\mu|$. El signo de $\mu$ determina el signo de la pendiente global.


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Cuando $\mu=0$ se denomina sencillamente /paseo aleatorio/: $\;\nabla Y_t =  U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-RW.png :exports code :results silent
fig = plot_paseo_aleatorio_analysis(n=400, semilla=2026)
fig.savefig('./img/lecc07/ACF-RW.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-RW.png]]


# +latex: \newpage


* Modelos ARIMA estacionales (SARIMA)
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:
   
El período estacional $S$ es el número mínimo de observaciones necesarias para recorrer un ciclo estacional completo. 
Por ejemplo, $S=12$ para datos mensuales, $S=4$ para datos trimestrales, $S=24$ para datos horarios, etc.

Captamos la estacionalidad con modelos ARIMA$(p,d,q)\times(P,D,Q)_S$ 
\[\boldsymbol{\phi}_p(\mathsf{B})\,\boldsymbol{\Phi}_P(\mathsf{B}^S)\,\nabla^d\,\nabla_{_S}^D\, Y_t
 =\boldsymbol{\theta}_q(\mathsf{B})\,\boldsymbol{\Theta}_q(\mathsf{B}^S)\, U_t; 
 \quad t\in\mathbb{Z}\]
donde
\begin{align*}
 \boldsymbol{\Phi}_P(\mathsf{B}^S)  = & 1-\Phi_1\mathsf{B}^{1\cdot S}-\Phi_2\mathsf{B}^{2\cdot S}-\cdots-\Phi_P\mathsf{B}^{P\cdot S}\\
 \boldsymbol{\Theta}_Q(\mathsf{B}^S)  = & 1-\Theta_1\mathsf{B}^{1\cdot S}-\Theta_2\mathsf{B}^{2\cdot S}-\cdots-\Theta_Q\mathsf{B}^{Q\cdot S}\\
 {\nabla}_{_S}^D = & (1 - \mathsf{B}^S)^D
\end{align*}
Es decir, el modelo consta de polinomios autorregresivos y de media móvil tanto regulares (en minúsculas) como estacionales (en mayúsculas).
#+latex: \medskip
 
#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Veamos un ejemplo de un modelo MA($1$) estacional y otro de un modelo AR($1$) estacional...

** MA(1) estacional con raíz positiva
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
MA($1$) estacional:
$\quad\boldsymbol{\Theta}=1-0.9z^{12}\quad\Rightarrow\quad X_t= (1-0.9 \mathsf{B}^{12})U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-SMA1p.png :exports code :results none 
SMA1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(seasonal_ma_params=SMA1, s=12)
fig.savefig('./img/lecc07/ACF-SMA1p.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-SMA1p.png]]

#+name: raices-SMA1p
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Roots-SMA1p.png :results silent :exports results
dfi.export(polynomial_roots_table(Seasonal_to_regular_polynomial(SMA1, 12).coef),
           "./img/lecc07/Roots-SMA1p.png", dpi=300, table_conversion="matplotlib")
#+END_SRC

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+attr_html: :width 500px
#+attr_org: :width 800
#+attr_latex: :width 250px
[[file:img/lecc07/Roots-SMA1p.png]]



#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-SMA1p.png :exports code :results silent
fig = plot_sarima_analysis(seasonal_ma_params=SMA1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-SMA1p.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-SMA1p.png]]

** AR(1) estacional con raíz positiva
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
AR($1$) estacional:
$\quad\boldsymbol{\Phi}=1-0.9z^{12}\quad\Rightarrow\quad (1-0.9 \mathsf{B}^{12})X_t= U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-SAR1p.png :exports code :results none 
SAR1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(seasonal_ar_params=SAR1, s=12)
fig.savefig('./img/lecc07/ACF-SAR1p.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-SAR1p.png]]

#+name: raices-SAR1p
#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Roots-SAR1p.png :results silent :exports results
dfi.export(polynomial_roots_table(Seasonal_to_regular_polynomial(SAR1, 12).coef),
           "./img/lecc07/Roots-SAR1p.png", dpi=300, table_conversion="matplotlib")
#+END_SRC

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+attr_html: :width 500px
#+attr_org: :width 800
#+attr_latex: :width 250px
[[file:img/lecc07/Roots-SAR1p.png]]

#+attr_ipynb: (slideshow . ((slide_type . notes)))
Evidentemente las raíces son iguales a las del caso anterior (aunque ahora corresponden al polinomio autorregresivo).


#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-SAR1p.png :exports code :results silent
fig = plot_sarima_analysis(seasonal_ar_params=SAR1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-SAR1p.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-SAR1p.png]]




#+attr_ipynb: (slideshow . ((slide_type . subslide)))
Con estos dos ejemplos hemos podido apreciar que:
- las pautas de autocorrelación son análogas a las de los MA(1) y  AR(2), pero ahora los retardos significativos corresponden a los retardos estacionales, es decir, a múltiplos del período estacional $S$.
- En estos ejemplos, en los que $S=12$, los retardos estacionales son: 12, 24, 36, 48, 60,...
- las correlaciones correspondientes a los “retardos regulares” (es decir, todos menos menos los estacionales) son no significativas en general.
#+latex:\bigskip

#+attr_ipynb: (slideshow . ((slide_type . fragment)))
Veamos ahora un par de ejemplos de modelos estacionales multiplicativos (i.e., con parte regular y parte estacional).

** ARIMA (0,0,1)x(0,0,1) 
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:

#+attr_ipynb: (slideshow . ((slide_type . slide)))
ARIMA$(0,0,1)\times(0,0,1)_{12}$: $\quad X_t= (1-0.9 \mathsf{B})(1-0.9 \mathsf{B}^{12})U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-MA1SMA1.png :exports code :results none 
MA1  = [1, -0.8]
SMA1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(ma_params=MA1, seasonal_ma_params=SMA1, s=12)
fig.savefig('./img/lecc07/ACF-MA1SMA1.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-MA1SMA1.png]]



#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-MA1SMA1.png :exports code :results silent
fig = plot_sarima_analysis(ma_params=MA1, seasonal_ma_params=SMA1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-MA1SMA1.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-MA1SMA1.png]]

** ARIMA (1,0,0)x(0,0,1) 
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:

#+attr_ipynb: (slideshow . ((slide_type . subslide)))
ARIMA$(1,0,0)\times(0,0,1)_{12}$: $\quad (1-0.9 \mathsf{B})X_t= (1-0.9 \mathsf{B}^{12})U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-AR1SMA1.png :exports code :results none 
AR1  = [1, -0.8]
SMA1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(ar_params=AR1, seasonal_ma_params=SMA1, s=12)
fig.savefig('./img/lecc07/ACF-AR1SMA1.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-AR1SMA1.png]]



#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-AR1SMA1.png :exports code :results silent
fig = plot_sarima_analysis(ar_params=AR1, seasonal_ma_params=SMA1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-AR1SMA1.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-AR1SMA1.png]]

** ARIMA (1,0,0)x(1,0,0) 
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
ARIMA$(1,0,0)\times(1,0,0)_{12}$: $\quad (1-0.9 \mathsf{B})(1-0.9 \mathsf{B}^{12})X_t= U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-AR1SAR1.png :exports code :results none 
AR1  = [1, -0.8]
SAR1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(ar_params=AR1, seasonal_ar_params=SAR1, s=12, logs=True)
fig.savefig('./img/lecc07/ACF-AR1SAR1.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-AR1SAR1.png]]



#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-AR1SAR1.png :exports code :results silent
fig = plot_sarima_analysis(ar_params=AR1, seasonal_ar_params=SAR1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-AR1SAR1.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-AR1SAR1.png]]

** ARIMA (0,0,1)x(1,0,0) 
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
ARIMA$(0,0,1)\times(1,0,0)_{12}$: $\quad (1-0.9 \mathsf{B}^{12})X_t= (1-0.9 \mathsf{B})U_t$

#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file ACF-MA1SAR1.png :exports code :results none 
MA1  = [1, -0.8]
SAR1 = [1, -0.9]
fig = plot_arma_seasonal_parametric_diagnostics(ma_params=MA1, seasonal_ar_params=SAR1, s=12)
fig.savefig('./img/lecc07/ACF-MA1SAR1.png', dpi=300, bbox_inches='tight')
#+END_SRC

#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/ACF-MA1SAR1.png]]



#+attr_ipynb: (slideshow . ((slide_type . skip)))
#+BEGIN_SRC jupyter-python :results file :output-dir ./img/lecc07/ :file Sim-MA1SAR1.png :exports code :results silent
fig = plot_sarima_analysis(ma_params=MA1, seasonal_ar_params=SAR1, s=12, seed=2025)
fig.savefig('./img/lecc07/Sim-MA1SAR1.png', dpi=300, bbox_inches='tight')
#+END_SRC


#+attr_ipynb: (slideshow . ((slide_type . fragment)))
#+attr_org: :width 800
#+attr_html: :width 900px
#+attr_latex: :width 425px
[[./img/lecc07/Sim-MA1SAR1.png]]


#+attr_ipynb: (slideshow . ((slide_type . subslide)))
En estos cuatro ejemplos hemos podido apreciar que
- en el entorno de los retardos estacionales surgen una serie de coeficientes significativos (“satélites”) que proceden de la interacción entre las estructuras regular y estacional
- Estos satélites son útiles para identificar en qué retardos estacionales hay autocorrelaciones no nulas, pero no requieren una parametrización especial.


* Resumen del análisis univariante de series temporales 
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . notes)))
   :END:

** Ideas principales respecto a la modelización univariante
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . slide)))
   :END:

- Es una modelización sin variables exógenas 
- Modelizan la interdependencia temporal con polinomios de órdenes reducidos.
- Está especialmente indicada para hacer predicción.
- Parte de dos supuestos sobre el proceso estocástico subyacente:
  a) es débilmente estacionario
  b) tiene representación como proceso lineal:
     $\; Y_t=\mu+\sum_{j=0}^\infty a_j U_{t-j};\quad$ con
     $\quad\mu\in\mathbb{R},\;$ $\quad\boldsymbol{a}\in\ell^2\quad$ y
     $\quad\boldsymbol{U}\sim WN(0,\sigma^2)$
   
     # (se suele asumir normalidad en $U_t$)
- Utiliza múltiples instrumentos: (a) gráficos (b) función de autocorrelación (c) función de autocorrelación parcial, (d) estadístico Q de Ljung-Box, etc...
- Si la serie original no "parece" débilmente estacionaria, se induce esta propiedad mediante las transformaciones adecuadas

# Los procesos lineales tienen pautas de autocorrelación teórica características y reconocibles

#+ATTR_HTML: :border 2 :rules all :frame border
#+ATTR_LATEX: :environment longtable :align p{3.4cm}p{5.4cm}p{5.4cm}
|                    | *ACF finita*                                             | *ACF persistente*                |
|--------------------+----------------------------------------------------------+----------------------------------|
| *PACF finita*      | _Ruido blanco_: retardos conjuntamente NO significativos | _AR_: orden indicado por la PACF |
| *PACF persistente* | _MA_: orden indicado por la ACF                          | _ARMA_                           |

** Metodología
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . subslide)))
   :END:
   
Tres fases:
- Identificación :: Se elige una especificación provisional para el proceso estocástico subyacente en función de las características medibles de los datos (``dejar que los datos hablen'')
- Estimación :: suele requerir métodos iterativos (/Gretl se encarga de esto/)
- Diagnosis :: de la calidad estadística del modelo ajustado. Algunos controles estándar son:
  - Significatividad de los parámetros estimados
  - Estacionariedad y homocedasticidad de los residuos
  - ¿Existe un patrón de autocorrelación residual que podría ser modelado? 
    ¿O hemos logrado que los residuos sean */"ruido blanco"/*?

Si la diagnosis no es satisfactoria, se vuelve a la primera fase.

Si la diagnosis es satisfactoria... ¡hemos logrado un modelo aceptable!... que podremos usar para realizar pronósticos.


* Introducción intuitiva e informal al concepto de ergodicidad
:PROPERTIES:
:metadata: (slideshow . ((slide_type . skip)))
:ID:       53eeba38-4fe7-432c-84f5-a5e9a0dc0808
:END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
La *ergodicidad* de un proceso estocástico es importante porque conecta el comportamiento en el tiempo de una sola realización del proceso con las propiedades estadísticas del conjunto de todas las posibles realizaciones. En otras palabras, nos dice cuándo “promediar en la muestra” puede arrojar un buen estimador de algún momento teórico. Es decir, si el proceso es *ergódico*:
- estimadores como la media muestral, la varianza muestral o las autocorrelaciones son *consistentes y útiles*, porque en el límite nos dan el verdadero parámetro poblacional.
  Por ejemplo, cuando el proceso es ergódico, el *promedio* de una sola realización (la media muestral de una serie larga) *converge* al valor esperado teórico (el promedio de la distribución incondicional del proceso).
- En muchas aplicaciones, eso también se traduce en eficiencia asintótica (dependiendo del tipo de proceso y del estimador).

#+attr_ipynb: (slideshow . ((slide_type . skip)))
En cambio, si el proceso *no es ergódico* los promedios en el tiempo dependen de la trayectoria concreta observada, y pueden *no coincidir* con los momentos teóricos.
- En tal caso, un estimador basado en una única realización puede estar *sesgado* o ser *inconsistente*: no converge al parámetro teórico, sino a algo dependiente de la trayectoria particular. 
  Lo cual significa que los métodos estadísticos usuales de series temporales (media muestral, autocorrelaciones, regresiones dinámicas) dejan de ser fiables.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
En términos prácticos: la ergodicidad es lo que “salva” la inferencia empírica. Sin ergodicidad, trabajar con datos de una sola realización en el tiempo (que es lo que tenemos en economía, física o finanzas) no nos dice realmente cómo es la distribución del proceso estocástico.
- Si un proceso es ergódico, entonces una única trayectoria suficientemente larga contiene información estadística suficiente para realizar inferencia estadística sobre el proceso estocástico.
- Esto significa que podemos estimar valores esperados (media, varianza, correlaciones) observando un solo experimento durante mucho tiempo, sin necesidad de repetirlo muchas veces.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
- En la práctica, cuando medimos fenómenos físicos, económicos o sociales, solo observamos *una* trayectoria del proceso (ejemplo: una serie temporal de precios).
- La ergodicidad garantiza que los cálculos hechos sobre esa única serie son representativos de la distribución subyacente.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
- La ergodicidad suele estudiarse en procesos estacionarios. No todos los procesos estacionarios son ergódicos, pero la ergodicidad asegura que la ``estabilidad en el tiempo'' realmente refleja la ``estabilidad en probabilidad''.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
- Implicaciones en teoría y aplicaciones:
  - En estadística de series temporales: justifica el uso de una muestra temporal.
  - En economía: permite usar datos históricos de una variable para inferir expectativas a largo plazo.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
En resumen: la ergodicidad es crucial porque es lo que hace posible inferir la estructura probabilística de un proceso observando solo una realización larga en el tiempo, algo que es lo habitual en el mundo real.

** Ejemplo de un proceso no ergódico
   :PROPERTIES:
   :metadata: (slideshow . ((slide_type . skip)))
   :END:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Consideremos un proceso de Bernoulli con un /parámetro aleatorio fijo/ $p$:

#+attr_ipynb: (slideshow . ((slide_type . skip)))
1. Primero se elige al azar una probabilidad $p$ con distribución uniforme en el intervalo $[0,1]$.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
2 Luego se genera una secuencia infinita de lanzamientos de moneda con esa probabilidad $p$.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
En dicho experimento, como el valor esperado para el parámetro $p$ es $0.5$ (es el valor esperado de una distribución uniforme $[0,1]$), la esperanza de un lanzamiento es $E(X)=p=0.5$; 
y si promediáramos los promedios de muchas realizaciones de esas secuencias infinitas, nos aproximaríamos al verdadero valor esperado $E(X)=0.5$. 

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Pero si tan solo tenemos una realización del proceso, a largo plazo el promedio temporal de la secuencia tenderá casi seguro al valor particular de $p$ que nos haya salido al principio, que no es necesariamente a $0.5$.

#+attr_ipynb: (slideshow . ((slide_type . skip)))
Por tanto, este proceso no es ergódico: los promedios temporales dependen de la trayectoria inicial (dependen del valor de $p$ que nos ``tocó'' inicialmente).


* COMMENT ipynb y slides                                           :noexport:

#+BEGIN_SRC emacs-lisp :results silent :eval yes
(require 'ox-ipynb)
(ox-ipynb-export-org-file-to-ipynb-file "Lecc07.org")
#+END_SRC

#+BEGIN_SRC sh :results silent :eval yes
jupyter nbconvert --execute --inplace Lecc07.ipynb
#+END_SRC

#+BEGIN_SRC sh :results silent :eval yes
jupyter nbconvert --config ../mycfg-GitHubPages.py --to slides --reveal-prefix "https://unpkg.com/reveal.js@5.2.1" --execute Lecc07.ipynb
#+END_SRC

